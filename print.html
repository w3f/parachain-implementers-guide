<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>The Polkadot Parachain Host Implementers&#x27; Guide</title>
                <meta name="robots" content="noindex" />
                

        <!-- Custom HTML head -->
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

                <link rel="icon" href="favicon.svg">
                        <link rel="shortcut icon" href="favicon.png">
                <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
                <link rel="stylesheet" href="css/print.css" media="print">
        
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
                <link rel="stylesheet" href="fonts/fonts.css">
        
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        
            </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="index.html">Preamble</a></li><li class="chapter-item expanded "><a href="whence-parachains.html"><strong aria-hidden="true">1.</strong> Whence Parachains</a></li><li class="chapter-item expanded "><a href="protocol-overview.html"><strong aria-hidden="true">2.</strong> Protocol Overview</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="protocol-approval.html"><strong aria-hidden="true">2.1.</strong> Approval Process</a></li><li class="chapter-item expanded "><a href="protocol-disputes.html"><strong aria-hidden="true">2.2.</strong> Disputes Process</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="disputes-flow.html"><strong aria-hidden="true">2.2.1.</strong> Dispute Flow</a></li></ol></li><li class="chapter-item expanded "><a href="protocol-chain-selection.html"><strong aria-hidden="true">2.3.</strong> Chain Selection and Finalization</a></li></ol></li><li class="chapter-item expanded "><a href="architecture.html"><strong aria-hidden="true">3.</strong> Architecture Overview</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="messaging.html"><strong aria-hidden="true">3.1.</strong> Messaging Overview</a></li></ol></li><li class="chapter-item expanded "><a href="runtime/index.html"><strong aria-hidden="true">4.</strong> Runtime Architecture</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="runtime/initializer.html"><strong aria-hidden="true">4.1.</strong> Initializer Module</a></li><li class="chapter-item expanded "><a href="runtime/configuration.html"><strong aria-hidden="true">4.2.</strong> Configuration Module</a></li><li class="chapter-item expanded "><a href="runtime/shared.html"><strong aria-hidden="true">4.3.</strong> Shared</a></li><li class="chapter-item expanded "><a href="runtime/disputes.html"><strong aria-hidden="true">4.4.</strong> Disputes Module</a></li><li class="chapter-item expanded "><a href="runtime/paras.html"><strong aria-hidden="true">4.5.</strong> Paras Module</a></li><li class="chapter-item expanded "><a href="runtime/scheduler.html"><strong aria-hidden="true">4.6.</strong> Scheduler Module</a></li><li class="chapter-item expanded "><a href="runtime/inclusion.html"><strong aria-hidden="true">4.7.</strong> Inclusion Module</a></li><li class="chapter-item expanded "><a href="runtime/parainherent.html"><strong aria-hidden="true">4.8.</strong> ParaInherent Module</a></li><li class="chapter-item expanded "><a href="runtime/dmp.html"><strong aria-hidden="true">4.9.</strong> DMP Module</a></li><li class="chapter-item expanded "><a href="runtime/ump.html"><strong aria-hidden="true">4.10.</strong> UMP Module</a></li><li class="chapter-item expanded "><a href="runtime/hrmp.html"><strong aria-hidden="true">4.11.</strong> HRMP Module</a></li><li class="chapter-item expanded "><a href="runtime/session_info.html"><strong aria-hidden="true">4.12.</strong> Session Info Module</a></li></ol></li><li class="chapter-item expanded "><a href="runtime-api/index.html"><strong aria-hidden="true">5.</strong> Runtime APIs</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="runtime-api/validators.html"><strong aria-hidden="true">5.1.</strong> Validators</a></li><li class="chapter-item expanded "><a href="runtime-api/validator-groups.html"><strong aria-hidden="true">5.2.</strong> Validator Groups</a></li><li class="chapter-item expanded "><a href="runtime-api/availability-cores.html"><strong aria-hidden="true">5.3.</strong> Availability Cores</a></li><li class="chapter-item expanded "><a href="runtime-api/persisted-validation-data.html"><strong aria-hidden="true">5.4.</strong> Persisted Validation Data</a></li><li class="chapter-item expanded "><a href="runtime-api/session-index.html"><strong aria-hidden="true">5.5.</strong> Session Index</a></li><li class="chapter-item expanded "><a href="runtime-api/validation-code.html"><strong aria-hidden="true">5.6.</strong> Validation Code</a></li><li class="chapter-item expanded "><a href="runtime-api/candidate-pending-availability.html"><strong aria-hidden="true">5.7.</strong> Candidate Pending Availability</a></li><li class="chapter-item expanded "><a href="runtime-api/candidate-events.html"><strong aria-hidden="true">5.8.</strong> Candidate Events</a></li><li class="chapter-item expanded "><a href="runtime-api/disputes-info.html"><strong aria-hidden="true">5.9.</strong> Disputes Info</a></li><li class="chapter-item expanded "><a href="runtime-api/candidates-included.html"><strong aria-hidden="true">5.10.</strong> Candidates Included</a></li></ol></li><li class="chapter-item expanded "><a href="node/index.html"><strong aria-hidden="true">6.</strong> Node Architecture</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="node/subsystems-and-jobs.html"><strong aria-hidden="true">6.1.</strong> Subsystems and Jobs</a></li><li class="chapter-item expanded "><a href="node/overseer.html"><strong aria-hidden="true">6.2.</strong> Overseer</a></li><li class="chapter-item expanded "><a href="node/grandpa-voting-rule.html"><strong aria-hidden="true">6.3.</strong> GRANDPA Voting Rule</a></li><li class="chapter-item expanded "><a href="node/collators/index.html"><strong aria-hidden="true">6.4.</strong> Collator Subsystems</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="node/collators/collation-generation.html"><strong aria-hidden="true">6.4.1.</strong> Collation Generation</a></li><li class="chapter-item expanded "><a href="node/collators/collator-protocol.html"><strong aria-hidden="true">6.4.2.</strong> Collator Protocol</a></li></ol></li><li class="chapter-item expanded "><a href="node/backing/index.html"><strong aria-hidden="true">6.5.</strong> Backing Subsystems</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="node/backing/candidate-backing.html"><strong aria-hidden="true">6.5.1.</strong> Candidate Backing</a></li><li class="chapter-item expanded "><a href="node/backing/statement-distribution.html"><strong aria-hidden="true">6.5.2.</strong> Statement Distribution</a></li></ol></li><li class="chapter-item expanded "><a href="node/availability/index.html"><strong aria-hidden="true">6.6.</strong> Availability Subsystems</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="node/availability/availability-distribution.html"><strong aria-hidden="true">6.6.1.</strong> Availability Distribution</a></li><li class="chapter-item expanded "><a href="node/availability/availability-recovery.html"><strong aria-hidden="true">6.6.2.</strong> Availability Recovery</a></li><li class="chapter-item expanded "><a href="node/availability/bitfield-distribution.html"><strong aria-hidden="true">6.6.3.</strong> Bitfield Distribution</a></li><li class="chapter-item expanded "><a href="node/availability/bitfield-signing.html"><strong aria-hidden="true">6.6.4.</strong> Bitfield Signing</a></li></ol></li><li class="chapter-item expanded "><a href="node/approval/index.html"><strong aria-hidden="true">6.7.</strong> Approval Subsystems</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="node/approval/approval-voting.html"><strong aria-hidden="true">6.7.1.</strong> Approval Voting</a></li><li class="chapter-item expanded "><a href="node/approval/approval-distribution.html"><strong aria-hidden="true">6.7.2.</strong> Approval Distribution</a></li></ol></li><li class="chapter-item expanded "><a href="node/disputes/index.html"><strong aria-hidden="true">6.8.</strong> Disputes Subsystems</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="node/disputes/dispute-coordinator.html"><strong aria-hidden="true">6.8.1.</strong> Dispute Coordinator</a></li><li class="chapter-item expanded "><a href="node/disputes/dispute-participation.html"><strong aria-hidden="true">6.8.2.</strong> Dispute Participation</a></li><li class="chapter-item expanded "><a href="node/disputes/dispute-distribution.html"><strong aria-hidden="true">6.8.3.</strong> Dispute Distribution</a></li></ol></li><li class="chapter-item expanded "><a href="node/utility/index.html"><strong aria-hidden="true">6.9.</strong> Utility Subsystems</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="node/utility/availability-store.html"><strong aria-hidden="true">6.9.1.</strong> Availability Store</a></li><li class="chapter-item expanded "><a href="node/utility/candidate-validation.html"><strong aria-hidden="true">6.9.2.</strong> Candidate Validation</a></li><li class="chapter-item expanded "><a href="node/utility/provisioner.html"><strong aria-hidden="true">6.9.3.</strong> Provisioner</a></li><li class="chapter-item expanded "><a href="node/utility/network-bridge.html"><strong aria-hidden="true">6.9.4.</strong> Network Bridge</a></li><li class="chapter-item expanded "><a href="node/utility/gossip-support.html"><strong aria-hidden="true">6.9.5.</strong> Gossip Support</a></li><li class="chapter-item expanded "><a href="node/utility/misbehavior-arbitration.html"><strong aria-hidden="true">6.9.6.</strong> Misbehavior Arbitration</a></li><li class="chapter-item expanded "><a href="node/utility/peer-set-manager.html"><strong aria-hidden="true">6.9.7.</strong> Peer Set Manager</a></li><li class="chapter-item expanded "><a href="node/utility/runtime-api.html"><strong aria-hidden="true">6.9.8.</strong> Runtime API Requests</a></li><li class="chapter-item expanded "><a href="node/utility/chain-api.html"><strong aria-hidden="true">6.9.9.</strong> Chain API Requests</a></li><li class="chapter-item expanded "><a href="node/utility/chain-selection.html"><strong aria-hidden="true">6.9.10.</strong> Chain Selection Request</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="types/index.html"><strong aria-hidden="true">7.</strong> Data Structures and Types</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="types/candidate.html"><strong aria-hidden="true">7.1.</strong> Candidate</a></li><li class="chapter-item expanded "><a href="types/backing.html"><strong aria-hidden="true">7.2.</strong> Backing</a></li><li class="chapter-item expanded "><a href="types/availability.html"><strong aria-hidden="true">7.3.</strong> Availability</a></li><li class="chapter-item expanded "><a href="types/overseer-protocol.html"><strong aria-hidden="true">7.4.</strong> Overseer and Subsystem Protocol</a></li><li class="chapter-item expanded "><a href="types/runtime.html"><strong aria-hidden="true">7.5.</strong> Runtime</a></li><li class="chapter-item expanded "><a href="types/chain.html"><strong aria-hidden="true">7.6.</strong> Chain</a></li><li class="chapter-item expanded "><a href="types/messages.html"><strong aria-hidden="true">7.7.</strong> Messages</a></li><li class="chapter-item expanded "><a href="types/network.html"><strong aria-hidden="true">7.8.</strong> Network</a></li><li class="chapter-item expanded "><a href="types/approval.html"><strong aria-hidden="true">7.9.</strong> Approvals</a></li><li class="chapter-item expanded "><a href="types/disputes.html"><strong aria-hidden="true">7.10.</strong> Disputes</a></li></ol></li><li class="chapter-item expanded "><a href="glossary.html">Glossary</a></li><li class="chapter-item expanded affix "><a href="further-reading.html">Further Reading</a></li></ol>            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                                                <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                                            </div>

                    <h1 class="menu-title">The Polkadot Parachain Host Implementers&#x27; Guide</h1>

                    <div class="right-buttons">
                                                <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                                                                        
                    </div>
                </div>

                                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="preamble"><a class="header" href="#preamble">Preamble</a></h1>
<p>This document aims to describe the purpose, functionality, and implementation of the host for Polkadot's <em>parachains</em> functionality - that is, the software which provides security and advancement for constituent parachains. It is not for the implementer of a specific parachain but rather for the implementer of the Parachain Host. In practice, this is for the implementers of Polkadot in general.</p>
<p>There are a number of other documents describing the research in more detail. All referenced documents will be linked here and should be read alongside this document for the best understanding of the full picture. However, this is the only document which aims to describe key aspects of Polkadot's particular instantiation of much of that research down to low-level technical details and software architecture.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="whence-parachains"><a class="header" href="#whence-parachains">Whence Parachains</a></h1>
<p>Parachains are the solution to a problem. As with any solution, it cannot be understood without first understanding the problem. So let's start by going over the issues faced by blockchain technology that led to us beginning to explore the design space for something like parachains.</p>
<h2 id="issue-1-scalability"><a class="header" href="#issue-1-scalability">Issue 1: Scalability</a></h2>
<p>It became clear a few years ago that the transaction throughput of simple Proof-of-Work (PoW) blockchains such as Bitcoin, Ethereum, and myriad others was simply too low.</p>
<blockquote>
<p>TODO: what if there were more blockchains, etc.</p>
</blockquote>
<p>Proof-of-Stake (PoS) systems can accomplish higher throughput than PoW blockchains. PoS systems are secured by bonded capital as opposed to spent effort - liquidity opportunity cost vs. burning electricity. The way they work is by selecting a set of validators with known economic identity who lock up tokens in exchange for earning the right to &quot;validate&quot; or participate in the consensus process. If they are found to carry out that process wrongly, they will be slashed, meaning some or all of the locked tokens will be burned. This provides a strong disincentive in the direction of misbehavior.</p>
<p>Since the consensus protocol doesn't revolve around wasting effort, block times and agreement can occur much faster. Solutions to PoW challenges don't have to be found before a block can be authored, so the overhead of authoring a block is reduced to only the costs of creating and distributing the block.</p>
<p>However, consensus on a PoS chain requires full agreement of 2/3+ of the validator set for everything that occurs at Layer 1: all logic which is carried out as part of the blockchain's state machine. This means that everybody still needs to check everything. Furthermore, validators may have different views of the system based on the information that they receive over an asynchronous network, making agreement on the latest state more difficult.</p>
<p>Parachains are an example of a <strong>sharded</strong> protocol. Sharding is a concept borrowed from traditional database architecture. Rather than requiring every participant to check every transaction, we require each participant to check some subset of transactions, with enough redundancy baked in that byzantine (arbitrarily malicious) participants can't sneak in invalid transactions - at least not without being detected and getting slashed, with those transactions reverted.</p>
<p>Sharding and Proof-of-Stake in coordination with each other allow a parachain host to provide full security on many parachains, even without all participants checking all state transitions.</p>
<blockquote>
<p>TODO: note about network effects &amp; bridging</p>
</blockquote>
<h2 id="issue-2-flexibility--specialization"><a class="header" href="#issue-2-flexibility--specialization">Issue 2: Flexibility / Specialization</a></h2>
<p>&quot;dumb&quot; VMs don't give you the flexibility. Any engineer knows that being able to specialize on a problem gives them and their users more <em>leverage</em>.</p>
<blockquote>
<p>TODO: expand on leverage</p>
</blockquote>
<p>Having recognized these issues, we set out to find a solution to these problems, which could allow developers to create and deploy purpose-built blockchains unified under a common source of security, with the capability of message-passing between them; a <em>heterogeneous sharding solution</em>, which we have come to know as <strong>Parachains</strong>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="protocol-overview"><a class="header" href="#protocol-overview">Protocol Overview</a></h1>
<p>This section aims to describe, at a high level, the actors and protocols involved in running parachains in Polkadot. Specifically, we describe how different actors communicate with each other, what data structures they keep both individually and collectively, and the high-level purpose on why they do these things.</p>
<p>Our top-level goal is to carry a parachain block from authoring to secure inclusion, and define a process which can be carried out repeatedly and in parallel for many different parachains to extend them over time. Understanding of the high-level approach taken here is important to provide context for the proposed architecture further on. The key parts of Polkadot relevant to this are the main Polkadot blockchain, known as the relay-chain, and the actors which provide security and inputs to this blockchain.</p>
<p>First, it's important to go over the main actors we have involved in this protocol.</p>
<ol>
<li>Validators. These nodes are responsible for validating proposed parachain blocks. They do so by checking a Proof-of-Validity (PoV) of the block and ensuring that the PoV remains available. They put financial capital down as &quot;skin in the game&quot; which can be slashed (destroyed) if they are proven to have misvalidated.</li>
<li>Collators. These nodes are responsible for creating the Proofs-of-Validity that validators know how to check. Creating a PoV typically requires familiarity with the transaction format and block authoring rules of the parachain, as well as having access to the full state of the parachain.</li>
<li>Fishermen. These are user-operated, permissionless nodes whose goal is to catch misbehaving validators in exchange for a bounty. Collators and validators can behave as Fishermen too. Fishermen aren't necessary for security, and aren't covered in-depth by this document.</li>
</ol>
<p>This implies a simple pipeline where collators send validators parachain blocks and their requisite PoV to check. Then, validators validate the block using the PoV, signing statements which describe either the positive or negative outcome, and with enough positive statements, the block can be noted on the relay-chain. Negative statements are not a veto but will lead to a dispute, with those on the wrong side being slashed. If another validator later detects that a validator or group of validators incorrectly signed a statement claiming a block was valid, then those validators will be <em>slashed</em>, with the checker receiving a bounty.</p>
<p>However, there is a problem with this formulation. In order for another validator to check the previous group of validators' work after the fact, the PoV must remain <em>available</em> so the other validator can fetch it in order to check the work. The PoVs are expected to be too large to include in the blockchain directly, so we require an alternate <em>data availability</em> scheme which requires validators to prove that the inputs to their work will remain available, and so their work can be checked. Empirical tests tell us that many PoVs may be between 1 and 10MB during periods of heavy load.</p>
<p>Here is a description of the Inclusion Pipeline: the path a parachain block (or parablock, for short) takes from creation to inclusion:</p>
<ol>
<li>Validators are selected and assigned to parachains by the Validator Assignment routine.</li>
<li>A collator produces the parachain block, which is known as a parachain candidate or candidate, along with a PoV for the candidate.</li>
<li>The collator forwards the candidate and PoV to validators assigned to the same parachain via the <a href="node/collators/collator-protocol.html">Collator Protocol</a>.</li>
<li>The validators assigned to a parachain at a given point in time participate in the <a href="node/backing/candidate-backing.html">Candidate Backing subsystem</a> to validate candidates that were put forward for validation. Candidates which gather enough signed validity statements from validators are considered &quot;backable&quot;. Their backing is the set of signed validity statements.</li>
<li>A relay-chain block author, selected by BABE, can note up to one (1) backable candidate for each parachain to include in the relay-chain block alongside its backing. A backable candidate once included in the relay-chain is considered backed in that fork of the relay-chain.</li>
<li>Once backed in the relay-chain, the parachain candidate is considered to be &quot;pending availability&quot;. It is not considered to be included as part of the parachain until it is proven available.</li>
<li>In the following relay-chain blocks, validators will participate in the <a href="node/availability/availability-distribution.html">Availability Distribution subsystem</a> to ensure availability of the candidate. Information regarding the availability of the candidate will be noted in the subsequent relay-chain blocks.</li>
<li>Once the relay-chain state machine has enough information to consider the candidate's PoV as being available, the candidate is considered to be part of the parachain and is graduated to being a full parachain block, or parablock for short.</li>
</ol>
<p>Note that the candidate can fail to be included in any of the following ways:</p>
<ul>
<li>The collator is not able to propagate the candidate to any validators assigned to the parachain.</li>
<li>The candidate is not backed by validators participating in the Candidate Backing Subsystem.</li>
<li>The candidate is not selected by a relay-chain block author to be included in the relay chain</li>
<li>The candidate's PoV is not considered as available within a timeout and is discarded from the relay chain.</li>
</ul>
<p>This process can be divided further down. Steps 2 &amp; 3 relate to the work of the collator in collating and distributing the candidate to validators via the Collation Distribution Subsystem. Steps 3 &amp; 4 relate to the work of the validators in the Candidate Backing Subsystem and the block author (itself a validator) to include the block into the relay chain. Steps 6, 7, and 8 correspond to the logic of the relay-chain state-machine (otherwise known as the Runtime) used to fully incorporate the block into the chain. Step 7 requires further work on the validators' parts to participate in the Availability Distribution Subsystem and include that information into the relay chain for step 8 to be fully realized.</p>
<p>This brings us to the second part of the process. Once a parablock is considered available and part of the parachain, it is still &quot;pending approval&quot;. At this stage in the pipeline, the parablock has been backed by a majority of validators in the group assigned to that parachain, and its data has been guaranteed available by the set of validators as a whole. Once it's considered available, the host will even begin to accept children of that block. At this point, we can consider the parablock as having been tentatively included in the parachain, although more confirmations are desired. However, the validators in the parachain-group (known as the &quot;Parachain Validators&quot; for that parachain) are sampled from a validator set which contains some proportion of byzantine, or arbitrarily malicious members. This implies that the Parachain Validators for some parachain may be majority-dishonest, which means that (secondary) approval checks must be done on the block before it can be considered approved. This is necessary only because the Parachain Validators for a given parachain are sampled from an overall validator set which is assumed to be up to &lt;1/3 dishonest - meaning that there is a chance to randomly sample Parachain Validators for a parachain that are majority or fully dishonest and can back a candidate wrongly. The Approval Process allows us to detect such misbehavior after-the-fact without allocating more Parachain Validators and reducing the throughput of the system. A parablock's failure to pass the approval process will invalidate the block as well as all of its descendents. However, only the validators who backed the block in question will be slashed, not the validators who backed the descendents.</p>
<p>The Approval Process, at a glance, looks like this:</p>
<ol>
<li>Parablocks that have been included by the Inclusion Pipeline are pending approval for a time-window known as the secondary checking window.</li>
<li>During the secondary-checking window, validators randomly self-select to perform secondary checks on the parablock.</li>
<li>These validators, known in this context as secondary checkers, acquire the parablock and its PoV, and re-run the validation function.</li>
<li>The secondary checkers gossip the result of their checks. Contradictory results lead to escalation, where all validators are required to check the block. The validators on the losing side of the dispute are slashed.</li>
<li>At the end of the Approval Process, the parablock is either Approved or it is rejected. More on the rejection process later.</li>
</ol>
<p>More information on the Approval Process can be found in the dedicated section on <a href="protocol-approval.html">Approval</a>. More information on Disputes can be found in the dedicated section on <a href="protocol-disputes.html">Disputes</a>.</p>
<p>These two pipelines sum up the sequence of events necessary to extend and acquire full security on a Parablock. Note that the Inclusion Pipeline must conclude for a specific parachain before a new block can be accepted on that parachain. After inclusion, the Approval Process kicks off, and can be running for many parachain blocks at once.</p>
<p>Reiterating the lifecycle of a candidate:</p>
<ol>
<li>Candidate: put forward by a collator to a validator.</li>
<li>Seconded: put forward by a validator to other validators</li>
<li>Backable: validity attested to by a majority of assigned validators</li>
<li>Backed: Backable &amp; noted in a fork of the relay-chain.</li>
<li>Pending availability: Backed but not yet considered available.</li>
<li>Included: Backed and considered available.</li>
<li>Accepted: Backed, available, and undisputed</li>
</ol>
<p><img src="protocol_overview_inclusion_pipeline_0.generated.svg" alt="" title="Inclusion Pipeline" /></p>
<p>The diagram above shows the happy path of a block from (1) Candidate to the (7) Approved state.</p>
<p>It is also important to take note of the fact that the relay-chain is extended by BABE, which is a forkful algorithm. That means that different block authors can be chosen at the same time, and may not be building on the same block parent. Furthermore, the set of validators is not fixed, nor is the set of parachains. And even with the same set of validators and parachains, the validators' assignments to parachains is flexible. This means that the architecture proposed in the next chapters must deal with the variability and multiplicity of the network state.</p>
<p><img src="protocol_overview_1.generated.svg" alt="" /></p>
<p>In this example, group 1 has received block C while the others have not due to network asynchrony. Now, a validator from group 2 may be able to build another block on top of B, called C'. Assume that afterwards, some validators become aware of both C and C', while others remain only aware of one.</p>
<p><img src="protocol_overview_2.generated.svg" alt="" /></p>
<p>Those validators that are aware of many competing heads must be aware of the work happening on each one. They may contribute to some or a full extent on both. It is possible that due to network asynchrony two forks may grow in parallel for some time, although in the absence of an adversarial network this is unlikely in the case where there are validators who are aware of both chain heads.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="approval-process"><a class="header" href="#approval-process">Approval Process</a></h1>
<p>The Approval Process is the mechanism by which the relay-chain ensures that only valid parablocks are finalized and that backing validators are held accountable for managing to get bad blocks included into the relay chain.</p>
<p>Having a parachain include a bad block into a fork of the relay-chain is not catastrophic as long as the block isn't finalized by the relay-chain's finality gadget, GRANDPA. If the block isn't finalized, that means that the fork of the relay-chain can be reverted in favor of another by means of a dynamic fork-choice rule which leads honest validators to ignore any forks containing that parablock.</p>
<p>Dealing with a bad parablock proceeds in these stages:</p>
<ol>
<li>Detection</li>
<li>Escalation</li>
<li>Consequences</li>
</ol>
<p>First, the bad block must be detected by an honest party. Second, the honest party must escalate the bad block to be checked by all validators. And last, the correct consequences of a bad block must occur. The first consequence, as mentioned above, is to revert the chain so what full nodes perceive to be best no longer contains the bad parablock. The second consequence is to slash all malicious validators. Note that, if the chain containing the bad block is reverted, that the result of the dispute needs to be transplanted or at least transplantable to all other forks of the chain so that malicious validators are slashed in all possible histories. Phrased alternatively, there needs to be no possible relay-chain in which malicious validators get away cost-free.</p>
<p>Accepting a parablock is the end result of having passed through the detection stage without dispute, or having passed through the escalation/dispute stage with a positive outcome. For this to work, we need the detection procedure to have the properties that enough honest validators are always selected to check the parablock and that they cannot be interfered with by an adversary. This needs to be balanced with the scaling concern of parachains in general: the easiest way to get the first property is to have everyone check everything, but that is clearly too heavy. So we also have a desired constraint on the other property that we have as few validators as possible check any particular parablock. Our assignment function is the method by which we select validators to do approval checks on parablocks.</p>
<p>It often makes more sense to think of relay-chain blocks as having been approved or not as opposed to thinking about whether parablocks have been approved. A relay-chain block containing a single bad parablock needs to be reverted, and a relay-chain block that contains only approved parablocks can be called approved, as long as its parent relay-chain block is also approved. It is important that the validity of any particular relay-chain block depend on the validity of its ancestry, so we do not finalize a block which has a bad block in its ancestry.</p>
<p><img src="approval_process_approval_process_0.generated.svg" alt="" title="Approval Process" /></p>
<p>Approval has roughly two parts:</p>
<ul>
<li>
<p><strong>Assignments</strong> determines which validators performs approval checks on which candidates.  It ensures that each candidate receives enough random checkers, while reducing adversaries' odds for obtaining enough checkers, and limiting adversaries' foreknowledge.  It tracks approval votes to identify when &quot;no show&quot; approval check takes suspiciously long, perhaps indicating the node being under attack, and assigns more checks in this case.  It tracks relay chain equivocations to determine when adversaries possibly gained foreknowledge about assignments, and adds additional checks in this case.</p>
</li>
<li>
<p><strong>Approval checks</strong> listens to the assignments subsystem for outgoing assignment notices that we shall check specific candidates.  It then performs these checks by first invoking the reconstruction subsystem to obtain the candidate, second invoking the candidate validity utility subsystem upon the candidate, and finally sending out an approval vote, or perhaps initiating a dispute.</p>
</li>
</ul>
<p>These both run first as off-chain consensus protocols using messages gossiped among all validators, and second as an on-chain record of this off-chain protocols' progress after the fact.  We need the on-chain protocol to provide rewards for the off-chain protocol.</p>
<p>Approval requires two gossiped message types, assignment notices created by its assignments subsystem, and approval votes sent by our approval checks subsystem when authorized by the candidate validity utility subsystem.</p>
<h3 id="approval-keys"><a class="header" href="#approval-keys">Approval keys</a></h3>
<p>We need two separate keys for the approval subsystem:</p>
<ul>
<li>
<p><strong>Approval assignment keys</strong> are sr25519/schnorrkel keys used only for the assignment criteria VRFs.  We implicitly sign assignment notices with approval assignment keys by including their relay chain context and additional data in the VRF's extra message, but exclude these from its VRF input.</p>
</li>
<li>
<p><strong>Approval vote keys</strong> would only sign off on candidate parablock validity and has no natural key type restrictions. There's no need for this to actualy embody a new session key type. We just want to make a distinction between assignments and approvals, although distant future node configurations might favor separate roles. We re-use the same keys as are used for parachain backing in practice.</p>
</li>
</ul>
<p>Approval vote keys could relatively easily be handled by some hardened signer tooling, perhaps even HSMs assuming we select ed25519 for approval vote keys.  Approval assignment keys might or might not support hardened signer tooling, but doing so sounds far more complex.  In fact, assignment keys determine only VRF outputs that determine approval checker assignments, for which they can only act or not act, so they cannot equivocate, lie, etc. and represent little if any slashing risk for validator operators. </p>
<p>In future, we shall determine which among the several hardening techniques best benefits the netwrok as a whole.  We could provide a multi-process multi-machine architecture for validators, perhaps even reminiscent of GNUNet, or perhaps more resembling smart HSM tooling.  We might instead design a system that more resembled full systems, like like Cosmos' sentry nodes.  In either case, approval assignments might be handled by a slightly hardened machine, but not necessarily nearly as hardened as approval votes, but approval votes machines must similarly run foreign WASM code, which increases their risk, so assignments being separate sounds helpful.</p>
<h2 id="assignments"><a class="header" href="#assignments">Assignments</a></h2>
<p>Approval assignment determines on which candidate parachain blocks each validator performs approval checks.  An approval session considers only one relay chain block and assigns only those candidates that relay chain block declares available.</p>
<p>Assignment balances several concerns:</p>
<ul>
<li>limits adversaries' foreknowledge about assignments,</li>
<li>ensures enough checkers, and </li>
<li>distributes assignments relatively equitably.</li>
</ul>
<p>Assignees determine their own assignments to check specific candidates using two or three assignment criteria.  Assignees never reveal their assignments until relevant, and gossip delays assignments sent early, which limits others' foreknowledge.  Assignees learn their assignment only with the relay chain block. </p>
<p>All criteria require the validator evaluate a verifiable random function (VRF) using their VRF secret key.  All criteria input specific data called &quot;stories&quot; about the session's relay chain block, and output candidates to check and a precedence called a <code>DelayTranche</code>.</p>
<p>We liberate availability cores when their candidate becomes available of course, but one approval assignment criteria continues associating each candidate with the core number it occupied when it became available. </p>
<p>Assignment operates in loosely timed rounds determined by this <code>DelayTranche</code>s, which proceed roughly 12 times faster than six second block production assuming half second gossip times.  If a candidate <code>C</code> needs more approval checkers by the time we reach round <code>t</code> then any validators with an assignment to <code>C</code> in delay tranche <code>t</code> gossip their send assignment notice for <code>C</code>.  We continue until all candidates have enough approval checkers assigned.  We take entire tranches together if we do not yet have enough, so we expect strictly more than enough checkers.  We also take later tranches if some checkers return their approval votes too slow (see no shows below).</p>
<p>Assignment ensures validators check those relay chain blocks for which they have delay tranche zero aka the highest precedence, so that adversaries always face honest checkers equal to the expected number of assignments with delay tranche zero.</p>
<p>Among these criteria, the BABE VRF output provides the story for two, which reduces how frequently adversaries could position their own checkers.  We have one criterion whose story consists of the candidate's block hash plus external knowledge that a relay chain equivocation exists with a conflicting candidate.  It provides unforeseeable assignments when adversaries gain foreknowledge about the other two by committing an equivocation in relay chain block production.</p>
<h2 id="announcements--notices"><a class="header" href="#announcements--notices">Announcements / Notices</a></h2>
<p>We gossip assignment notices among nodes so that all validators know which validators should check each candidate, and if any candidate requires more checkers.</p>
<p>Assignment notices consist of a relay chain context given by a block hash, an assignment criteria, consisting of the criteria identifier and optionally a criteria specific field, an assignee identifier, and a VRF signature by the assignee, which itself consists of a VRF pre-output and a DLEQ proof.  Its VRF input consists of the criteria, usually including a criteria specific field, and a &quot;story&quot; about its relay chain context block. </p>
<p>We never include stories inside the gossip messages containing assignment notices, but require each validator reconstruct them.  We never care about assignments in the disputes process, so this does not complicate remote disputes.</p>
<p>In a Schnorr VRF, there is an extra signed message distinct from this input, which we set to the relay chain block hash.  As a result, assignment notices are self signing and can be &quot;politely&quot; gossiped without additional signatures, meaning between nodes who can compute the story from the relay chain context.  In other words, if we cannot compute the story required by an assignment notice's VRF part then our self signing property fails and we cannot verify its origin.  We could fix this with either another signature layer (64 bytes) or by including the VRF input point computed from the story (32 bytes), but doing so appears unhelpful.</p>
<p>Any validator could send their assignment notices and/or approval votes too early.  We gossip the approval votes early because they represent a major commitment by the validator.  We delay gossiping the assignment notices until they agree with our local clock however.  We also impose a politeness condition that the recipient knows the relay chain context used by the assignment notice.</p>
<h2 id="stories"><a class="header" href="#stories">Stories</a></h2>
<p>We based assignment criteria upon two possible &quot;stories&quot; about the relay chain block <code>R</code> that included the candidate aka declared the candidate available.  All stories have an output that attempts to minimize adversarial influence, which then acts as the VRF input for an assignment criteria.</p>
<p>We first have a <code>RelayVRFStory</code> that outputs the randomness from another VRF output produced by the relay chain block producer when creating <code>R</code>.  Among honest nodes, only this one relay chain block producer who creates <code>R</code> knew the story in advance, and even they knew nothing two epochs previously.</p>
<p>In BABE, we create this value calling <code>schnorrkel::vrf::VRFInOut::make_bytes</code> with a context &quot;A&amp;V RC-VRF&quot;, with the <code>VRFInOut</code> coming from either the VRF that authorized block production for primary blocks, or else from the secondary block VRF for the secondary block type.</p>
<p>In Sassafras, we shall always use the non-anonymized recycling VRF output, never the anonymized ring VRF that authorizes block production.  We do not currently know if Sassafras shall have a separate schnorrkel key, but if it reuses its ring VRF key there is an equivalent <code>ring_vrf::VRFInOut::make_bytes</code>.</p>
<p>We like that <code>RelayVRFStory</code> admits relatively few choices, but an adversary who equivocates in relay chain block production could learn assignments that depend upon the <code>RelayVRFStory</code> too early because the same relay chain VRF appears in multiple blocks. </p>
<p>We therefore provide a secondary <code>RelayEquivocationStory</code> that outputs the candidate's block hash, but only for candidate equivocations.  We say a candidate <code>C</code> in <code>R</code> is an equivocation when there exists another relay chain block <code>R1</code> that equivocates for <code>R</code> in the sense that <code>R</code> and <code>R1</code> have the same <code>RelayVRFStory</code>, but <code>R</code> contains <code>C</code> and <code>R1</code> does not contain <code>C</code>.</p>
<p>We want checkers for candidate equivocations that lie outside our preferred relay chain as well, which represents a slightly different usage for the assignments module, and might require more information in the gossip messages.</p>
<h2 id="assignment-criteria"><a class="header" href="#assignment-criteria">Assignment criteria</a></h2>
<p>Assignment criteria compute actual assignments using stories and the validators' secret approval assignment key.  Assignment criteria output a <code>Position</code> consisting of both a <code>ParaId</code> to be checked, as well as a precedence <code>DelayTranche</code> for when the assignment becomes valid.</p>
<p>Assignment criteria come in three flavors, <code>RelayVRFModulo</code>, <code>RelayVRFDelay</code> and <code>RelayEquivocation</code>.  Among these, both <code>RelayVRFModulo</code> and <code>RelayVRFDelay</code> run a VRF whose input is the output of a <code>RelayVRFStory</code>, while <code>RelayEquivocation</code> runs a VRF whose input is the output of a <code>RelayEquivocationStory</code>.</p>
<p>Among these, we have two distinct VRF output computations:</p>
<p><code>RelayVRFModulo</code> runs several distinct samples whose VRF input is the <code>RelayVRFStory</code> and the sample number.  It computes the VRF output with <code>schnorrkel::vrf::VRFInOut::make_bytes</code> using the context &quot;A&amp;V Core&quot;, reduces this number modulo the number of availability cores, and outputs the candidate just declared available by, and included by aka leaving, that availability core.  We drop any samples that return no candidate because no candidate was leaving the sampled availability core in this relay chain block.  We choose three samples initially, but we could make polkadot more secure and efficient by increasing this to four or five, and reducing the backing checks accordingly.  All successful <code>RelayVRFModulo</code> samples are assigned delay tranche zero.</p>
<p>There is no sampling process for <code>RelayVRFDelay</code> and <code>RelayEquivocation</code>.  We instead run them on specific candidates and they compute a delay from their VRF output.  <code>RelayVRFDelay</code> runs for all candidates included under, aka declared available by, a relay chain block, and inputs the associated VRF output via <code>RelayVRFStory</code>.  <code>RelayEquivocation</code> runs only on candidate block equivocations, and inputs their block hashes via the <code>RelayEquivocation</code> story.</p>
<p><code>RelayVRFDelay</code> and <code>RelayEquivocation</code> both compute their output with <code>schnorrkel::vrf::VRFInOut::make_bytes</code> using the context &quot;A&amp;V Tranche&quot; and reduce the result modulo <code>num_delay_tranches + zeroth_delay_tranche_width</code>, and consolidate results 0 through <code>zeroth_delay_tranche_width</code> to be 0.  In this way, they ensure the zeroth delay tranche has <code>zeroth_delay_tranche_width+1</code> times as many assignments as any other tranche.</p>
<p>As future work (or TODO?), we should merge assignment notices with the same delay and story using <code>vrf_merge</code>.  We cannot merge those with the same delay and different stories because <code>RelayEquivocationStory</code>s could change but <code>RelayVRFStory</code> never changes. </p>
<h2 id="announcer-and-watchertracker"><a class="header" href="#announcer-and-watchertracker">Announcer and Watcher/Tracker</a></h2>
<p>We track all validators' announced approval assignments for each candidate associated to each relay chain block, which tells us which validators were assigned to which candidates.</p>
<p>We permit at most one assignment per candidate per story per validator, so one validator could be assigned under both the <code>RelayVRFDelay</code> and <code>RelayEquivocation</code> criteria, but not under both <code>RelayVRFModulo</code> and <code>RelayVRFDelay</code> criteria, since those both use the same story.  We permit only one approval vote per candidate per validator, which counts for any applicable criteria. </p>
<p>We announce, and start checking for, our own assignments when their tranche's delay is reached, but only if the tracker says the assignee candidate requires more approval checkers.  We never announce an assignment we believe unnecessary because early announcements gives an adversary information.  All delay tranche zero assignments always get announced, which includes all <code>RelayVRFModulo</code> assignments.</p>
<p>In other words, if some candidate <code>C</code> needs more approval checkers by the time we reach round <code>t</code> then any validators with an assignment to <code>C</code> in delay tranche <code>t</code> gossip their send assignment notice for <code>C</code>, and begin reconstruction and validation for 'C.  If however <code>C</code> reached enough assignments, then validators with later assignments skip announcing their assignments.</p>
<p>We continue until all candidates have enough approval checkers assigned.  We never prioritize assignments within tranches and count all or no assignments for a given tranche together, so we often overshoot the target number of assigned approval checkers.</p>
<h3 id="no-shows"><a class="header" href="#no-shows">No shows</a></h3>
<p>We have a &quot;no show&quot; timeout longer than one relay chain slot, so at least 6 seconds, during which we expect approval checks should succeed in reconstructing the candidate block, in redoing its erasure coding to check the candidate receipt, and finally in rechecking the candidate block itself.</p>
<p>We consider a validator a &quot;no show&quot; if they do not approve or dispute within this &quot;no show&quot; timeout from our receiving their assignment notice.  We time this from our receipt of their assignment notice instead of our imagined real time for their tranche because otherwise receiving late assignment notices creates immediate &quot;no shows&quot; and unnecessary work.</p>
<p>We worry &quot;no shows&quot; represent a validator under denial of service attack, presumably to prevent it from reconstructing the candidate, but perhaps delaying it form gossiping a dispute too.  We therefore always replace &quot;no shows&quot; by adding one entire extra delay tranche worth of validators, so such attacks always result in additional checkers. </p>
<p>As an example, imagine we need 20 checkers, but tranche zero produces only 14, and tranche one only 4, then we take all 5 from tranche two, and thus require 23 checkers for that candidate.  If one checker Charlie from tranche one or two does not respond within say 8 seconds, then we add all 7 checkers from tranche three.  If again one checker Cindy from tranche three does not respond within 8 seconds then we take all 3 checkers from tranche four.  We now have 33 checkers working on the candidate, so this escalated quickly.</p>
<p>We escalated so quickly because we worried that Charlie and Cindy might be the only honest checkers assigned to that candidate.  If therefore either Charlie or Cindy finally return an approval, then we can conclude approval, and abandon the checkers from tranche four.</p>
<p>We therefore require the &quot;no show&quot; timeout to be longer than a relay chain slot so that we can witness &quot;no shows&quot; on-chain.  We discuss below how this helps reward validators who replace &quot;no shows&quot;.</p>
<p>We avoid slashing for &quot;no shows&quot; per se, although being &quot;no show&quot; could enter into some computation that punishes repeated poor performance, presumably replaces ImOnline, and we could reduce their rewards and further rewards those who filled in.</p>
<p>As future work, we foresee expanding the &quot;no show&quot; scheme to anonymizes the additional checkers, like by using assignment noticed with a new criteria that employs a ring VRF and then all validators providing cover by requesting a couple erasure coded pieces, but such anonymity scheme sound extremely complex and lie far beyond our initial functionality.</p>
<h2 id="assignment-postponement"><a class="header" href="#assignment-postponement">Assignment postponement</a></h2>
<p>We expect validators could occasionally overloaded when they randomly acquire too many assignments.  All these fluctuations amortize over multiple blocks fairly well, but this slows down finality.</p>
<p>We therefore permit validators to delay sending their assignment noticed intentionally.  If nobody knows about their assignment then they avoid creating &quot;no shows&quot; and the workload progresses normally.</p>
<p>We strongly prefer if postponements come from tranches higher aka less important than zero because tranche zero checks provide somewhat more security.</p>
<p>TODO: When?  Is this optimal for the network?  etc.</p>
<h2 id="on-chain-verification"><a class="header" href="#on-chain-verification">On-chain verification</a></h2>
<p>We should verify approval on-chain to reward approval checkers. We therefore require the &quot;no show&quot; timeout to be longer than a relay chain slot so that we can witness &quot;no shows&quot; on-chain, which helps with this goal. The major challenge with an on-chain record of the off-chain process is adversarial block producers who may either censor votes or publish votes to the chain which cause other votes to be ignored and unrewards (reward stealing).</p>
<p>In principle, all validators have some &quot;tranche&quot; at which they're assigned to the parachain candidate, which ensures we reach enough validators eventually.  As noted above, we often retract &quot;no shows&quot; when the slow validator eventually shows up, so witnessing their initially being a &quot;no show&quot; helps manage rewards.</p>
<p>We expect on-chain verification should work in two phases:  We first record assignments notices and approval votes on-chain in relay chain block, doing the VRF or regular signature verification again in block verification, and inserting chain authenticated unsigned notes into the relay chain state that contain the checker, tranche, paraid, and relay block height for each assignment notice.  We then later have another relay chain block that runs some &quot;approved&quot; intrinsic, which extract all these notes from the state and feeds them into our approval code.</p>
<p>We now encounter one niche concern in the interaction between postponement and on-chain verification:  Any validator with a tranche zero (or other low) assignment could delay sending an assignment notice, like because they postponed their assigned tranche (which is allowed).  If they later send this assignment notices right around finality time, then they race with this approved. intrinsic:  If their announcement gets on-chain (also allowed), then yes it delays finality. If it does not get on-chain, then yes we've one announcement that the off-chain consensus system says is valid, but the chain ignores for being too slow.</p>
<p>We need the chain to win in this case, but doing this requires imposing an annoyingly long overarching delay upon finality.  We might explore limits on postponement too, but this sounds much harder.</p>
<h2 id="parameters"><a class="header" href="#parameters">Parameters</a></h2>
<p>We prefer doing approval checkers assignments under <code>RelayVRFModulo</code> as opposed to <code>RelayVRFDelay</code> because <code>RelayVRFModulo</code> avoids giving individual checkers too many assignments and tranche zero assignments benefit security the most.  We suggest assigning at least 16 checkers under <code>RelayVRFModulo</code> although assignment levels have never been properly analysed. </p>
<p>Our delay criteria <code>RelayVRFDelay</code> and <code>RelayEquivocation</code> both have two primary paramaters, expected checkers per tranche and the zeroth delay tranche width.</p>
<p>We require expected checkers per tranche to be less than three because otherwise an adversary with 1/3 stake could force all nodes into checking all blocks.  We strongly recommend expected checkers per tranche to be less than two, which helps avoid both accedental and intentional explosions.  We also suggest expected checkers per tranche be larger than one, which helps prevent adversaries from predicting than advancing one tranche adds only their own validators.</p>
<p>We improve security more with tranche zero assignments, so <code>RelayEquivocation</code> should consolidates its first several tranches into tranche zero.  We describe this as the zeroth delay tranche width, which initially we set to 12 for <code>RelayEquivocation</code> and <code>1</code> for <code>RelayVRFDelay</code>.</p>
<h2 id="why-vrfs"><a class="header" href="#why-vrfs">Why VRFs?</a></h2>
<p>We do assignments with VRFs to give &quot;enough&quot; checkers some meaning beyond merely &quot;expected&quot; checkers:</p>
<p>We could specify a protocol that used only system randomness, which works because our strongest defense is the expected number of honest checkers who assign themselves.  In this, adversaries could trivially flood their own blocks with their own checkers, so this strong defense becomes our only defense, and delay tranches become useless, so some blocks actually have zero approval checkers and possibly only one checker overall.</p>
<p>VRFs though require adversaries wait far longer between such attacks, which also helps against adversaries with little at stake because they compromised validators.  VRFs raise user confidence that no such &quot;drive by&quot; attacks occurred because the delay tranche system ensure at least some minimum number of approval checkers.  In this vein, VRFs permit reducing backing checks and increasing approval checks, which makes polkadot more efficient.</p>
<h2 id="gossip"><a class="header" href="#gossip">Gossip</a></h2>
<p>Any validator could send their assignment notices and/or approval votes too early.  We gossip the approval votes because they represent a major commitment by the validator.  We retain but delay gossiping the assignment notices until they agree with our local clock.</p>
<p>Assignment notices being gossiped too early might create a denial of service vector.  If so, we might exploit the relative time scheme that synchronises our clocks, which conceivably permits just dropping excessively early assignments. </p>
<h2 id="finality-grandpa-voting-rule"><a class="header" href="#finality-grandpa-voting-rule">Finality GRANDPA Voting Rule</a></h2>
<p>The relay-chain requires validators to participate in GRANDPA. In GRANDPA, validators submit off-chain votes on what they believe to be the best block of the chain, and GRANDPA determines the common block contained by a supermajority of sub-chains. There are also additional constraints on what can be submitted based on results of previous rounds of voting.</p>
<p>In order to avoid finalizing anything which has not received enough approval votes or is disputed, we will pair the approval protocol with an alteration to the GRANDPA voting strategy for honest nodes which causes them to vote only on chains where every parachain candidate within has been approved.  Furthermore, the voting rule prevents voting for chains where there is any live dispute or any dispute has resolved to a candidate being invalid.</p>
<p>Thus, the finalized relay-chain should contain only relay-chain blocks where a majority believe that every block within has been sufficiently approved.</p>
<h3 id="future-work"><a class="header" href="#future-work">Future work</a></h3>
<p>We could consider additional gossip messages with which nodes claims &quot;slow availability&quot; and/or &quot;slow candidate&quot; to fine tune the assignments &quot;no show&quot; system, but long enough &quot;no show&quot; delays suffice probably.</p>
<p>We shall develop more practical experience with UDP once the availability system works using direct UDP connections.  In this, we should discover if reconstruction performs adequately with a complete graphs or<br />
benefits from topology restrictions.  At this point, an assignment notices could implicitly request pieces from a random 1/3rd, perhaps topology restricted, which saves one gossip round.  If this preliminary fast reconstruction fails, then nodes' request alternative pieces directly.  There is an interesting design space in how this overlaps with &quot;slow availability&quot; claims.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="disputes"><a class="header" href="#disputes">Disputes</a></h1>
<p>Fast forward to <a href="./disputes-flow.html">more detailed disputes requirments</a>.</p>
<h2 id="motivation-and-background"><a class="header" href="#motivation-and-background">Motivation and Background</a></h2>
<p>All parachain blocks that end up in the finalized relay chain should be valid. This does not apply to blocks that are only backed, but not included.</p>
<p>We have two primary components for ensuring that nothing invalid ends up in the finalized relay chain:</p>
<ul>
<li>Approval Checking, as described <a href="./protocol-approval.html">here</a> and implemented according to the <a href="node/approval/approval-voting.html">Approval Voting</a> subsystem. This protocol can be shown to prevent invalid parachain blocks from making their way into the finalized relay chain as long as the amount of attempts are limited.</li>
<li>Disputes, this protocol, which ensures that each attempt to include something bad is caught, and the offending validators are punished.
Disputes differ from backing and approval process (and can not be part of those) in that a dispute is independent of a particular fork, while both backing and approval operate on particular forks. This distinction is important! Approval voting stops, if an alternative fork which might not contain the currently approved candidate gets finalized. This is totally fine from the perspective of approval voting as its sole purpose is to make sure invalid blocks won't get finalized. For disputes on the other hand we have different requirements: Even though the &quot;danger&quot; is past and the adversaries were not able to get their invalid block approved, we still want them to get slashed for the attempt. Otherwise they just have been able to get a free try, but this is something we need to avoid in our security model, as it is based on the assumption that the probability of getting an invalid block finalized is very low and an attacker would get bankrupt before it could have tried often enough.</li>
</ul>
<p>Every dispute stems from a disagreement among two or more validators. If a bad actor creates a bad block, but the bad actor never distributes it to honest validators, then nobody will dispute it. Of course, such a situation is not even an attack on the network, so we don't need to worry about defending against it.</p>
<p>From most to least important, here are the attack scenarios we are interested in identifying and deterring:</p>
<ul>
<li>A parablock included on a branch of the relay chain is bad</li>
<li>A parablock backed on a branch of the relay chain is bad</li>
<li>A parablock seconded, but not backed on any branch of the relay chain, is bad.</li>
</ul>
<p>As covered in the <a href="./protocol-overview.html">protocol overview</a>, checking a parachain block requires 3 pieces of data: the parachain validation code, the <a href="types/availability.html"><code>AvailableData</code></a>, and the <a href="types/candidate.html"><code>CandidateReceipt</code></a>. The validation code is available on-chain, and published ahead of time, so that no two branches of the relay chain have diverging views of the validation code for a given parachain. Note that only for the first scenario, where the parablock has been included on a branch of the relay chain, is the data necessarily available. Thus, dispute processes should begin with an availability process to ensure availability of the <code>AvailableData</code>. This availability process will conclude quickly if the data is already available. If the data is not already available, then the initiator of the dispute must make it available.</p>
<p>Disputes have both an on-chain and an off-chain component. Slashing and punishment is handled on-chain, so votes by validators on either side of the dispute must be placed on-chain. Furthermore, a dispute on one branch of the relay chain should be transposed to all other active branches of the relay chain. The fact that slashing occurs <em>in all histories</em> is crucial for deterring attempts to attack the network. The attacker should not be able to escape with their funds because the network has moved on to another branch of the relay chain where no attack was attempted.</p>
<p>In fact, this is why we introduce a distinction between <em>local</em> and <em>remote</em> disputes. We categorize disputes as either local or remote relative to any particular branch of the relay chain. Local disputes are about dealing with our first scenario, where a parablock has been included on the specific branch we are looking at. In these cases, the chain is corrupted all the way back to the point where the parablock was backed and must be discarded. However, as mentioned before, the dispute must propagate to all other branches of the relay chain. All other disputes are considered <em>remote</em>. For the on-chain component, when handling a dispute for a block which was not included in the current fork of the relay chain, it is impossible to discern between our attack scenarios. It is possible that the parablock was included somewhere, or backed somewhere, or wasn't backed anywhere. The on-chain component for handling these cases will be the same.</p>
<h2 id="initiation"><a class="header" href="#initiation">Initiation</a></h2>
<p>Disputes are initiated by any validator who finds their opinion on the validity of a parablock in opposition to another issued statement. As all statements currently gathered by the relay chain imply validity, disputes will be initiated only by nodes which perceive that the parablock is bad.</p>
<p>The initiation of a dispute begins off-chain. A validator signs a message indicating that it disputes the validity of the parablock and notifies all other validators, off-chain, of all of the statements it is aware of for the disputed parablock. These may be backing statements or approval-checking statements. It is worth noting that there is no special message type for initiating a dispute. It is the same message as is used to participate in a dispute and vote negatively. As such, there is no consensus required on who initiated a dispute, only on the fact that there is a dispute in-progress.</p>
<p>In practice, the initiator of a dispute will be either one of the backers or one of the approval checkers for the parablock. If the result of execution is found to be invalid, the validator will initiate the dispute as described above. Furthermore, if the dispute occurs during the backing phase, the initiator must make the data available to other validators. If the dispute occurs during approval checking, the data is already available.</p>
<p>Lastly, it is possible that for backing disputes, i.e. where the data is not already available among all validators, that an adversary may DoS the few parties who are checking the block to prevent them from distributing the data to other validators participating in the dispute process. Note that this can only occur pre-inclusion for any given parablock, so the downside of this attack is small and it is not security-critical to address these cases. However, we assume that the adversary can only prevent the validator from issuing messages for a limited amount of time. We also assume that there is a side-channel where the relay chain's governance mechanisms can trigger disputes by providing the full PoV and candidate receipt on-chain manually.</p>
<h2 id="dispute-participation"><a class="header" href="#dispute-participation">Dispute Participation</a></h2>
<p>Once becoming aware of a dispute, it is the responsibility of all validators to participate in the dispute. Concretely, this means:</p>
<ul>
<li>Circulate all statements about the candidate that we are aware of - backing statements, approval checking statements, and dispute statements.</li>
<li>If we have already issued any type of statement about the candidate, go no further.</li>
<li>Download the <a href="types/availability.html"><code>AvailableData</code></a>. If possible, this should first be attempted from other dispute participants or backing validators, and then <a href="node/availability/availability-recovery.html">(via erasure-coding)</a> from all validators.</li>
<li>Extract the Validation Code from any recent relay chain block. Code is guaranteed to be kept available on-chain, so we don't need to download any particular fork of the chain.</li>
<li>Execute the block under the validation code, using the <code>AvailableData</code>, and check that all outputs are correct, including the <code>erasure-root</code> of the <a href="types/candidate.html"><code>CandidateReceipt</code></a>.</li>
<li>Issue a dispute participation statement to the effect of the validity of the candidate block.</li>
</ul>
<p>Disputes <em>conclude</em> after ⅔ supermajority is reached in either direction.</p>
<p>The on-chain component of disputes can be initiated by providing any two conflicting votes and it also waits for a ⅔ supermajority on either side. The on-chain component also tracks which parablocks have already been disputed so the same parablock may only be disputed once on any particular branch of the relay chain. Lastly, it also tracks which blocks have been included on the current branch of the relay chain. When a dispute is initiated for a para, inclusion is halted for the para until the dispute concludes.</p>
<p>The author of a relay chain block should initiate the on-chain component of disputes for all disputes which the chain is not aware of, and provide all statements to the on-chain component as well. This should all be done via <em>inherents</em>.</p>
<p>Validators can learn about dispute statements in two ways:</p>
<ul>
<li>Receiving them from other validators over gossip</li>
<li>Scraping them from imported blocks of the relay chain. This is also used for validators to track other types of statements, such as backing statements.</li>
</ul>
<p>Validators are rewarded for providing statements to the chain as well as for participating in the dispute, on either side. However, the losing side of the dispute is slashed.</p>
<h2 id="dispute-conclusion"><a class="header" href="#dispute-conclusion">Dispute Conclusion</a></h2>
<p>Disputes, roughly, are over when one side reaches a ⅔ supermajority. They may also conclude after a timeout, without either side witnessing supermajority, which will only happen if the majority of validators are unable to vote for some reason. Furthermore, disputes on-chain will stay open for some fixed amount of time even after concluding, to accept new votes.</p>
<p>Late votes, after the dispute already reached a ⅔ supermajority, must be rewarded (albeit a smaller amount) as well.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="disputes-flows"><a class="header" href="#disputes-flows">Disputes Flows</a></h1>
<p>A component-free description in what-if form with addition state graphs of the dispute.</p>
<pre class="mermaid">stateDiagram-v2
    [*] --&gt; WaitForBackingVote: negative Vote received
    [*] --&gt; WaitForDisputeVote: backing Vote received
    WaitForBackingVote --&gt; Open: negative Vote received
    WaitForDisputeVote --&gt; Open: backing Vote received
    Open --&gt; Concluded: Timeout without supermajority
    Open --&gt; Concluded: Incoming Vote via Gossip
    Open --&gt; Open: No ⅔ supermajority
    Open --&gt; [*]
    Concluded --&gt; [*]
</pre>
<hr />
<pre class="mermaid">stateDiagram-v2
    [*] --&gt; Open: First Vote(s) received
    Open --&gt; HasPoV : Fetch Availability Store for PoV

    HasPoV --&gt; HasCode : Fetch historical Code
    HasCode --&gt; VerifyWithRuntime: All Data locally avail

    Open --&gt; DisputeAvailabilityDataReceived
    DisputeAvailabilityDataReceived --&gt; VerifyWithRuntime: Received Gossip

    HasPoV --&gt; RequestDisputeAvailabilityData: nope
    HasCode --&gt; RequestDisputeAvailabilityData: nope
    RequestDisputeAvailabilityData --&gt; VerifyWithRuntime: Received
    RequestDisputeAvailabilityData --&gt; RequestDisputeAvailabilityData: Timed out - pick another peer

    VerifyWithRuntime --&gt; CastVoteValid: Block Valid
    VerifyWithRuntime --&gt; CastVoteInvalid: Block Invalid
    CastVoteInvalid --&gt; GossipVote
    CastVoteValid --&gt; GossipVote
    GossipVote --&gt; [*]

</pre>
<hr />
<p>Dispute Availability Data</p>
<pre class="mermaid">stateDiagram-v2
    [*] --&gt; Open: First Vote(s) received
    Open --&gt; DisputeDataAvail: somehow the data became available
    Open --&gt; RespondUnavailable: Data not available
    IncomingRequestDisputeAvailabilityData --&gt; RespondUnavailable
    IncomingRequestDisputeAvailabilityData --&gt; DisputeDataAvail
    DisputeDataAvail --&gt; RespondWithDisputeAvailabilityData: Send
    VoteGossipReceived --&gt; Track: implies source peer has&lt;br /&gt;dispute availablity data
</pre>
<hr />
<p>Peer handling</p>
<pre class="mermaid">stateDiagram-v2
    [*] --&gt; Open: First Vote(s) received
    Open --&gt; GossipVotes: for all current peers
    Open --&gt; PeerConnected: another
    PeerConnected --&gt; GossipVotes: Peer connects
    GossipVotes --&gt; [*]
</pre>
<h2 id="conditional-formulation"><a class="header" href="#conditional-formulation">Conditional formulation</a></h2>
<p>The set of validators eligible to vote consists of
the validators that had duty at the time of backing, plus backing votes by the backing validators.</p>
<p>If a validator receives an initial dispute message (a set of votes where there are at least two opposing votes contained), and the PoV or Code are hence not reconstructable from local storage, that validator must request the required data from its peers.</p>
<p>The dispute availability message must contain code, persisted validation data, and the proof of validity.</p>
<p>Only peers that already voted shall be queried for the dispute availability data.</p>
<p>The peer to be queried for disputes data, must be picked at random.</p>
<p>A validator must retain code, persisted validation data and PoV until a block, that contains the dispute resolution, is finalized - plus an additional 24h.</p>
<p>Dispute availability gossip must continue beyond the dispute resolution, until the post resolution timeout expired (equiv to the timeout until which additional late votes are accepted).</p>
<p>Remote disputes are disputes that are in relation to a chain that is not part of the local validators active heads.</p>
<p>All incoming votes must be persisted.</p>
<p>Persisted votes stay persisted for <code>N</code> sessions, and are cleaned up on a per session basis.</p>
<p>Votes must be queryable by a particular validator, identified by its signing key.</p>
<p>Votes must be queryable by a particular validator, identified by a session index and the validator index valid in that session.</p>
<p>If there exists a negative and a positive fork for a particular block, a dispute is detected.</p>
<p>If a dispute is detected, all currently available votes for that block must be gossiped.</p>
<p>If an incoming dispute vote is detected, a validator must cast their own vote. The vote is determined by validating the PoV with the Code at the time of backing the block in question.</p>
<p>If the validator was also a backer of the block, validation and casting an additional vote should be skipped.</p>
<p>If the count of votes pro or cons regarding the disputed block, reaches the required ⅔ supermajority (including the backing votes), the conclusion must be recorded on chain and the voters on the loosing and no-shows being slashed appropriately.</p>
<p>If a block is found invalid by a dispute resolution, it must be blacklisted to avoid resync or further build on that chain if other chains are available (to be detailed in the grandpa fork choice rule).</p>
<p>A dispute accepts Votes after the dispute is resolved, for 1d.</p>
<p>If a vote is received, after the dispute is resolved, the vote shall still be recorded in the state root, albeit yielding less reward.</p>
<p>Recording in the state root might happen batched, at timeout expiry.</p>
<p>If a new active head/chain appears, and the dispute resolution was not recorded on that chain yet, the dispute resolution or open dispute must be recorded / transplanted to that chain as well, since the disputes must be present on all chains to make sure the offender is punished.</p>
<p>If a validator votes in two opposing ways, this composes of a double vote like in other cases (backing, approval voting).</p>
<p>If a dispute is not resolved within due time, all validators are to be slashed for a small amount.</p>
<p>If a dispute is not resolved within due time, governance mode shall be entered for manual resolution.</p>
<p>If a validator unexpectedly restarts, the dispute shall be continued with the state based on votes being cast and being present in persistent storage.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chain-selection"><a class="header" href="#chain-selection">Chain Selection</a></h1>
<p>Chain selection processes in blockchains are used for the purpose of selecting blocks to build on and finalize. It is important for these processes to be consistent among nodes and resilient to a maximum proportion of malicious nodes which do not obey the chain selection process.</p>
<p>The parachain host uses both a block authoring system and a finality gadget. The chain selection strategy of the parachain host involves two key components: a <em>leaf-selection</em> rule and a set of <em>finality constraints</em>. When it's a validator's turn to author on a block, they are expected to select the best block via the leaf-selection rule to build on top of. When a validator is participating in finality, there is a minimum block which can be voted on, which is usually the finalized block. The validator should select the best chain according to the leaf-selection rule and subsequently apply the finality constraints to arrive at the actual vote cast by that validator.</p>
<p>Before diving into the particularities of the leaf-selection rule and the finality constraints, it's important to discuss the goals that these components are meant to achieve. For this it is useful to create the definitions of <em>viable</em> and <em>finalizable</em> blocks.</p>
<h3 id="property-definitions"><a class="header" href="#property-definitions">Property Definitions</a></h3>
<p>A block is considered <strong>viable</strong> when all of the following hold:</p>
<ol>
<li>It is or descends from the finalized block</li>
<li>It is not <strong>stagnant</strong></li>
<li>It is not <strong>reverted</strong>.</li>
</ol>
<p>A block is considered a <strong>viable leaf</strong> when all of the following hold:</p>
<ol>
<li>It is <strong>viable</strong></li>
<li>It has no <strong>viable</strong> descendant.</li>
</ol>
<p>A block is considered <strong>stagnant</strong> when either:</p>
<ol>
<li>It is unfinalized, is not approved, and has not been approved within 2 minutes</li>
<li>Its parent is <strong>stagnant</strong>.</li>
</ol>
<p>A block is considered <strong>reverted</strong> when either:</p>
<ol>
<li>It is unfinalized and includes a candidate which has lost a dispute</li>
<li>Its parent is <strong>reverted</strong></li>
</ol>
<p>A block is considered <strong>finalizable</strong> when all of the following hold:</p>
<ol>
<li>It is <strong>viable</strong></li>
<li>Its parent, if unfinalized, is <strong>finalizable</strong>.</li>
<li>It is either finalized or approved.</li>
<li>It is either finalized or includes no candidates which have unresolved disputes or have lost a dispute.</li>
</ol>
<h3 id="the-leaf-selection-rule"><a class="header" href="#the-leaf-selection-rule">The leaf-selection rule</a></h3>
<p>We assume that every block has an implicit weight or score which can be used to compare blocks. In BABE, this is determined by the number of primary slots included in the chain. In PoW, this is the chain with either the most work or GHOST weight.</p>
<p>The leaf-selection rule based on our definitions above is simple: we take the maximum-scoring viable leaf we are aware of. In the case of a tie we select the one with a lower lexicographical block hash.</p>
<h3 id="the-best-chain-containing-rule"><a class="header" href="#the-best-chain-containing-rule">The best-chain-containing rule</a></h3>
<p>Finality gadgets, as mentioned above, will often impose an additional requirement to vote on a chain containing a specific block, known as the <strong>required</strong> block. Although this is typically the most recently finalized block, it is possible that it may be a block that is unfinalized. When receiving such a request:</p>
<ol>
<li>If the required block is the best finalized block, then select the best viable leaf.</li>
<li>If the required block is unfinalized and non-viable, then select the required block and go no further. This is likely an indication that something bad will be finalized in the network, which will never happen when approvals &amp; disputes are functioning correctly. Nevertheless we account for the case here.</li>
<li>If the required block is unfinalized and non-viable, then iterate over the viable leaves in descending order by score and select the first one which contains the required block in its chain. Backwards iteration is a simple way to check this, but if unfinalized chains grow long then Merkle Mountain-Ranges will most likely be more efficient.</li>
</ol>
<p>Once selecting a leaf, the chain should be constrained to the maximum of the required block or the highest <strong>finalizable</strong> ancestor.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="architecture-overview"><a class="header" href="#architecture-overview">Architecture Overview</a></h1>
<p>This section aims to describe, at a high level, the code architecture and subsystems involved in the implementation of an individual Parachain Host. It also illuminates certain subtleties and challenges faced in the design and implementation of those subsystems.</p>
<p>To recap, Polkadot includes a blockchain known as the relay-chain. A blockchain is a Directed Acyclic Graph (DAG) of state transitions, where every block can be considered to be the head of a linked-list (known as a &quot;chain&quot; or &quot;fork&quot;) with a cumulative state which is determined by applying the state transition of each block in turn. All paths through the DAG terminate at the Genesis Block. In fact, the blockchain is a tree, since each block can have only one parent.</p>
<p><img src="architecture_overview_0.generated.svg" alt="" /></p>
<p>A blockchain network is comprised of nodes. These nodes each have a view of many different forks of a blockchain and must decide which forks to follow and what actions to take based on the forks of the chain that they are aware of.</p>
<p>So in specifying an architecture to carry out the functionality of a Parachain Host, we have to answer two categories of questions:</p>
<ol>
<li>What is the state-transition function of the blockchain? What is necessary for a transition to be considered valid, and what information is carried within the implicit state of a block?</li>
<li>Being aware of various forks of the blockchain as well as global private state such as a view of the current time, what behaviors should a node undertake? What information should a node extract from the state of which forks, and how should that information be used?</li>
</ol>
<p>The first category of questions will be addressed by the Runtime, which defines the state-transition logic of the chain. Runtime logic only has to focus on the perspective of one chain, as each state has only a single parent state.</p>
<p>The second category of questions addressed by Node-side behavior. Node-side behavior defines all activities that a node undertakes, given its view of the blockchain/block-DAG. Node-side behavior can take into account all or many of the forks of the blockchain, and only conditionally undertake certain activities based on which forks it is aware of, as well as the state of the head of those forks.</p>
<p><img src="architecture_overview_1.generated.svg" alt="" /></p>
<p>It is also helpful to divide Node-side behavior into two further categories: Networking and Core. Networking behaviors relate to how information is distributed between nodes. Core behaviors relate to internal work that a specific node does. These two categories of behavior often interact, but can be heavily abstracted from each other. Core behaviors care that information is distributed and received, but not the internal details of how distribution and receipt function. Networking behaviors act on requests for distribution or fetching of information, but are not concerned with how the information is used afterwards. This allows us to create clean boundaries between Core and Networking activities, improving the modularity of the code.</p>
<pre><code class="language-text">          ___________________                    ____________________
         /       Core        \                  /     Networking     \
         |                   |  Send &quot;Hello&quot;    |                    |
         |                   |-  to &quot;foo&quot;   ---&gt;|                    |
         |                   |                  |                    |
         |                   |                  |                    |
         |                   |                  |                    |
         |                   |    Got &quot;World&quot;   |                    |
         |                   |&lt;--  from &quot;bar&quot; --|                    |
         |                   |                  |                    |
         \___________________/                  \____________________/
                                                   ______| |______
                                                   ___Transport___

</code></pre>
<p>Node-side behavior is split up into various subsystems. Subsystems are long-lived workers that perform a particular category of work. Subsystems can communicate with each other, and do so via an <a href="node/overseer.html">Overseer</a> that prevents race conditions.</p>
<p>Runtime logic is divided up into Modules and APIs. Modules encapsulate particular behavior of the system. Modules consist of storage, routines, and entry-points. Routines are invoked by entry points, by other modules, upon block initialization or closing. Routines can read and alter the storage of the module. Entry-points are the means by which new information is introduced to a module and can limit the origins (user, root, parachain) that they accept being called by. Each block in the blockchain contains a set of Extrinsics. Each extrinsic targets a a specific entry point to trigger and which data should be passed to it. Runtime APIs provide a means for Node-side behavior to extract meaningful information from the state of a single fork.</p>
<p>These two aspects of the implementation are heavily dependent on each other. The Runtime depends on Node-side behavior to author blocks, and to include Extrinsics which trigger the correct entry points. The Node-side behavior relies on Runtime APIs to extract information necessary to determine which actions to take.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="messaging-overview"><a class="header" href="#messaging-overview">Messaging Overview</a></h1>
<p>The Polkadot Host has a few mechanisms that are responsible for message passing. They can be generally divided
on two categories: Horizontal and Vertical. Horizontal Message Passing (HMP) refers to mechanisms
that are responsible for exchanging messages between parachains. Vertical Message Passing (VMP) is
used for communication between the relay chain and parachains.</p>
<h2 id="vertical-message-passing"><a class="header" href="#vertical-message-passing">Vertical Message Passing</a></h2>
<p><img src="messaging_overview_0.generated.svg" alt="" /></p>
<p>Downward Message Passing (DMP) is a mechanism for delivering messages to parachains from the relay chain.</p>
<p>Each parachain has its own queue that stores all pending inbound downward messages. A parachain
doesn't have to process all messages at once, however, there are rules as to how the downward message queue
should be processed. Currently, at least one message must be consumed per candidate if the queue is not empty.
The downward message queue doesn't have a cap on its size and it is up to the relay-chain to put mechanisms
that prevent spamming in place.</p>
<p>Upward Message Passing (UMP) is a mechanism responsible for delivering messages in the opposite direction:
from a parachain up to the relay chain. Upward messages are essentially byte blobs. However, they are interpreted
by the relay-chain according to the XCM standard.</p>
<p>The XCM standard is a common vocabulary of messages. The XCM standard doesn't require a particular interpretation of
a message. However, the parachains host (e.g. Polkadot) guarantees certain semantics for those.</p>
<p>Moreover, while most XCM messages are handled by the on-chain XCM interpreter, some of the messages are special
cased. Specifically, those messages can be checked during the acceptance criteria and thus invalid
messages would lead to rejecting the candidate itself.</p>
<p>One kind of such a message is <code>Xcm::Transact</code>. This upward message can be seen as a way for a parachain
to execute arbitrary entrypoints on the relay-chain. <code>Xcm::Transact</code> messages resemble regular extrinsics with the exception that they
originate from a parachain.</p>
<p>The payload of <code>Xcm::Transact</code> messages is referred as to <code>Dispatchable</code>. When a candidate with such a message is enacted
the dispatchables are put into a queue corresponding to the parachain. There can be only so many dispatchables in that queue at once.
The weight that processing of the dispatchables can consume is limited by a preconfigured value. Therefore, it is possible
that some dispatchables will be left for later blocks. To make the dispatching more fair, the queues are processed turn-by-turn
in a round robin fashion.</p>
<p>The second category of special cased XCM messages are for horizontal messaging channel management,
namely messages meant to request opening and closing HRMP channels (HRMP will be described below).</p>
<h2 id="horizontal-message-passing"><a class="header" href="#horizontal-message-passing">Horizontal Message Passing</a></h2>
<p><img src="messaging_overview_1.generated.svg" alt="" /></p>
<h3 id="cross-chain-message-passing"><a class="header" href="#cross-chain-message-passing">Cross-Chain Message Passing</a></h3>
<p>The most important member of this family is XCMP.</p>
<blockquote>
<p>ℹ️ XCMP is currently under construction and details are subject for change.</p>
</blockquote>
<p>XCMP is a message passing mechanism between parachains that require minimal involvement of the relay chain.
The relay chain provides means for sending parachains to authenticate messages sent to recipient parachains.</p>
<p>Semantically communication occurs through so called channels. A channel is unidirectional and it has
two endpoints, for sender and for recipient. A channel can be opened only if the both parties agree
and closed unilaterally.</p>
<p>Only the channel metadata is stored on the relay-chain in a very compact form: all messages and their
contents sent by the sender parachain are encoded using only one root hash. This root is referred as
MQC head.</p>
<p>The authenticity of the messages must be proven using that root hash to the receiving party at the
candidate authoring time. The proof stems from the relay parent storage that contains the root hash of the channel.
Since not all messages are required to be processed by the receiver's candidate, only the processed
messages are supplied (i.e. preimages), rest are provided as hashes.</p>
<p>Further details can be found at the official repository for the
<a href="https://github.com/paritytech/xcm-format/blob/master/README.md">Cross-Consensus Message Format (XCM)</a>, as well as
at the <a href="https://research.web3.foundation/en/latest/polkadot/XCMP.html">W3F research website</a> and
<a href="https://medium.com/web3foundation/polkadots-messaging-scheme-b1ec560908b7">this blogpost</a>.</p>
<p>HRMP (Horizontally Relay-routed Message Passing) is a stop gap that predates XCMP. Semantically, it mimics XCMP's interface.
The crucial difference from XCMP though is that all the messages are stored in the relay-chain storage. That makes
things simple but at the same time that makes HRMP more demanding in terms of resources thus making it more expensive.</p>
<p>Once XCMP is available we expect to retire HRMP.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="runtime-architecture"><a class="header" href="#runtime-architecture">Runtime Architecture</a></h1>
<p>It's clear that we want to separate different aspects of the runtime logic into different modules. Modules define their own storage, routines, and entry-points. They also define initialization and finalization logic.</p>
<p>Due to the (lack of) guarantees provided by a particular blockchain-runtime framework, there is no defined or dependable order in which modules' initialization or finalization logic will run. Supporting this blockchain-runtime framework is important enough to include that same uncertainty in our model of runtime modules in this guide. Furthermore, initialization logic of modules can trigger the entry-points or routines of other modules. This is one architectural pressure against dividing the runtime logic into multiple modules. However, in this case the benefits of splitting things up outweigh the costs, provided that we take certain precautions against initialization and entry-point races.</p>
<p>We also expect, although it's beyond the scope of this guide, that these runtime modules will exist alongside various other modules. This has two facets to consider. First, even if the modules that we describe here don't invoke each others' entry points or routines during initialization, we still have to protect against those other modules doing that. Second, some of those modules are expected to provide governance capabilities for the chain. Configuration exposed by parachain-host modules is mostly for the benefit of these governance modules, to allow the operators or community of the chain to tweak parameters.</p>
<p>The runtime's primary roles to manage scheduling and updating of parachains and parathreads, as well as handling misbehavior reports and slashing. This guide doesn't focus on how parachains or parathreads are registered, only that they are. Also, this runtime description assumes that validator sets are selected somehow, but doesn't assume any other details than a periodic <em>session change</em> event. Session changes give information about the incoming validator set and the validator set of the following session.</p>
<p>The runtime also serves another role, which is to make data available to the Node-side logic via Runtime APIs. These Runtime APIs should be sufficient for the Node-side code to author blocks correctly.</p>
<p>There is some functionality of the relay chain relating to parachains that we also consider beyond the scope of this document. In particular, all modules related to how parachains are registered aren't part of this guide, although we do provide routines that should be called by the registration process.</p>
<p>We will split the logic of the runtime up into these modules:</p>
<ul>
<li>Initializer: manage initialization order of the other modules.</li>
<li>Shared: manages shared storage and configurations for other modules.</li>
<li>Configuration: manage configuration and configuration updates in a non-racy manner.</li>
<li>Paras: manage chain-head and validation code for parachains and parathreads.</li>
<li>Scheduler: manages parachain and parathread scheduling as well as validator assignments.</li>
<li>Inclusion: handles the inclusion and availability of scheduled parachains and parathreads.</li>
<li>Validity: handles secondary checks and dispute resolution for included, available parablocks.</li>
<li>Hrmp: handles horizontal messages between paras.</li>
<li>Ump: Handles upward messages from a para to the relay chain.</li>
<li>Dmp: Handles downward messages from the relay chain to the para.</li>
</ul>
<p>The <a href="runtime/initializer.html">Initializer module</a> is special - it's responsible for handling the initialization logic of the other modules to ensure that the correct initialization order and related invariants are maintained. The other modules won't specify a on-initialize logic, but will instead expose a special semi-private routine that the initialization module will call. The other modules are relatively straightforward and perform the roles described above.</p>
<p>The Parachain Host operates under a changing set of validators. Time is split up into periodic sessions, where each session brings a potentially new set of validators. Sessions are buffered by one, meaning that the validators of the upcoming session <code>n+1</code> are determined at the end of session <code>n-1</code>, right before session <code>n</code> starts. Parachain Host runtime modules need to react to changes in the validator set, as it will affect the runtime logic for processing candidate backing, availability bitfields, and misbehavior reports. The Parachain Host modules can't determine ahead-of-time exactly when session change notifications are going to happen within the block (note: this depends on module initialization order again - better to put session before parachains modules).</p>
<p>The relay chain is intended to use BABE or SASSAFRAS, which both have the property that a session changing at a block is determined not by the number of the block but instead by the time the block is authored. In some sense, sessions change in-between blocks, not at blocks. This has the side effect that the session of a child block cannot be determined solely by the parent block's identifier. Being able to unilaterally determine the validator-set at a specific block based on its parent hash would make a lot of Node-side logic much simpler.</p>
<p>In order to regain the property that the validator set of a block is predictable by its parent block, we delay session changes' application to Parachains by 1 block. This means that if there is a session change at block X, that session change will be stored and applied during initialization of direct descendents of X. This principal side effect of this change is that the Parachains runtime can disagree with session or consensus modules about which session it currently is. Misbehavior reporting routines in particular will be affected by this, although not severely. The parachains runtime might believe it is the last block of the session while the system is really in the first block of the next session. In such cases, a historical validator-set membership proof will need to accompany any misbehavior report, although they typically do not need to during current-session misbehavior reports.</p>
<p>So the other role of the initializer module is to forward session change notifications to modules in the initialization order. Session change is also the point at which the <a href="runtime/configuration.html">Configuration Module</a> updates the configuration. Most of the other modules will handle changes in the configuration during their session change operation, so the initializer should provide both the old and new configuration to all the other
modules alongside the session change notification. This means that a session change notification should consist of the following data:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct SessionChangeNotification {
 // The new validators in the session.
 validators: Vec&lt;ValidatorId&gt;,
 // The validators for the next session.
 queued: Vec&lt;ValidatorId&gt;,
 // The configuration before handling the session change.
 prev_config: HostConfiguration,
 // The configuration after handling the session change.
 new_config: HostConfiguration,
 // A secure randomn seed for the session, gathered from BABE.
 random_seed: [u8; 32],
 // The session index of the beginning session.
 session_index: SessionIndex,
}
<span class="boring">}
</span></code></pre></pre>
<blockquote>
<p>TODO Diagram: order of runtime operations (initialization, session change)</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="initializer-module"><a class="header" href="#initializer-module">Initializer Module</a></h1>
<p>This module is responsible for initializing the other modules in a deterministic order. It also has one other purpose as described in the overview of the runtime: accepting and forwarding session change notifications.</p>
<h2 id="storage"><a class="header" href="#storage">Storage</a></h2>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>HasInitialized: bool;
// buffered session changes along with the block number at which they should be applied.
//
// typically this will be empty or one element long. ordered ascending by BlockNumber and insertion
// order.
BufferedSessionChanges: Vec&lt;(BlockNumber, ValidatorSet, ValidatorSet)&gt;;
<span class="boring">}
</span></code></pre></pre>
<h2 id="initialization"><a class="header" href="#initialization">Initialization</a></h2>
<p>Before initializing modules, remove all changes from the <code>BufferedSessionChanges</code> with number less than or equal to the current block number, and apply the last one. The session change is applied to all modules in the same order as initialization.</p>
<p>The other parachains modules are initialized in this order:</p>
<ol>
<li>Configuration</li>
<li>Shared</li>
<li>Paras</li>
<li>Scheduler</li>
<li>Inclusion</li>
<li>SessionInfo</li>
<li>Disputes</li>
<li>DMP</li>
<li>UMP</li>
<li>HRMP</li>
</ol>
<p>The <a href="runtime/configuration.html">Configuration Module</a> is first, since all other modules need to operate under the same configuration as each other. Then the <a href="runtime/shared.html">Shared</a> module is invoked, which determines the set of active validators. It would lead to inconsistency if, for example, the scheduler ran first and then the configuration was updated before the Inclusion module.</p>
<p>Set <code>HasInitialized</code> to true.</p>
<h2 id="session-change"><a class="header" href="#session-change">Session Change</a></h2>
<p>Store the session change information in <code>BufferedSessionChange</code> along with the block number at which it was submitted, plus one. Although the expected operational parameters of the block authorship system should prevent more than one change from being buffered at any time, it may occur. Regardless, we always need to track the block number at which the session change can be applied so as to remain flexible over session change notifications being issued before or after initialization of the current block.</p>
<h2 id="finalization"><a class="header" href="#finalization">Finalization</a></h2>
<p>Finalization order is less important in this case than initialization order, so we finalize the modules in the reverse order from initialization.</p>
<p>Set <code>HasInitialized</code> to false.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="configuration-module"><a class="header" href="#configuration-module">Configuration Module</a></h1>
<p>This module is responsible for managing all configuration of the parachain host in-flight. It provides a central point for configuration updates to prevent races between configuration changes and parachain-processing logic. Configuration can only change during the session change routine, and as this module handles the session change notification first it provides an invariant that the configuration does not change throughout the entire session. Both the <a href="runtime/scheduler.html">scheduler</a> and <a href="runtime/inclusion.html">inclusion</a> modules rely on this invariant to ensure proper behavior of the scheduler.</p>
<p>The configuration that we will be tracking is the <a href="runtime/../types/runtime.html#host-configuration"><code>HostConfiguration</code></a> struct.</p>
<h2 id="storage-1"><a class="header" href="#storage-1">Storage</a></h2>
<p>The configuration module is responsible for two main pieces of storage.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// The current configuration to be used.
Configuration: HostConfiguration;
/// A pending configuration to be applied on session change.
PendingConfiguration: Option&lt;HostConfiguration&gt;;
<span class="boring">}
</span></code></pre></pre>
<h2 id="session-change-1"><a class="header" href="#session-change-1">Session change</a></h2>
<p>The session change routine for the Configuration module is simple. If the <code>PendingConfiguration</code> is <code>Some</code>, take its value and set <code>Configuration</code> to be equal to it. Reset <code>PendingConfiguration</code> to <code>None</code>.</p>
<h2 id="routines"><a class="header" href="#routines">Routines</a></h2>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Get the host configuration.
pub fn configuration() -&gt; HostConfiguration {
  Configuration::get()
}

/// Updating the pending configuration to be applied later.
fn update_configuration(f: impl FnOnce(&amp;mut HostConfiguration)) {
  PendingConfiguration::mutate(|pending| {
    let mut x = pending.unwrap_or_else(Self::configuration);
    f(&amp;mut x);
    *pending = Some(x);
  })
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="entry-points"><a class="header" href="#entry-points">Entry-points</a></h2>
<p>The Configuration module exposes an entry point for each configuration member. These entry-points accept calls only from governance origins. These entry-points will use the <code>update_configuration</code> routine to update the specific configuration field.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="shared-module"><a class="header" href="#shared-module">Shared Module</a></h1>
<p>This module is responsible for managing shared storage and configuration for other modules.</p>
<p>It is important that other pallets are able to use the Shared Module, so it should not have a
dependency on any other modules in the Parachains Runtime.</p>
<p>For the moment, it is used exclusively to track the current session index across the Parachains
Runtime system, and when it should be allowed to schedule future changes to Paras or Configurations.</p>
<h2 id="constants"><a class="header" href="#constants">Constants</a></h2>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// `SESSION_DELAY` is used to delay any changes to Paras registration or configurations.
// Wait until the session index is 2 larger then the current index to apply any changes,
// which guarantees that at least one full session has passed before any changes are applied.
pub(crate) const SESSION_DELAY: SessionIndex = 2;
<span class="boring">}
</span></code></pre></pre>
<h2 id="storage-2"><a class="header" href="#storage-2">Storage</a></h2>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// The current session index within the Parachains Runtime system.
CurrentSessionIndex: SessionIndex;
/// All the validators actively participating in parachain consensus.
/// Indices are into the broader validator set.
ActiveValidatorIndices: Vec&lt;ValidatorIndex&gt;,
/// The parachain attestation keys of the validators actively participating in parachain consensus.
/// This should be the same length as `ActiveValidatorIndices`.
ActiveValidatorKeys: Vec&lt;ValidatorId&gt;
<span class="boring">}
</span></code></pre></pre>
<h2 id="initialization-1"><a class="header" href="#initialization-1">Initialization</a></h2>
<p>The Shared Module currently has no initialization routines.</p>
<p>The Shared Module is initialized directly after the Configuration module, but before all other
modules. It is important to update the Shared Module before any other module since its state may be
used within the logic of other modules, and it is important that the state is consistent across
them.</p>
<h2 id="session-change-2"><a class="header" href="#session-change-2">Session Change</a></h2>
<p>During a session change, the Shared Module receives and stores the current Session Index directly from the initializer module, along with the broader validator set, and it returns the new list of validators.</p>
<p>The list of validators should be first shuffled according to the chain's random seed and then truncated. The indices of these validators should be set to <code>ActiveValidatorIndices</code> and then returned back to the initializer. <code>ActiveValidatorKeys</code> should be set accordingly.</p>
<p>This information is used in the:</p>
<ul>
<li>Configuration Module: For delaying updates to configurations until at lease one full session has
passed.</li>
<li>Paras Module: For delaying updates to paras until at least one full session has passed.</li>
</ul>
<h2 id="finalization-1"><a class="header" href="#finalization-1">Finalization</a></h2>
<p>The Shared Module currently has no finalization routines.</p>
<h2 id="functions"><a class="header" href="#functions">Functions</a></h2>
<ul>
<li><code>scheduled_sessions() -&gt; SessionIndex</code>: Return the next session index where updates to the
Parachains Runtime system would be safe to apply.</li>
<li><code>set_session_index(SessionIndex)</code>: For tests. Set the current session index in the Shared Module.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="disputes-module"><a class="header" href="#disputes-module">Disputes Module</a></h1>
<p>After a backed candidate is made available, it is included and proceeds into an acceptance period during which validators are randomly selected to do (secondary) approval checks of the parablock. Any reports disputing the validity of the candidate will cause escalation, where even more validators are requested to check the block, and so on, until either the parablock is determined to be invalid or valid. Those on the wrong side of the dispute are slashed and, if the parablock is deemed invalid, the relay chain is rolled back to a point before that block was included.</p>
<p>However, this isn't the end of the story. We are working in a forkful blockchain environment, which carries three important considerations:</p>
<ol>
<li>For security, validators that misbehave shouldn't only be slashed on one fork, but on all possible forks. Validators that misbehave shouldn't be able to create a new fork of the chain when caught and get away with their misbehavior.</li>
<li>It is possible (and likely) that the parablock being contested has not appeared on all forks.</li>
<li>If a block author believes that there is a disputed parablock on a specific fork that will resolve to a reversion of the fork, that block author is better incentivized to build on a different fork which does not include that parablock.</li>
</ol>
<p>This means that in all likelihood, there is the possibility of disputes that are started on one fork of the relay chain, and as soon as the dispute resolution process starts to indicate that the parablock is indeed invalid, that fork of the relay chain will be abandoned and the dispute will never be fully resolved on that chain.</p>
<p>Even if this doesn't happen, there is the possibility that there are two disputes underway, and one resolves leading to a reversion of the chain before the other has concluded. In this case we want to both transplant the concluded dispute onto other forks of the chain as well as the unconcluded dispute.</p>
<p>We account for these requirements by having the disputes module handle two kinds of disputes.</p>
<ol>
<li>Local disputes: those contesting the validity of the current fork by disputing a parablock included within it.</li>
<li>Remote disputes: a dispute that has partially or fully resolved on another fork which is transplanted to the local fork for completion and eventual slashing.</li>
</ol>
<p>When a local dispute concludes negatively, the chain needs to be abandoned and reverted back to a block where the state does not contain the bad parablock. We expect that due to the <a href="runtime/../protocol-approval.html">Approval Checking Protocol</a>, the current executing block should not be finalized. So we do two things when a local dispute concludes negatively:</p>
<ol>
<li>Freeze the state of parachains so nothing further is backed or included.</li>
<li>Issue a digest in the header of the block that signals to nodes that this branch of the chain is to be abandoned.</li>
</ol>
<p>If, as is expected, the chain is unfinalized, the freeze will have no effect as no honest validator will attempt to build on the frozen chain. However, if the approval checking protocol has failed and the bad parablock is finalized, the freeze serves to put the chain into a governance-only mode.</p>
<p>The storage of this module is designed around tracking <a href="runtime/../types/disputes.html#disputestate"><code>DisputeState</code>s</a>, updating them with votes, and tracking blocks included by this branch of the relay chain. It also contains a <code>Frozen</code> parameter designed to freeze the state of all parachains.</p>
<h2 id="storage-3"><a class="header" href="#storage-3">Storage</a></h2>
<p>Storage Layout:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>LastPrunedSession: Option&lt;SessionIndex&gt;,

// All ongoing or concluded disputes for the last several sessions.
Disputes: double_map (SessionIndex, CandidateHash) -&gt; Option&lt;DisputeState&gt;,
// All included blocks on the chain, as well as the block number in this chain that
// should be reverted back to if the candidate is disputed and determined to be invalid.
Included: double_map (SessionIndex, CandidateHash) -&gt; Option&lt;BlockNumber&gt;,
// Maps session indices to a vector indicating the number of potentially-spam disputes 
// each validator is participating in. Potentially-spam disputes are remote disputes which have
// fewer than `byzantine_threshold + 1` validators.
//
// The i'th entry of the vector corresponds to the i'th validator in the session.
SpamSlots: map SessionIndex -&gt; Vec&lt;u32&gt;,
// Whether the chain is frozen or not. Starts as `false`. When this is `true`,
// the chain will not accept any new parachain blocks for backing or inclusion.
// It can only be set back to `false` by governance intervention.
Frozen: bool,
<span class="boring">}
</span></code></pre></pre>
<blockquote>
<p><code>byzantine_threshold</code> refers to the maximum number <code>f</code> of validators which may be byzantine. The total number of validators is <code>n = 3f + e</code> where <code>e in { 1, 2, 3 }</code>.</p>
</blockquote>
<h2 id="session-change-3"><a class="header" href="#session-change-3">Session Change</a></h2>
<ol>
<li>If the current session is not greater than <code>config.dispute_period + 1</code>, nothing to do here.</li>
<li>Set <code>pruning_target = current_session - config.dispute_period - 1</code>. We add the extra <code>1</code> because we want to keep things for <code>config.dispute_period</code> <em>full</em> sessions. The stuff at the end of the most recent session has been around for ~0 sessions, not ~1.</li>
<li>If <code>LastPrunedSession</code> is <code>None</code>, then set <code>LastPrunedSession</code> to <code>Some(pruning_target)</code> and return.</li>
<li>Otherwise, clear out all disputes, included candidates, and <code>SpamSlots</code> entries in the range <code>last_pruned..=pruning_target</code> and set <code>LastPrunedSession</code> to <code>Some(pruning_target)</code>.</li>
</ol>
<h2 id="block-initialization"><a class="header" href="#block-initialization">Block Initialization</a></h2>
<ol>
<li>Iterate through all disputes. If any have not concluded and started more than <code>config.dispute_conclusion_by_timeout_period</code> blocks ago, set them to <code>Concluded</code> and mildly punish all validators associated, as they have failed to distribute available data. If the <code>Included</code> map does not contain the candidate and there are fewer than <code>byzantine_threshold + 1</code> participating validators, reduce <code>SpamSlots</code> for all participating validators.</li>
</ol>
<h2 id="routines-1"><a class="header" href="#routines-1">Routines</a></h2>
<ul>
<li>
<p><code>provide_multi_dispute_data(MultiDisputeStatementSet) -&gt; Vec&lt;(SessionIndex, Hash)&gt;</code>:</p>
<ol>
<li>Fail if any disputes in the set are duplicate or concluded before the <code>config.dispute_post_conclusion_acceptance_period</code> window relative to now.</li>
<li>Pass on each dispute statement set to <code>provide_dispute_data</code>, propagating failure.</li>
<li>Return a list of all candidates who just had disputes initiated.</li>
</ol>
</li>
<li>
<p><code>provide_dispute_data(DisputeStatementSet) -&gt; bool</code>: Provide data to an ongoing dispute or initiate a dispute.</p>
<ol>
<li>All statements must be issued under the correct session for the correct candidate. </li>
<li><code>SessionInfo</code> is used to check statement signatures and this function should fail if any signatures are invalid.</li>
<li>If there is no dispute under <code>Disputes</code>, create a new <code>DisputeState</code> with blank bitfields.</li>
<li>If <code>concluded_at</code> is <code>Some</code>, and is <code>concluded_at + config.post_conclusion_acceptance_period &lt; now</code>, return false.</li>
<li>If the overlap of the validators in the <code>DisputeStatementSet</code> and those already present in the <code>DisputeState</code> is fewer in number than <code>byzantine_threshold + 1</code> and the candidate is not present in the <code>Included</code> map</li>
<li>increment <code>SpamSlots</code> for each validator in the <code>DisputeStatementSet</code> which is not already in the <code>DisputeState</code>. Initialize the <code>SpamSlots</code> to a zeroed vector first, if necessary.</li>
<li>If the value for any spam slot exceeds <code>config.dispute_max_spam_slots</code>, return false.</li>
<li>If the overlap of the validators in the <code>DisputeStatementSet</code> and those already present in the <code>DisputeState</code> is at least <code>byzantine_threshold + 1</code>, the <code>DisputeState</code> has fewer than <code>byzantine_threshold + 1</code> validators, and the candidate is not present in the <code>Included</code> map, decrement <code>SpamSlots</code> for each validator in the <code>DisputeState</code>.</li>
<li>Import all statements into the dispute. This should fail if any statements are duplicate; if the corresponding bit for the corresponding validator is set in the dispute already.</li>
<li>If <code>concluded_at</code> is <code>None</code>, reward all statements slightly less.</li>
<li>If <code>concluded_at</code> is <code>Some</code>, reward all statements slightly less.</li>
<li>If either side now has supermajority, slash the other side. This may be both sides, and we support this possibility in code, but note that this requires validators to participate on both sides which has negative expected value. Set <code>concluded_at</code> to <code>Some(now)</code>.</li>
<li>If just concluded against the candidate and the <code>Included</code> map contains <code>(session, candidate)</code>: invoke <code>revert_and_freeze</code> with the stored block number.</li>
<li>Return true if just initiated, false otherwise.</li>
</ol>
</li>
<li>
<p><code>disputes() -&gt; Vec&lt;(SessionIndex, CandidateHash, DisputeState)&gt;</code>: Get a list of all disputes and info about dispute state.</p>
<ol>
<li>Iterate over all disputes in <code>Disputes</code>. Set the flag according to <code>concluded</code>.</li>
</ol>
</li>
<li>
<p><code>note_included(SessionIndex, CandidateHash, included_in: BlockNumber)</code>:</p>
<ol>
<li>Add <code>(SessionIndex, CandidateHash)</code> to the <code>Included</code> map with <code>included_in - 1</code> as the value.</li>
<li>If there is a dispute under <code>(Sessionindex, CandidateHash)</code> with fewer than <code>byzantine_threshold + 1</code> participating validators, decrement <code>SpamSlots</code> for each validator in the <code>DisputeState</code>.</li>
<li>If there is a dispute under <code>(SessionIndex, CandidateHash)</code> that has concluded against the candidate, invoke <code>revert_and_freeze</code> with the stored block number.</li>
</ol>
</li>
<li>
<p><code>could_be_invalid(SessionIndex, CandidateHash) -&gt; bool</code>: Returns whether a candidate has a live dispute ongoing or a dispute which has already concluded in the negative.</p>
</li>
<li>
<p><code>is_frozen()</code>: Load the value of <code>Frozen</code> from storage.</p>
</li>
<li>
<p>`revert_and_freeze(BlockNumber):</p>
<ol>
<li>If <code>is_frozen()</code> return.</li>
<li>issue a digest in the block header which indicates the chain is to be abandoned back to the stored block number.</li>
<li>Set <code>Frozen</code> to true.</li>
</ol>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="paras-module"><a class="header" href="#paras-module">Paras Module</a></h1>
<p>The Paras module is responsible for storing information on parachains and parathreads. Registered
parachains and parathreads cannot change except at session boundaries and after at least a full
session has passed. This is primarily to ensure that the number and meaning of bits required for the
availability bitfields does not change except at session boundaries.</p>
<p>It's also responsible for managing parachain validation code upgrades as well as maintaining
availability of old parachain code and its pruning.</p>
<h2 id="storage-4"><a class="header" href="#storage-4">Storage</a></h2>
<h3 id="utility-structs"><a class="header" href="#utility-structs">Utility Structs</a></h3>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// the two key times necessary to track for every code replacement.
pub struct ReplacementTimes {
 /// The relay-chain block number that the code upgrade was expected to be activated.
 /// This is when the code change occurs from the para's perspective - after the
 /// first parablock included with a relay-parent with number &gt;= this value.
 expected_at: BlockNumber,
 /// The relay-chain block number at which the parablock activating the code upgrade was
 /// actually included. This means considered included and available, so this is the time at which
 /// that parablock enters the acceptance period in this fork of the relay-chain.
 activated_at: BlockNumber,
}

/// Metadata used to track previous parachain validation code that we keep in
/// the state.
pub struct ParaPastCodeMeta {
 // Block numbers where the code was expected to be replaced and where the code
 // was actually replaced, respectively. The first is used to do accurate lookups
 // of historic code in historic contexts, whereas the second is used to do
 // pruning on an accurate timeframe. These can be used as indices
 // into the `PastCode` map along with the `ParaId` to fetch the code itself.
 upgrade_times: Vec&lt;ReplacementTimes&gt;,
 // This tracks the highest pruned code-replacement, if any.
 last_pruned: Option&lt;BlockNumber&gt;,
}

enum UseCodeAt {
 // Use the current code.
 Current,
 // Use the code that was replaced at the given block number.
 ReplacedAt(BlockNumber),
}

struct ParaGenesisArgs {
  /// The initial head-data to use.
  genesis_head: HeadData,
  /// The validation code to start with.
  validation_code: ValidationCode,
  /// True if parachain, false if parathread.
  parachain: bool,
}

/// The possible states of a para, to take into account delayed lifecycle changes.
pub enum ParaLifecycle {
  /// A Para is new and is onboarding.
  Onboarding,
  /// Para is a Parathread.
  Parathread,
  /// Para is a Parachain.
  Parachain,
  /// Para is a Parathread which is upgrading to a Parachain.
  UpgradingParathread,
  /// Para is a Parachain which is downgrading to a Parathread.
  DowngradingParachain,
  /// Parathread is being offboarded.
  OutgoingParathread,
  /// Parachain is being offboarded.
  OutgoingParachain,
}
<span class="boring">}
</span></code></pre></pre>
<h4 id="para-lifecycle"><a class="header" href="#para-lifecycle">Para Lifecycle</a></h4>
<p>Because the state of parachains and parathreads are delayed by a session, we track the specific
state of the para using the <code>ParaLifecycle</code> enum.</p>
<pre><code>None                 Parathread                  Parachain
 +                        +                          +
 |                        |                          |
 |   (2 Session Delay)    |                          |
 |                        |                          |
 +-----------------------&gt;+                          |
 |       Onboarding       |                          |
 |                        |                          |
 +--------------------------------------------------&gt;+
 |       Onboarding       |                          |
 |                        |                          |
 |                        +-------------------------&gt;+
 |                        |   UpgradingParathread    |
 |                        |                          |
 |                        +&lt;-------------------------+
 |                        |   DowngradingParachain   |
 |                        |                          |
 |&lt;-----------------------+                          |
 |   OutgoingParathread   |                          |
 |                        |                          |
 +&lt;--------------------------------------------------+
 |                        |    OutgoingParachain     |
 |                        |                          |
 +                        +                          +
</code></pre>
<p>During the transition period, the para object is still considered in its existing state.</p>
<h3 id="storage-layout"><a class="header" href="#storage-layout">Storage Layout</a></h3>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// All parachains. Ordered ascending by ParaId. Parathreads are not included.
Parachains: Vec&lt;ParaId&gt;,
/// The current lifecycle state of all known Para Ids.
ParaLifecycle: map ParaId =&gt; Option&lt;ParaLifecycle&gt;,
/// The head-data of every registered para.
Heads: map ParaId =&gt; Option&lt;HeadData&gt;;
/// The validation code hash of every live para.
CurrentCodeHash: map ParaId =&gt; Option&lt;ValidationCodeHash&gt;;
/// Actual past code hash, indicated by the para id as well as the block number at which it became outdated.
PastCodeHash: map (ParaId, BlockNumber) =&gt; Option&lt;ValidationCodeHash&gt;;
/// Past code of parachains. The parachains themselves may not be registered anymore,
/// but we also keep their code on-chain for the same amount of time as outdated code
/// to keep it available for secondary checkers.
PastCodeMeta: map ParaId =&gt; ParaPastCodeMeta;
/// Which paras have past code that needs pruning and the relay-chain block at which the code was replaced.
/// Note that this is the actual height of the included block, not the expected height at which the
/// code upgrade would be applied, although they may be equal.
/// This is to ensure the entire acceptance period is covered, not an offset acceptance period starting
/// from the time at which the parachain perceives a code upgrade as having occurred.
/// Multiple entries for a single para are permitted. Ordered ascending by block number.
PastCodePruning: Vec&lt;(ParaId, BlockNumber)&gt;;
/// The block number at which the planned code change is expected for a para.
/// The change will be applied after the first parablock for this ID included which executes
/// in the context of a relay chain block with a number &gt;= `expected_at`.
FutureCodeUpgrades: map ParaId =&gt; Option&lt;BlockNumber&gt;;
/// The actual future code of a para.
FutureCodeHash: map ParaId =&gt; Option&lt;ValidationCodeHash&gt;;
/// The actions to perform during the start of a specific session index.
ActionsQueue: map SessionIndex =&gt; Vec&lt;ParaId&gt;;
/// Upcoming paras instantiation arguments.
UpcomingParasGenesis: map ParaId =&gt; Option&lt;ParaGenesisArgs&gt;;
/// The number of references on the validation code in `CodeByHash` storage.
CodeByHashRefs: map ValidationCodeHash =&gt; u32;
/// Validation code stored by its hash.
CodeByHash: map ValidationCodeHash =&gt; Option&lt;ValidationCode&gt;
<span class="boring">}
</span></code></pre></pre>
<h2 id="session-change-4"><a class="header" href="#session-change-4">Session Change</a></h2>
<ol>
<li>Execute all queued actions for paralifecycle changes:</li>
<li>Clean up outgoing paras.
<ol>
<li>This means removing the entries under <code>Heads</code>, <code>CurrentCode</code>, <code>FutureCodeUpgrades</code>, and
<code>FutureCode</code>. An according entry should be added to <code>PastCode</code>, <code>PastCodeMeta</code>, and
<code>PastCodePruning</code> using the outgoing <code>ParaId</code> and removed <code>CurrentCode</code> value. This is
because any outdated validation code must remain available on-chain for a determined amount
of blocks, and validation code outdated by de-registering the para is still subject to that
invariant.</li>
</ol>
</li>
<li>Apply all incoming paras by initializing the <code>Heads</code> and <code>CurrentCode</code> using the genesis
parameters.</li>
<li>Amend the <code>Parachains</code> list and <code>ParaLifecycle</code> to reflect changes in registered parachains.</li>
<li>Amend the <code>ParaLifecycle</code> set to reflect changes in registered parathreads.</li>
<li>Upgrade all parathreads that should become parachains, updating the <code>Parachains</code> list and
<code>ParaLifecycle</code>.</li>
<li>Downgrade all parachains that should become parathreads, updating the <code>Parachains</code> list and
<code>ParaLifecycle</code>.</li>
<li>Return list of outgoing paras to the initializer for use by other modules.</li>
</ol>
<h2 id="initialization-2"><a class="header" href="#initialization-2">Initialization</a></h2>
<ol>
<li>Do pruning based on all entries in <code>PastCodePruning</code> with <code>BlockNumber &lt;= now</code>. Update the
corresponding <code>PastCodeMeta</code> and <code>PastCode</code> accordingly.</li>
</ol>
<h2 id="routines-2"><a class="header" href="#routines-2">Routines</a></h2>
<ul>
<li><code>schedule_para_initialize(ParaId, ParaGenesisArgs)</code>: Schedule a para to be initialized at the next
session. Noop if para is already registered in the system with some <code>ParaLifecycle</code>.</li>
<li><code>schedule_para_cleanup(ParaId)</code>: Schedule a para to be cleaned up after the next full session.</li>
<li><code>schedule_parathread_upgrade(ParaId)</code>: Schedule a parathread to be upgraded to a parachain.</li>
<li><code>schedule_parachain_downgrade(ParaId)</code>: Schedule a parachain to be downgraded to a parathread.</li>
<li><code>schedule_code_upgrade(ParaId, CurrentCode, expected_at: BlockNumber)</code>: Schedule a future code
upgrade of the given parachain, to be applied after inclusion of a block of the same parachain
executed in the context of a relay-chain block with number &gt;= <code>expected_at</code>.</li>
<li><code>note_new_head(ParaId, HeadData, BlockNumber)</code>: note that a para has progressed to a new head,
where the new head was executed in the context of a relay-chain block with given number. This will
apply pending code upgrades based on the block number provided.</li>
<li><code>validation_code_at(ParaId, at: BlockNumber, assume_intermediate: Option&lt;BlockNumber&gt;)</code>: Fetches
the validation code to be used when validating a block in the context of the given relay-chain
height. A second block number parameter may be used to tell the lookup to proceed as if an
intermediate parablock has been included at the given relay-chain height. This may return past,
current, or (with certain choices of <code>assume_intermediate</code>) future code. <code>assume_intermediate</code>, if
provided, must be before <code>at</code>. If the validation code has been pruned, this will return <code>None</code>.</li>
<li><code>validation_code_hash_at(ParaId, at: BlockNumber, assume_intermediate: Option&lt;BlockNumber&gt;)</code>: Just like <code>validation_code_at</code>, but returns the code hash.</li>
<li><code>lifecycle(ParaId) -&gt; Option&lt;ParaLifecycle&gt;</code>: Return the <code>ParaLifecycle</code> of a para.</li>
<li><code>is_parachain(ParaId) -&gt; bool</code>: Returns true if the para ID references any live parachain,
including those which may be transitioning to a parathread in the future.</li>
<li><code>is_parathread(ParaId) -&gt; bool</code>: Returns true if the para ID references any live parathread,
including those which may be transitioning to a parachain in the future.</li>
<li><code>is_valid_para(ParaId) -&gt; bool</code>: Returns true if the para ID references either a live parathread
or live parachain.</li>
<li><code>last_code_upgrade(id: ParaId, include_future: bool) -&gt; Option&lt;BlockNumber&gt;</code>: The block number of
the last scheduled upgrade of the requested para. Includes future upgrades if the flag is set.
This is the <code>expected_at</code> number, not the <code>activated_at</code> number.</li>
</ul>
<h2 id="finalization-2"><a class="header" href="#finalization-2">Finalization</a></h2>
<p>No finalization routine runs for this module.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="scheduler-module"><a class="header" href="#scheduler-module">Scheduler Module</a></h1>
<blockquote>
<p>TODO: this section is still heavily under construction. key questions about availability cores and validator assignment are still open and the flow of the the section may be contradictory or inconsistent</p>
</blockquote>
<p>The Scheduler module is responsible for two main tasks:</p>
<ul>
<li>Partitioning validators into groups and assigning groups to parachains and parathreads.</li>
<li>Scheduling parachains and parathreads</li>
</ul>
<p>It aims to achieve these tasks with these goals in mind:</p>
<ul>
<li>It should be possible to know at least a block ahead-of-time, ideally more, which validators are going to be assigned to which parachains.</li>
<li>Parachains that have a candidate pending availability in this fork of the chain should not be assigned.</li>
<li>Validator assignments should not be gameable. Malicious cartels should not be able to manipulate the scheduler to assign themselves as desired.</li>
<li>High or close to optimal throughput of parachains and parathreads. Work among validator groups should be balanced.</li>
</ul>
<h2 id="availability-cores"><a class="header" href="#availability-cores">Availability Cores</a></h2>
<p>The Scheduler manages resource allocation using the concept of &quot;Availability Cores&quot;. There will be one availability core for each parachain, and a fixed number of cores used for multiplexing parathreads. Validators will be partitioned into groups, with the same number of groups as availability cores. Validator groups will be assigned to different availability cores over time.</p>
<p>An availability core can exist in either one of two states at the beginning or end of a block: free or occupied. A free availability core can have a parachain or parathread assigned to it for the potential to have a backed candidate included. After backing, the core enters the occupied state as the backed candidate is pending availability. There is an important distinction: a core is not considered occupied until it is in charge of a block pending availability, although the implementation may treat scheduled cores the same as occupied ones for brevity. A core exits the occupied state when the candidate is no longer pending availability - either on timeout or on availability. A core starting in the occupied state can move to the free state and back to occupied all within a single block, as availability bitfields are processed before backed candidates. At the end of the block, there is a possible timeout on availability which can move the core back to the free state if occupied.</p>
<p>Cores are treated as an ordered list and are typically referred to by their index in that list.</p>
<p><img src="runtime/scheduler_module_0.generated.svg" alt="" /></p>
<p><img src="runtime/scheduler_module_1.generated.svg" alt="" /></p>
<h2 id="validator-groups"><a class="header" href="#validator-groups">Validator Groups</a></h2>
<p>Validator group assignments do not need to change very quickly. The security benefits of fast rotation are redundant with the challenge mechanism in the <a href="runtime/../protocol-approval.html">Approval process</a>. Because of this, we only divide validators into groups at the beginning of the session and do not shuffle membership during the session. However, we do take steps to ensure that no particular validator group has dominance over a single parachain or parathread-multiplexer for an entire session to provide better guarantees of liveness.</p>
<p>Validator groups rotate across availability cores in a round-robin fashion, with rotation occurring at fixed intervals. The i'th group will be assigned to the <code>(i+k)%n</code>'th core at any point in time, where <code>k</code> is the number of rotations that have occurred in the session, and <code>n</code> is the number of cores. This makes upcoming rotations within the same session predictable.</p>
<p>When a rotation occurs, validator groups are still responsible for distributing availability chunks for any previous cores that are still occupied and pending availability. In practice, rotation and availability-timeout frequencies should be set so this will only be the core they have just been rotated from. It is possible that a validator group is rotated onto a core which is currently occupied. In this case, the validator group will have nothing to do until the previously-assigned group finishes their availability work and frees the core or the availability process times out. Depending on if the core is for a parachain or parathread, a different timeout <code>t</code> from the <a href="runtime/../types/runtime.html#host-configuration"><code>HostConfiguration</code></a> will apply. Availability timeouts should only be triggered in the first <code>t-1</code> blocks after the beginning of a rotation.</p>
<h2 id="claims"><a class="header" href="#claims">Claims</a></h2>
<p>Parathreads operate on a system of claims. Collators participate in auctions to stake a claim on authoring the next block of a parathread, although the auction mechanism is beyond the scope of the scheduler. The scheduler guarantees that they'll be given at least a certain number of attempts to author a candidate that is backed. Attempts that fail during the availability phase are not counted, since ensuring availability at that stage is the responsibility of the backing validators, not of the collator. When a claim is accepted, it is placed into a queue of claims, and each claim is assigned to a particular parathread-multiplexing core in advance. Given that the current assignments of validator groups to cores are known, and the upcoming assignments are predictable, it is possible for parathread collators to know who they should be talking to now and how they should begin establishing connections with as a fallback.</p>
<p>With this information, the Node-side can be aware of which parathreads have a good chance of being includable within the relay-chain block and can focus any additional resources on backing candidates from those parathreads. Furthermore, Node-side code is aware of which validator group will be responsible for that thread. If the necessary conditions are reached for core reassignment, those candidates can be backed within the same block as the core being freed.</p>
<p>Parathread claims, when scheduled onto a free core, may not result in a block pending availability. This may be due to collator error, networking timeout, or censorship by the validator group. In this case, the claims should be retried a certain number of times to give the collator a fair shot.</p>
<h2 id="storage-5"><a class="header" href="#storage-5">Storage</a></h2>
<p>Utility structs:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// A claim on authoring the next block for a given parathread.
struct ParathreadClaim(ParaId, CollatorId);

// An entry tracking a claim to ensure it does not pass the maximum number of retries.
struct ParathreadEntry {
  claim: ParathreadClaim,
  retries: u32,
}

// A queued parathread entry, pre-assigned to a core.
struct QueuedParathread {
  claim: ParathreadEntry,
  /// offset within the set of para-threads ranged `0..config.parathread_cores`.
  core_offset: u32,
}

struct ParathreadQueue {
  queue: Vec&lt;QueuedParathread&gt;,
  /// offset within the set of para-threads ranged `0..config.parathread_cores`.
  next_core_offset: u32,
}

enum CoreOccupied {
  Parathread(ParathreadEntry), // claim &amp; retries
  Parachain,
}

enum AssignmentKind {
  Parachain,
  Parathread(CollatorId, u32),
}

struct CoreAssignment {
  core: CoreIndex,
  para_id: ParaId,
  kind: AssignmentKind,
  group_idx: GroupIndex,
}
// reasons a core might be freed.
enum FreedReason {
  Concluded,
  TimedOut,
}
<span class="boring">}
</span></code></pre></pre>
<p>Storage layout:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// All the validator groups. One for each core. Indices are into the `ActiveValidators` storage.
ValidatorGroups: Vec&lt;Vec&lt;ValidatorIndex&gt;&gt;;
/// A queue of upcoming claims and which core they should be mapped onto.
ParathreadQueue: ParathreadQueue;
/// One entry for each availability core. Entries are `None` if the core is not currently occupied.
/// The i'th parachain belongs to the i'th core, with the remaining cores all being
/// parathread-multiplexers.
AvailabilityCores: Vec&lt;Option&lt;CoreOccupied&gt;&gt;;
/// An index used to ensure that only one claim on a parathread exists in the queue or is
/// currently being handled by an occupied core.
ParathreadClaimIndex: Vec&lt;ParaId&gt;;
/// The block number where the session start occurred. Used to track how many group rotations have occurred.
SessionStartBlock: BlockNumber;
/// Currently scheduled cores - free but up to be occupied.
/// The value contained here will not be valid after the end of a block. 
/// Runtime APIs should be used to determine scheduled cores
/// for the upcoming block.
Scheduled: Vec&lt;CoreAssignment&gt;, // sorted ascending by CoreIndex.
<span class="boring">}
</span></code></pre></pre>
<h2 id="session-change-5"><a class="header" href="#session-change-5">Session Change</a></h2>
<p>Session changes are the only time that configuration can change, and the <a href="runtime/configuration.html">Configuration module</a>'s session-change logic is handled before this module's. We also lean on the behavior of the <a href="runtime/inclusion.html">Inclusion module</a> which clears all its occupied cores on session change. Thus we don't have to worry about cores being occupied across session boundaries and it is safe to re-size the <code>AvailabilityCores</code> bitfield.</p>
<p>Actions:</p>
<ol>
<li>Set <code>SessionStartBlock</code> to current block number + 1, as session changes are applied at the end of the block.</li>
<li>Clear all <code>Some</code> members of <code>AvailabilityCores</code>. Return all parathread claims to queue with retries un-incremented.</li>
<li>Set <code>configuration = Configuration::configuration()</code> (see <a href="runtime/../types/runtime.html#host-configuration"><code>HostConfiguration</code></a>)</li>
<li>Fetch <code>Shared::ActiveValidators</code> as AV.</li>
<li>Determine the number of cores &amp; validator groups as <code>n_cores</code>. This is the maximum of
<ol>
<li><code>Paras::parachains().len() + configuration.parathread_cores</code></li>
<li><code>n_validators / max_validators_per_core</code> if <code>configuration.max_validators_per_core</code> is <code>Some</code> and non-zero.</li>
</ol>
</li>
<li>Resize <code>AvailabilityCores</code> to have length <code>n_cores</code> with all <code>None</code> entries.</li>
<li>Compute new validator groups by shuffling using a secure randomness beacon
<ul>
<li>Note that the total number of validators <code>V</code> in AV may not be evenly divided by <code>n_cores</code>.</li>
<li>The groups are selected by partitioning AV.  The first V % N groups will have (V / n_cores) + 1 members, while the remaining groups will have (V / N) members each.</li>
<li>Instead of using the indices within AV, which point to the broader set, indices <em>into</em> AV should be used. This implies that groups should have simply ascending validator indices.</li>
</ul>
</li>
<li>Prune the parathread queue to remove all retries beyond <code>configuration.parathread_retries</code>.
<ul>
<li>Also prune all parathread claims corresponding to de-registered parathreads.</li>
<li>all pruned claims should have their entry removed from the parathread index.</li>
<li>assign all non-pruned claims to new cores if the number of parathread cores has changed between the <code>new_config</code> and <code>old_config</code> of the <code>SessionChangeNotification</code>.</li>
<li>Assign claims in equal balance across all cores if rebalancing, and set the <code>next_core</code> of the <code>ParathreadQueue</code> by incrementing the relative index of the last assigned core and taking it modulo the number of parathread cores. </li>
</ul>
</li>
</ol>
<h2 id="initialization-3"><a class="header" href="#initialization-3">Initialization</a></h2>
<p>No initialization routine runs for this module.</p>
<h2 id="finalization-3"><a class="header" href="#finalization-3">Finalization</a></h2>
<p>No finalization routine runs for this module.</p>
<h2 id="routines-3"><a class="header" href="#routines-3">Routines</a></h2>
<ul>
<li><code>add_parathread_claim(ParathreadClaim)</code>: Add a parathread claim to the queue.
<ul>
<li>Fails if any parathread claim on the same parathread is currently indexed.</li>
<li>Fails if the queue length is &gt;= <code>config.scheduling_lookahead * config.parathread_cores</code>.</li>
<li>The core used for the parathread claim is the <code>next_core</code> field of the <code>ParathreadQueue</code> and adding <code>Paras::parachains().len()</code> to it.</li>
<li><code>next_core</code> is then updated by adding 1 and taking it modulo <code>config.parathread_cores</code>.</li>
<li>The claim is then added to the claim index.</li>
</ul>
</li>
<li><code>schedule(Vec&lt;(CoreIndex, FreedReason)&gt;, now: BlockNumber)</code>: schedule new core assignments, with a parameter indicating previously-occupied cores which are to be considered returned and why they are being returned.
<ul>
<li>All freed parachain cores should be assigned to their respective parachain</li>
<li>All freed parathread cores whose reason for freeing was <code>FreedReason::Concluded</code> should have the claim removed from the claim index.</li>
<li>All freed parathread cores whose reason for freeing was <code>FreedReason::TimedOut</code> should have the claim added to the parathread queue again without retries incremented</li>
<li>All freed parathread cores should take the next parathread entry from the queue.</li>
<li>The i'th validator group will be assigned to the <code>(i+k)%n</code>'th core at any point in time, where <code>k</code> is the number of rotations that have occurred in the session, and <code>n</code> is the total number of cores. This makes upcoming rotations within the same session predictable. Rotations are based off of <code>now</code>.</li>
</ul>
</li>
<li><code>scheduled() -&gt; Vec&lt;CoreAssignment&gt;</code>: Get currently scheduled core assignments.</li>
<li><code>occupied(Vec&lt;CoreIndex&gt;)</code>. Note that the given cores have become occupied.
<ul>
<li>Behavior undefined if any given cores were not scheduled.</li>
<li>Behavior undefined if the given cores are not sorted ascending by core index</li>
<li>This clears them from <code>Scheduled</code> and marks each corresponding <code>core</code> in the <code>AvailabilityCores</code> as occupied.</li>
<li>Since both the availability cores and the newly-occupied cores lists are sorted ascending, this method can be implemented efficiently.</li>
</ul>
</li>
<li><code>core_para(CoreIndex) -&gt; ParaId</code>: return the currently-scheduled or occupied ParaId for the given core.</li>
<li><code>group_validators(GroupIndex) -&gt; Option&lt;Vec&lt;ValidatorIndex&gt;&gt;</code>: return all validators in a given group, if the group index is valid for this session.</li>
<li><code>availability_timeout_predicate() -&gt; Option&lt;impl Fn(CoreIndex, BlockNumber) -&gt; bool&gt;</code>: returns an optional predicate that should be used for timing out occupied cores. if <code>None</code>, no timing-out should be done. The predicate accepts the index of the core, and the block number since which it has been occupied. The predicate should be implemented based on the time since the last validator group rotation, and the respective parachain and parathread timeouts, i.e. only within <code>max(config.chain_availability_period, config.thread_availability_period)</code> of the last rotation would this return <code>Some</code>.</li>
<li><code>group_rotation_info(now: BlockNumber) -&gt; GroupRotationInfo</code>: Returns a helper for determining group rotation.</li>
<li><code>next_up_on_available(CoreIndex) -&gt; Option&lt;ScheduledCore&gt;</code>: Return the next thing that will be scheduled on this core assuming it is currently occupied and the candidate occupying it became available. Returns in <code>ScheduledCore</code> format (todo: link to Runtime APIs page; linkcheck doesn't allow this right now). For parachains, this is always the ID of the parachain and no specified collator. For parathreads, this is based on the next item in the <code>ParathreadQueue</code> assigned to that core, and is <code>None</code> if there isn't one.</li>
<li><code>next_up_on_time_out(CoreIndex) -&gt; Option&lt;ScheduledCore&gt;</code>: Return the next thing that will be scheduled on this core assuming it is currently occupied and the candidate occupying it timed out. Returns in <code>ScheduledCore</code> format (todo: link to Runtime APIs page; linkcheck doesn't allow this right now). For parachains, this is always the ID of the parachain and no specified collator. For parathreads, this is based on the next item in the <code>ParathreadQueue</code> assigned to that core, or if there isn't one, the claim that is currently occupying the core. Otherwise <code>None</code>.</li>
<li><code>clear()</code>:
<ul>
<li>Free all scheduled cores and return parathread claims to queue, with retries incremented. Skip parathreads which no longer exist under paras.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="inclusion-module"><a class="header" href="#inclusion-module">Inclusion Module</a></h1>
<p>The inclusion module is responsible for inclusion and availability of scheduled parachains and parathreads.</p>
<h2 id="storage-6"><a class="header" href="#storage-6">Storage</a></h2>
<p>Helper structs:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct AvailabilityBitfield {
  bitfield: BitVec, // one bit per core.
  submitted_at: BlockNumber, // for accounting, as meaning of bits may change over time.
}

struct CandidatePendingAvailability {
  core: CoreIndex, // availability core
  hash: CandidateHash,
  descriptor: CandidateDescriptor,
  availability_votes: Bitfield, // one bit per validator.
  relay_parent_number: BlockNumber, // number of the relay-parent.
  backers: Bitfield, // one bit per validator, set for those who backed the candidate.
  backed_in_number: BlockNumber,
  backing_group: GroupIndex,
}
<span class="boring">}
</span></code></pre></pre>
<p>Storage Layout:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// The latest bitfield for each validator, referred to by index.
bitfields: map ValidatorIndex =&gt; AvailabilityBitfield;
/// Candidates pending availability.
PendingAvailability: map ParaId =&gt; CandidatePendingAvailability;
/// The commitments of candidates pending availability, by ParaId.
PendingAvailabilityCommitments: map ParaId =&gt; CandidateCommitments;
<span class="boring">}
</span></code></pre></pre>
<h2 id="session-change-6"><a class="header" href="#session-change-6">Session Change</a></h2>
<ol>
<li>Clear out all candidates pending availability.</li>
<li>Clear out all validator bitfields.</li>
</ol>
<h2 id="routines-4"><a class="header" href="#routines-4">Routines</a></h2>
<p>All failed checks should lead to an unrecoverable error making the block invalid.</p>
<ul>
<li>
<p><code>process_bitfields(expected_bits, Bitfields, core_lookup: Fn(CoreIndex) -&gt; Option&lt;ParaId&gt;)</code>:</p>
<ol>
<li>check that there is at most 1 bitfield per validator and that the number of bits in each bitfield is equal to expected_bits.</li>
<li>check that there are no duplicates</li>
<li>check all validator signatures.</li>
<li>apply each bit of bitfield to the corresponding pending candidate. looking up parathread cores using the <code>core_lookup</code>. Disregard bitfields that have a <code>1</code> bit for any free cores.</li>
<li>For each applied bit of each availability-bitfield, set the bit for the validator in the <code>CandidatePendingAvailability</code>'s <code>availability_votes</code> bitfield. Track all candidates that now have &gt;2/3 of bits set in their <code>availability_votes</code>. These candidates are now available and can be enacted.</li>
<li>For all now-available candidates, invoke the <code>enact_candidate</code> routine with the candidate and relay-parent number.</li>
<li>Return a list of <code>(CoreIndex, CandidateHash)</code> from freed cores consisting of the cores where candidates have become available.</li>
</ol>
</li>
<li>
<p><code>process_candidates(parent_storage_root, BackedCandidates, scheduled: Vec&lt;CoreAssignment&gt;, group_validators: Fn(GroupIndex) -&gt; Option&lt;Vec&lt;ValidatorIndex&gt;&gt;)</code>:</p>
<ol>
<li>check that each candidate corresponds to a scheduled core and that they are ordered in the same order the cores appear in assignments in <code>scheduled</code>.</li>
<li>check that <code>scheduled</code> is sorted ascending by <code>CoreIndex</code>, without duplicates.</li>
<li>check that there is no candidate pending availability for any scheduled <code>ParaId</code>.</li>
<li>check that each candidate's <code>validation_data_hash</code> corresponds to a <code>PersistedValidationData</code> computed from the current state.</li>
</ol>
<blockquote>
<p>NOTE: With contextual execution in place, validation data will be obtained as of the state of the context block. However, only the state of the current block can be used for such a query.</p>
</blockquote>
<ol>
<li>If the core assignment includes a specific collator, ensure the backed candidate is issued by that collator.</li>
<li>Ensure that any code upgrade scheduled by the candidate does not happen within <code>config.validation_upgrade_frequency</code> of <code>Paras::last_code_upgrade(para_id, true)</code>, if any, comparing against the value of <code>Paras::FutureCodeUpgrades</code> for the given para ID.</li>
<li>Check the collator's signature on the candidate data.</li>
<li>check the backing of the candidate using the signatures and the bitfields, comparing against the validators assigned to the groups, fetched with the <code>group_validators</code> lookup.</li>
<li>call <code>Ump::check_upward_messages(para, commitments.upward_messages)</code> to check that the upward messages are valid.</li>
<li>call <code>Dmp::check_processed_downward_messages(para, commitments.processed_downward_messages)</code> to check that the DMQ is properly drained.</li>
<li>call <code>Hrmp::check_hrmp_watermark(para, commitments.hrmp_watermark)</code> for each candidate to check rules of processing the HRMP watermark.</li>
<li>using <code>Hrmp::check_outbound_hrmp(sender, commitments.horizontal_messages)</code> ensure that the each candidate sent a valid set of horizontal messages</li>
<li>create an entry in the <code>PendingAvailability</code> map for each backed candidate with a blank <code>availability_votes</code> bitfield.</li>
<li>create a corresponding entry in the <code>PendingAvailabilityCommitments</code> with the commitments.</li>
<li>Return a <code>Vec&lt;CoreIndex&gt;</code> of all scheduled cores of the list of passed assignments that a candidate was successfully backed for, sorted ascending by CoreIndex.</li>
</ol>
</li>
<li>
<p><code>enact_candidate(relay_parent_number: BlockNumber, CommittedCandidateReceipt)</code>:</p>
<ol>
<li>If the receipt contains a code upgrade, Call <code>Paras::schedule_code_upgrade(para_id, code, relay_parent_number + config.validationl_upgrade_delay)</code>.</li>
</ol>
<blockquote>
<p>TODO: Note that this is safe as long as we never enact candidates where the relay parent is across a session boundary. In that case, which we should be careful to avoid with contextual execution, the configuration might have changed and the para may de-sync from the host's understanding of it.</p>
</blockquote>
<ol>
<li>Reward all backing validators of each candidate, contained within the <code>backers</code> field.</li>
<li>call <code>Ump::receive_upward_messages</code> for each backed candidate, using the <a href="runtime/../types/messages.html#upward-message"><code>UpwardMessage</code>s</a> from the <a href="runtime/../types/candidate.html#candidate-commitments"><code>CandidateCommitments</code></a>.</li>
<li>call <code>Dmp::prune_dmq</code> with the para id of the candidate and the candidate's <code>processed_downward_messages</code>.</li>
<li>call <code>Hrmp::prune_hrmp</code> with the para id of the candiate and the candidate's <code>hrmp_watermark</code>.</li>
<li>call <code>Hrmp::queue_outbound_hrmp</code> with the para id of the candidate and the list of horizontal messages taken from the commitment,</li>
<li>Call <code>Paras::note_new_head</code> using the <code>HeadData</code> from the receipt and <code>relay_parent_number</code>.</li>
</ol>
</li>
<li>
<p><code>collect_pending</code>:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>  fn collect_pending(f: impl Fn(CoreIndex, BlockNumber) -&gt; bool) -&gt; Vec&lt;CoreIndex&gt; {
    // sweep through all paras pending availability. if the predicate returns true, when given the core index and
    // the block number the candidate has been pending availability since, then clean up the corresponding storage for that candidate and the commitments.
    // return a vector of cleaned-up core IDs.
  }
<span class="boring">}
</span></code></pre></pre>
</li>
<li>
<p><code>force_enact(ParaId)</code>: Forcibly enact the candidate with the given ID as though it had been deemed available by bitfields. Is a no-op if there is no candidate pending availability for this para-id. This should generally not be used but it is useful during execution of Runtime APIs, where the changes to the state are expected to be discarded directly after.</p>
</li>
<li>
<p><code>candidate_pending_availability(ParaId) -&gt; Option&lt;CommittedCandidateReceipt&gt;</code>: returns the <code>CommittedCandidateReceipt</code> pending availability for the para provided, if any.</p>
</li>
<li>
<p><code>pending_availability(ParaId) -&gt; Option&lt;CandidatePendingAvailability&gt;</code>: returns the metadata around the candidate pending availability for the para, if any.</p>
</li>
<li>
<p><code>collect_disputed(disputed: Vec&lt;CandidateHash&gt;) -&gt; Vec&lt;CoreIndex&gt;</code>: Sweeps through all paras pending availability. If the candidate hash is one of the disputed candidates, then clean up the corresponding storage for that candidate and the commitments. Return a vector of cleaned-up core IDs.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="parainherent"><a class="header" href="#parainherent">ParaInherent</a></h1>
<p>This module is responsible for providing all data given to the runtime by the block author to the various parachains modules. The entry-point is mandatory, in that it must be invoked exactly once within every block, and it is also &quot;inherent&quot;, in that it is provided with no origin by the block author. The data within it carries its own authentication; i.e. the data takes the form of signed statements by validators. If any of the steps within fails, the entry-point is considered as having failed and the block will be invalid.</p>
<p>This module does not have the same initialization/finalization concerns as the others, as it only requires that entry points be triggered after all modules have initialized and that finalization happens after entry points are triggered. Both of these are assumptions we have already made about the runtime's order of operations, so this module doesn't need to be initialized or finalized by the <code>Initializer</code>.</p>
<p>There are a couple of important notes to the operations in this inherent as they relate to disputes.</p>
<ol>
<li>We don't accept bitfields or backed candidates if in &quot;governance-only&quot; mode from having a local dispute conclude on this fork.</li>
<li>When disputes are initiated, we remove the block from pending availability. This allows us to roll back chains to the block before blocks are included as opposed to backing. It's important to do this before processing bitfields.</li>
<li><code>Inclusion::collect_disputed</code> is kind of expensive so it's important to gate this on whether there are actually any new disputes. Which should be never.</li>
<li>And we don't accept parablocks that have open disputes or disputes that have concluded against the candidate. It's important to import dispute statements before backing, but this is already the case as disputes are imported before processing bitfields.</li>
</ol>
<h2 id="storage-7"><a class="header" href="#storage-7">Storage</a></h2>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Included: Option&lt;()&gt;,
<span class="boring">}
</span></code></pre></pre>
<h2 id="finalization-4"><a class="header" href="#finalization-4">Finalization</a></h2>
<ol>
<li>Take (get and clear) the value of <code>Included</code>. If it is not <code>Some</code>, throw an unrecoverable error.</li>
</ol>
<h2 id="entry-points-1"><a class="header" href="#entry-points-1">Entry Points</a></h2>
<ul>
<li><code>enter</code>: This entry-point accepts three parameters: The relay-chain parent block header, <a href="runtime/../types/availability.html#signed-availability-bitfield"><code>Bitfields</code></a> and <a href="runtime/../types/backing.html#backed-candidate"><code>BackedCandidates</code></a>.
<ol>
<li>Hash the parent header and make sure that it corresponds to the block hash of the parent (tracked by the <code>frame_system</code> FRAME module),</li>
<li>Invoke <code>Disputes::provide_multi_dispute_data</code>.</li>
<li>If <code>Disputes::is_frozen</code>, return and set <code>Included</code> to <code>Some(())</code>.</li>
<li>If there are any created disputes from the current session, invoke <code>Inclusion::collect_disputed</code> with the disputed candidates. Annotate each returned core with <code>FreedReason::Concluded</code>.</li>
<li>The <code>Bitfields</code> are first forwarded to the <code>Inclusion::process_bitfields</code> routine, returning a set of freed cores. Provide the number of availability cores (<code>Scheduler::availability_cores().len()</code>) as the expected number of bits and a <code>Scheduler::core_para</code> as a core-lookup to the <code>process_bitfields</code> routine. Annotate each of these freed cores with <code>FreedReason::Concluded</code>.</li>
<li>For each freed candidate from the <code>Inclusion::process_bitfields</code> call, invoke <code>Disputes::note_included(current_session, candidate)</code>.</li>
<li>If <code>Scheduler::availability_timeout_predicate</code> is <code>Some</code>, invoke <code>Inclusion::collect_pending</code> using it and annotate each of those freed cores with <code>FreedReason::TimedOut</code>.</li>
<li>Combine and sort the dispute-freed cores, the bitfield-freed cores, and the timed-out cores.</li>
<li>Invoke <code>Scheduler::clear</code></li>
<li>Invoke <code>Scheduler::schedule(freed_cores, System::current_block())</code></li>
<li>Extract <code>parent_storage_root</code> from the parent header,</li>
<li>If <code>Disputes::could_be_invalid(current_session, candidate)</code> is true for any of the <code>backed_candidates</code>, fail.</li>
<li>Invoke the <code>Inclusion::process_candidates</code> routine with the parameters <code>(parent_storage_root, backed_candidates, Scheduler::scheduled(), Scheduler::group_validators)</code>.</li>
<li>Call <code>Scheduler::occupied</code> using the return value of the <code>Inclusion::process_candidates</code> call above, first sorting the list of assigned core indices.</li>
<li>Call the <code>Ump::process_pending_upward_messages</code> routine to execute all messages in upward dispatch queues.</li>
<li>If all of the above succeeds, set <code>Included</code> to <code>Some(())</code>.</li>
</ol>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dmp-module"><a class="header" href="#dmp-module">DMP Module</a></h1>
<p>A module responsible for Downward Message Processing (DMP). See <a href="runtime/../messaging.html">Messaging Overview</a> for more details.</p>
<h2 id="storage-8"><a class="header" href="#storage-8">Storage</a></h2>
<p>Storage layout required for implementation of DMP.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// The downward messages addressed for a certain para.
DownwardMessageQueues: map ParaId =&gt; Vec&lt;InboundDownwardMessage&gt;;
/// A mapping that stores the downward message queue MQC head for each para.
///
/// Each link in this chain has a form:
/// `(prev_head, B, H(M))`, where
/// - `prev_head`: is the previous head hash or zero if none.
/// - `B`: is the relay-chain block number in which a message was appended.
/// - `H(M)`: is the hash of the message being appended.
DownwardMessageQueueHeads: map ParaId =&gt; Hash;
<span class="boring">}
</span></code></pre></pre>
<h2 id="initialization-4"><a class="header" href="#initialization-4">Initialization</a></h2>
<p>No initialization routine runs for this module.</p>
<h2 id="routines-5"><a class="header" href="#routines-5">Routines</a></h2>
<p>Candidate Acceptance Function:</p>
<ul>
<li><code>check_processed_downward_messages(P: ParaId, processed_downward_messages: u32)</code>:
<ol>
<li>Checks that <code>DownwardMessageQueues</code> for <code>P</code> is at least <code>processed_downward_messages</code> long.</li>
<li>Checks that <code>processed_downward_messages</code> is at least 1 if <code>DownwardMessageQueues</code> for <code>P</code> is not empty.</li>
</ol>
</li>
</ul>
<p>Candidate Enactment:</p>
<ul>
<li><code>prune_dmq(P: ParaId, processed_downward_messages: u32)</code>:
<ol>
<li>Remove the first <code>processed_downward_messages</code> from the <code>DownwardMessageQueues</code> of <code>P</code>.</li>
</ol>
</li>
</ul>
<p>Utility routines.</p>
<p><code>queue_downward_message(P: ParaId, M: DownwardMessage)</code>:</p>
<ol>
<li>Check if the size of <code>M</code> exceeds the <code>config.max_downward_message_size</code>. If so, return an error.</li>
<li>Wrap <code>M</code> into <code>InboundDownwardMessage</code> using the current block number for <code>sent_at</code>.</li>
<li>Obtain a new MQC link for the resulting <code>InboundDownwardMessage</code> and replace <code>DownwardMessageQueueHeads</code> for <code>P</code> with the resulting hash.</li>
<li>Add the resulting <code>InboundDownwardMessage</code> into <code>DownwardMessageQueues</code> for <code>P</code>.</li>
</ol>
<h2 id="session-change-7"><a class="header" href="#session-change-7">Session Change</a></h2>
<ol>
<li>For each <code>P</code> in <code>outgoing_paras</code> (generated by <code>Paras::on_new_session</code>):
<ol>
<li>Remove all <code>DownwardMessageQueues</code> of <code>P</code>.</li>
<li>Remove <code>DownwardMessageQueueHeads</code> for <code>P</code>.</li>
</ol>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ump-module"><a class="header" href="#ump-module">UMP Module</a></h1>
<p>A module responsible for Upward Message Passing (UMP). See <a href="runtime/../messaging.html">Messaging Overview</a> for more details.</p>
<h2 id="storage-9"><a class="header" href="#storage-9">Storage</a></h2>
<p>Storage related to UMP</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// The messages waiting to be handled by the relay-chain originating from a certain parachain.
///
/// Note that some upward messages might have been already processed by the inclusion logic. E.g.
/// channel management messages.
///
/// The messages are processed in FIFO order.
RelayDispatchQueues: map ParaId =&gt; Vec&lt;UpwardMessage&gt;;
/// Size of the dispatch queues. Caches sizes of the queues in `RelayDispatchQueue`.
///
/// First item in the tuple is the count of messages and second
/// is the total length (in bytes) of the message payloads.
///
/// Note that this is an auxilary mapping: it's possible to tell the byte size and the number of
/// messages only looking at `RelayDispatchQueues`. This mapping is separate to avoid the cost of
/// loading the whole message queue if only the total size and count are required.
///
/// Invariant:
/// - The set of keys should exactly match the set of keys of `RelayDispatchQueues`.
RelayDispatchQueueSize: map ParaId =&gt; (u32, u32); // (num_messages, total_bytes)
/// The ordered list of `ParaId`s that have a `RelayDispatchQueue` entry.
///
/// Invariant:
/// - The set of items from this vector should be exactly the set of the keys in
///   `RelayDispatchQueues` and `RelayDispatchQueueSize`.
NeedsDispatch: Vec&lt;ParaId&gt;;
/// This is the para that gets dispatched first during the next upward dispatchable queue
/// execution round.
///
/// Invariant:
/// - If `Some(para)`, then `para` must be present in `NeedsDispatch`.
NextDispatchRoundStartWith: Option&lt;ParaId&gt;;
<span class="boring">}
</span></code></pre></pre>
<h2 id="initialization-5"><a class="header" href="#initialization-5">Initialization</a></h2>
<p>No initialization routine runs for this module.</p>
<h2 id="routines-6"><a class="header" href="#routines-6">Routines</a></h2>
<p>Candidate Acceptance Function:</p>
<ul>
<li><code>check_upward_messages(P: ParaId, Vec&lt;UpwardMessage&gt;</code>):
<ol>
<li>Checks that there are at most <code>config.max_upward_message_num_per_candidate</code> messages.</li>
<li>Checks that no message exceeds <code>config.max_upward_message_size</code>.</li>
<li>Verify that <code>RelayDispatchQueueSize</code> for <code>P</code> has enough capacity for the messages</li>
</ol>
</li>
</ul>
<p>Candidate Enactment:</p>
<ul>
<li><code>receive_upward_messages(P: ParaId, Vec&lt;UpwardMessage&gt;)</code>:
<ol>
<li>Process each upward message <code>M</code> in order:
<ol>
<li>Append the message to <code>RelayDispatchQueues</code> for <code>P</code></li>
<li>Increment the size and the count in <code>RelayDispatchQueueSize</code> for <code>P</code>.</li>
<li>Ensure that <code>P</code> is present in <code>NeedsDispatch</code>.</li>
</ol>
</li>
</ol>
</li>
</ul>
<p>The following routine is meant to execute pending entries in upward message queues. This function doesn't fail, even if
dispatching any of individual upward messages returns an error.</p>
<p><code>process_pending_upward_messages()</code>:</p>
<ol>
<li>Initialize a cumulative weight counter <code>T</code> to 0</li>
<li>Iterate over items in <code>NeedsDispatch</code> cyclically, starting with <code>NextDispatchRoundStartWith</code>. If the item specified is <code>None</code> start from the beginning. For each <code>P</code> encountered:</li>
<li>Dequeue the first upward message <code>D</code> from <code>RelayDispatchQueues</code> for <code>P</code></li>
<li>Decrement the size of the message from <code>RelayDispatchQueueSize</code> for <code>P</code></li>
<li>Delegate processing of the message to the runtime. The weight consumed is added to <code>T</code>.</li>
<li>If <code>T &gt;= config.ump_service_total_weight</code>, set <code>NextDispatchRoundStartWith</code> to <code>P</code> and finish processing.</li>
<li>If <code>RelayDispatchQueues</code> for <code>P</code> became empty, remove <code>P</code> from <code>NeedsDispatch</code>.</li>
<li>If <code>NeedsDispatch</code> became empty then finish processing and set <code>NextDispatchRoundStartWith</code> to <code>None</code>.</li>
</ol>
<blockquote>
<p>NOTE that in practice we would need to approach the weight calculation more thoroughly, i.e. incorporate all operations
that could take place on the course of handling these upward messages.</p>
</blockquote>
<h2 id="session-change-8"><a class="header" href="#session-change-8">Session Change</a></h2>
<ol>
<li>For each <code>P</code> in <code>outgoing_paras</code> (generated by <code>Paras::on_new_session</code>):
<ol>
<li>Remove <code>RelayDispatchQueueSize</code> of <code>P</code>.</li>
<li>Remove <code>RelayDispatchQueues</code> of <code>P</code>.</li>
<li>Remove <code>P</code> if it exists in <code>NeedsDispatch</code>.</li>
<li>If <code>P</code> is in <code>NextDispatchRoundStartWith</code>, then reset it to <code>None</code></li>
</ol>
<ul>
<li>Note that if we don't remove the open/close requests since they are going to die out naturally at the end of the session.</li>
</ul>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hrmp-module"><a class="header" href="#hrmp-module">HRMP Module</a></h1>
<p>A module responsible for Horizontally Relay-routed Message Passing (HRMP). See <a href="runtime/../messaging.html">Messaging Overview</a> for more details.</p>
<h2 id="storage-10"><a class="header" href="#storage-10">Storage</a></h2>
<p>HRMP related structs:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// A description of a request to open an HRMP channel.
struct HrmpOpenChannelRequest {
    /// Indicates if this request was confirmed by the recipient.
    confirmed: bool,
    /// How many session boundaries ago this request was seen.
    age: SessionIndex,
    /// The amount that the sender supplied at the time of creation of this request.
    sender_deposit: Balance,
    /// The maximum message size that could be put into the channel.
    max_message_size: u32,
    /// The maximum number of messages that can be pending in the channel at once.
    max_capacity: u32,
    /// The maximum total size of the messages that can be pending in the channel at once.
    max_total_size: u32,
}

/// A metadata of an HRMP channel.
struct HrmpChannel {
    /// The amount that the sender supplied as a deposit when opening this channel.
    sender_deposit: Balance,
    /// The amount that the recipient supplied as a deposit when accepting opening this channel.
    recipient_deposit: Balance,
    /// The maximum number of messages that can be pending in the channel at once.
    max_capacity: u32,
    /// The maximum total size of the messages that can be pending in the channel at once.
    max_total_size: u32,
    /// The maximum message size that could be put into the channel.
    max_message_size: u32,
    /// The current number of messages pending in the channel.
    /// Invariant: should be less or equal to `max_capacity`.
    msg_count: u32,
    /// The total size in bytes of all message payloads in the channel.
    /// Invariant: should be less or equal to `max_total_size`.
    total_size: u32,
    /// A head of the Message Queue Chain for this channel. Each link in this chain has a form:
    /// `(prev_head, B, H(M))`, where
    /// - `prev_head`: is the previous value of `mqc_head` or zero if none.
    /// - `B`: is the [relay-chain] block number in which a message was appended
    /// - `H(M)`: is the hash of the message being appended.
    /// This value is initialized to a special value that consists of all zeroes which indicates
    /// that no messages were previously added.
    mqc_head: Option&lt;Hash&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<p>HRMP related storage layout</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// The set of pending HRMP open channel requests.
///
/// The set is accompanied by a list for iteration.
///
/// Invariant:
/// - There are no channels that exists in list but not in the set and vice versa.
HrmpOpenChannelRequests: map HrmpChannelId =&gt; Option&lt;HrmpOpenChannelRequest&gt;;
HrmpOpenChannelRequestsList: Vec&lt;HrmpChannelId&gt;;

/// This mapping tracks how many open channel requests are inititated by a given sender para.
/// Invariant: `HrmpOpenChannelRequests` should contain the same number of items that has `(X, _)`
/// as the number of `HrmpOpenChannelRequestCount` for `X`.
HrmpOpenChannelRequestCount: map ParaId =&gt; u32;
/// This mapping tracks how many open channel requests were accepted by a given recipient para.
/// Invariant: `HrmpOpenChannelRequests` should contain the same number of items `(_, X)` with
/// `confirmed` set to true, as the number of `HrmpAcceptedChannelRequestCount` for `X`.
HrmpAcceptedChannelRequestCount: map ParaId =&gt; u32;

/// A set of pending HRMP close channel requests that are going to be closed during the session change.
/// Used for checking if a given channel is registered for closure.
///
/// The set is accompanied by a list for iteration.
///
/// Invariant:
/// - There are no channels that exists in list but not in the set and vice versa.
HrmpCloseChannelRequests: map HrmpChannelId =&gt; Option&lt;()&gt;;
HrmpCloseChannelRequestsList: Vec&lt;HrmpChannelId&gt;;

/// The HRMP watermark associated with each para.
/// Invariant:
/// - each para `P` used here as a key should satisfy `Paras::is_valid_para(P)` within a session.
HrmpWatermarks: map ParaId =&gt; Option&lt;BlockNumber&gt;;
/// HRMP channel data associated with each para.
/// Invariant:
/// - each participant in the channel should satisfy `Paras::is_valid_para(P)` within a session.
HrmpChannels: map HrmpChannelId =&gt; Option&lt;HrmpChannel&gt;;
/// Ingress/egress indexes allow to find all the senders and receivers given the opposite
/// side. I.e.
///
/// (a) ingress index allows to find all the senders for a given recipient.
/// (b) egress index allows to find all the recipients for a given sender.
///
/// Invariants:
/// - for each ingress index entry for `P` each item `I` in the index should present in `HrmpChannels`
///   as `(I, P)`.
/// - for each egress index entry for `P` each item `E` in the index should present in `HrmpChannels`
///   as `(P, E)`.
/// - there should be no other dangling channels in `HrmpChannels`.
/// - the vectors are sorted.
HrmpIngressChannelsIndex: map ParaId =&gt; Vec&lt;ParaId&gt;;
HrmpEgressChannelsIndex: map ParaId =&gt; Vec&lt;ParaId&gt;;
/// Storage for the messages for each channel.
/// Invariant: cannot be non-empty if the corresponding channel in `HrmpChannels` is `None`.
HrmpChannelContents: map HrmpChannelId =&gt; Vec&lt;InboundHrmpMessage&gt;;
/// Maintains a mapping that can be used to answer the question:
/// What paras sent a message at the given block number for a given reciever.
/// Invariants:
/// - The inner `Vec&lt;ParaId&gt;` is never empty.
/// - The inner `Vec&lt;ParaId&gt;` cannot store two same `ParaId`.
/// - The outer vector is sorted ascending by block number and cannot store two items with the same
///   block number.
HrmpChannelDigests: map ParaId =&gt; Vec&lt;(BlockNumber, Vec&lt;ParaId&gt;)&gt;;
<span class="boring">}
</span></code></pre></pre>
<h2 id="initialization-6"><a class="header" href="#initialization-6">Initialization</a></h2>
<p>No initialization routine runs for this module.</p>
<h2 id="routines-7"><a class="header" href="#routines-7">Routines</a></h2>
<p>Candidate Acceptance Function:</p>
<ul>
<li><code>check_hrmp_watermark(P: ParaId, new_hrmp_watermark)</code>:
<ol>
<li><code>new_hrmp_watermark</code> should be strictly greater than the value of <code>HrmpWatermarks</code> for <code>P</code> (if any).</li>
<li><code>new_hrmp_watermark</code> must not be greater than the context's block number.</li>
<li><code>new_hrmp_watermark</code> should be either
<ol>
<li>equal to the context's block number</li>
<li>or in <code>HrmpChannelDigests</code> for <code>P</code> an entry with the block number should exist</li>
</ol>
</li>
</ol>
</li>
<li><code>check_outbound_hrmp(sender: ParaId, Vec&lt;OutboundHrmpMessage&gt;)</code>:
<ol>
<li>Checks that there are at most <code>config.hrmp_max_message_num_per_candidate</code> messages.</li>
<li>Checks that horizontal messages are sorted by ascending recipient ParaId and there is no two horizontal messages have the same recipient.</li>
<li>For each horizontal message <code>M</code> with the channel <code>C</code> identified by <code>(sender, M.recipient)</code> check:
<ol>
<li>exists</li>
<li><code>M</code>'s payload size doesn't exceed a preconfigured limit <code>C.max_message_size</code></li>
<li><code>M</code>'s payload size summed with the <code>C.total_size</code> doesn't exceed a preconfigured limit <code>C.max_total_size</code>.</li>
<li><code>C.msg_count + 1</code> doesn't exceed a preconfigured limit <code>C.max_capacity</code>.</li>
</ol>
</li>
</ol>
</li>
</ul>
<p>Candidate Enactment:</p>
<ul>
<li><code>queue_outbound_hrmp(sender: ParaId, Vec&lt;OutboundHrmpMessage&gt;)</code>:
<ol>
<li>For each horizontal message <code>HM</code> with the channel <code>C</code> identified by <code>(sender, HM.recipient)</code>:
<ol>
<li>Append <code>HM</code> into <code>HrmpChannelContents</code> that corresponds to <code>C</code> with <code>sent_at</code> equals to the current block number.</li>
<li>Locate or create an entry in <code>HrmpChannelDigests</code> for <code>HM.recipient</code> and append <code>sender</code> into the entry's list.</li>
<li>Increment <code>C.msg_count</code></li>
<li>Increment <code>C.total_size</code> by <code>HM</code>'s payload size</li>
<li>Append a new link to the MQC and save the new head in <code>C.mqc_head</code>. Note that the current block number as of enactment is used for the link.</li>
</ol>
</li>
</ol>
</li>
<li><code>prune_hrmp(recipient, new_hrmp_watermark)</code>:
<ol>
<li>From <code>HrmpChannelDigests</code> for <code>recipient</code> remove all entries up to an entry with block number equal to <code>new_hrmp_watermark</code>.</li>
<li>From the removed digests construct a set of paras that sent new messages within the interval between the old and new watermarks.</li>
<li>For each channel <code>C</code> identified by <code>(sender, recipient)</code> for each <code>sender</code> coming from the set, prune messages up to the <code>new_hrmp_watermark</code>.</li>
<li>For each pruned message <code>M</code> from channel <code>C</code>:
<ol>
<li>Decrement <code>C.msg_count</code></li>
<li>Decrement <code>C.total_size</code> by <code>M</code>'s payload size.</li>
</ol>
</li>
<li>Set <code>HrmpWatermarks</code> for <code>P</code> to be equal to <code>new_hrmp_watermark</code></li>
</ol>
<blockquote>
<p>NOTE: That collecting digests can be inefficient and the time it takes grows very fast. Thanks to the aggresive
parametrization this shouldn't be a big of a deal.
If that becomes a problem consider introducing an extra dictionary which says at what block the given sender
sent a message to the recipient.</p>
</blockquote>
</li>
</ul>
<h2 id="entry-points-2"><a class="header" href="#entry-points-2">Entry-points</a></h2>
<p>The following entry-points are meant to be used for HRMP channel management.</p>
<p>Those entry-points are meant to be called from a parachain. <code>origin</code> is defined as the <code>ParaId</code> of
the parachain executed the message.</p>
<ul>
<li><code>hrmp_init_open_channel(recipient, proposed_max_capacity, proposed_max_message_size)</code>:
<ol>
<li>Check that the <code>origin</code> is not <code>recipient</code>.</li>
<li>Check that <code>proposed_max_capacity</code> is less or equal to <code>config.hrmp_channel_max_capacity</code> and greater than zero.</li>
<li>Check that <code>proposed_max_message_size</code> is less or equal to <code>config.hrmp_channel_max_message_size</code> and greater than zero.</li>
<li>Check that <code>recipient</code> is a valid para.</li>
<li>Check that there is no existing channel for <code>(origin, recipient)</code> in <code>HrmpChannels</code>.</li>
<li>Check that there is no existing open channel request (<code>origin</code>, <code>recipient</code>) in <code>HrmpOpenChannelRequests</code>.</li>
<li>Check that the sum of the number of already opened HRMP channels by the <code>origin</code> (the size
of the set found <code>HrmpEgressChannelsIndex</code> for <code>origin</code>) and the number of open requests by the
<code>origin</code> (the value from <code>HrmpOpenChannelRequestCount</code> for <code>origin</code>) doesn't exceed the limit of
channels (<code>config.hrmp_max_parachain_outbound_channels</code> or <code>config.hrmp_max_parathread_outbound_channels</code>) minus 1.</li>
<li>Check that <code>origin</code>'s balance is more or equal to <code>config.hrmp_sender_deposit</code></li>
<li>Reserve the deposit for the <code>origin</code> according to <code>config.hrmp_sender_deposit</code></li>
<li>Increase <code>HrmpOpenChannelRequestCount</code> by 1 for <code>origin</code>.</li>
<li>Append <code>(origin, recipient)</code> to <code>HrmpOpenChannelRequestsList</code>.</li>
<li>Add a new entry to <code>HrmpOpenChannelRequests</code> for <code>(origin, recipient)</code>
<ol>
<li>Set <code>sender_deposit</code> to <code>config.hrmp_sender_deposit</code></li>
<li>Set <code>max_capacity</code> to <code>proposed_max_capacity</code></li>
<li>Set <code>max_message_size</code> to <code>proposed_max_message_size</code></li>
<li>Set <code>max_total_size</code> to <code>config.hrmp_channel_max_total_size</code></li>
</ol>
</li>
<li>Send a downward message to <code>recipient</code> notifying about an inbound HRMP channel request.
<ul>
<li>The DM is sent using <code>queue_downward_message</code>.</li>
<li>The DM is represented by the <code>HrmpNewChannelOpenRequest</code>  XCM message.
<ul>
<li><code>sender</code> is set to <code>origin</code>,</li>
<li><code>max_message_size</code> is set to <code>proposed_max_message_size</code>,</li>
<li><code>max_capacity</code> is set to <code>proposed_max_capacity</code>.</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
<li><code>hrmp_accept_open_channel(sender)</code>:
<ol>
<li>Check that there is an existing request between (<code>sender</code>, <code>origin</code>) in <code>HrmpOpenChannelRequests</code>
<ol>
<li>Check that it is not confirmed.</li>
</ol>
</li>
<li>Check that the sum of the number of inbound HRMP channels opened to <code>origin</code> (the size of the set
found in <code>HrmpIngressChannelsIndex</code> for <code>origin</code>) and the number of accepted open requests by the <code>origin</code>
(the value from <code>HrmpAcceptedChannelRequestCount</code> for <code>origin</code>) doesn't exceed the limit of channels
(<code>config.hrmp_max_parachain_inbound_channels</code> or <code>config.hrmp_max_parathread_inbound_channels</code>)
minus 1.</li>
<li>Check that <code>origin</code>'s balance is more or equal to <code>config.hrmp_recipient_deposit</code>.</li>
<li>Reserve the deposit for the <code>origin</code> according to <code>config.hrmp_recipient_deposit</code></li>
<li>For the request in <code>HrmpOpenChannelRequests</code> identified by <code>(sender, P)</code>, set <code>confirmed</code> flag to <code>true</code>.</li>
<li>Increase <code>HrmpAcceptedChannelRequestCount</code> by 1 for <code>origin</code>.</li>
<li>Send a downward message to <code>sender</code> notifying that the channel request was accepted.
<ul>
<li>The DM is sent using <code>queue_downward_message</code>.</li>
<li>The DM is represented by the <code>HrmpChannelAccepted</code> XCM message.
<ul>
<li><code>recipient</code> is set to <code>origin</code>.</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
<li><code>hrmp_close_channel(ch)</code>:
<ol>
<li>Check that <code>origin</code> is either <code>ch.sender</code> or <code>ch.recipient</code></li>
<li>Check that <code>HrmpChannels</code> for <code>ch</code> exists.</li>
<li>Check that <code>ch</code> is not in the <code>HrmpCloseChannelRequests</code> set.</li>
<li>If not already there, insert a new entry <code>Some(())</code> to <code>HrmpCloseChannelRequests</code> for <code>ch</code>
and append <code>ch</code> to <code>HrmpCloseChannelRequestsList</code>.</li>
<li>Send a downward message to the opposite party notifying about the channel closing.
<ul>
<li>The DM is sent using <code>queue_downward_message</code>.</li>
<li>The DM is represented by the <code>HrmpChannelClosing</code> XCM message with:
<ul>
<li><code>initator</code> is set to <code>origin</code>,</li>
<li><code>sender</code> is set to <code>ch.sender</code>,</li>
<li><code>recipient</code> is set to <code>ch.recipient</code>.</li>
</ul>
</li>
<li>The opposite party is <code>ch.sender</code> if <code>origin</code> is <code>ch.recipient</code> and <code>ch.recipient</code> if <code>origin</code> is <code>ch.sender</code>.</li>
</ul>
</li>
</ol>
</li>
</ul>
<h2 id="session-change-9"><a class="header" href="#session-change-9">Session Change</a></h2>
<ol>
<li>For each <code>P</code> in <code>outgoing_paras</code> (generated by <code>Paras::on_new_session</code>):
<ol>
<li>Remove all inbound channels of <code>P</code>, i.e. <code>(_, P)</code>,</li>
<li>Remove all outbound channels of <code>P</code>, i.e. <code>(P, _)</code>,</li>
<li>Remove <code>HrmpOpenChannelRequestCount</code> for <code>P</code></li>
<li>Remove <code>HrmpAcceptedChannelRequestCount</code> for <code>P</code>.</li>
</ol>
</li>
<li>For each channel designator <code>D</code> in <code>HrmpOpenChannelRequestsList</code> we query the request <code>R</code> from <code>HrmpOpenChannelRequests</code>:
<ol>
<li>if <code>R.confirmed = false</code>:
<ol>
<li>increment <code>R.age</code> by 1.</li>
<li>if <code>R.age</code> reached a preconfigured time-to-live limit <code>config.hrmp_open_request_ttl</code>, then:
<ol>
<li>refund <code>R.sender_deposit</code> to the sender</li>
<li>decrement <code>HrmpOpenChannelRequestCount</code> for <code>D.sender</code> by 1.</li>
<li>remove <code>R</code></li>
<li>remove <code>D</code></li>
</ol>
</li>
</ol>
</li>
<li>if <code>R.confirmed = true</code>,
<ol>
<li>if both <code>D.sender</code> and <code>D.recipient</code> are not offboarded.</li>
<li>create a new channel <code>C</code> between <code>(D.sender, D.recipient)</code>.
<ol>
<li>Initialize the <code>C.sender_deposit</code> with <code>R.sender_deposit</code> and <code>C.recipient_deposit</code>
with the value found in the configuration <code>config.hrmp_recipient_deposit</code>.</li>
<li>Insert <code>sender</code> into the set <code>HrmpIngressChannelsIndex</code> for the <code>recipient</code>.</li>
<li>Insert <code>recipient</code> into the set <code>HrmpEgressChannelsIndex</code> for the <code>sender</code>.</li>
</ol>
</li>
<li>decrement <code>HrmpOpenChannelRequestCount</code> for <code>D.sender</code> by 1.</li>
<li>decrement <code>HrmpAcceptedChannelRequestCount</code> for <code>D.recipient</code> by 1.</li>
<li>remove <code>R</code></li>
<li>remove <code>D</code></li>
</ol>
</li>
</ol>
</li>
<li>For each HRMP channel designator <code>D</code> in <code>HrmpCloseChannelRequestsList</code>
<ol>
<li>remove the channel identified by <code>D</code>, if exists.</li>
<li>remove <code>D</code> from <code>HrmpCloseChannelRequests</code>.</li>
<li>remove <code>D</code> from <code>HrmpCloseChannelRequestsList</code></li>
</ol>
</li>
</ol>
<p>To remove a HRMP channel <code>C</code> identified with a tuple <code>(sender, recipient)</code>:</p>
<ol>
<li>Return <code>C.sender_deposit</code> to the <code>sender</code>.</li>
<li>Return <code>C.recipient_deposit</code> to the <code>recipient</code>.</li>
<li>Remove <code>C</code> from <code>HrmpChannels</code>.</li>
<li>Remove <code>C</code> from <code>HrmpChannelContents</code>.</li>
<li>Remove <code>recipient</code> from the set <code>HrmpEgressChannelsIndex</code> for <code>sender</code>.</li>
<li>Remove <code>sender</code> from the set <code>HrmpIngressChannelsIndex</code> for <code>recipient</code>.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="session-info"><a class="header" href="#session-info">Session Info</a></h1>
<p>For disputes and approvals, we need access to information about validator sets from prior sessions. We also often want easy access to the same information about the current session's validator set. This module aggregates and stores this information in a rolling window while providing easy APIs for access.</p>
<h2 id="storage-11"><a class="header" href="#storage-11">Storage</a></h2>
<p>Helper structs:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct SessionInfo {
    // validators in canonical ordering. These are the public keys used for backing,
    // dispute participation, and approvals.
    validators: Vec&lt;ValidatorId&gt;,
    // validators' authority discovery keys for the session in canonical ordering.
    discovery_keys: Vec&lt;DiscoveryId&gt;,
    // The assignment keys for validators.
    assignment_keys: Vec&lt;AssignmentId&gt;,
    // validators in shuffled ordering - these are the validator groups as produced
    // by the `Scheduler` module for the session and are typically referred to by
    // `GroupIndex`.
    validator_groups: Vec&lt;Vec&lt;ValidatorIndex&gt;&gt;,
    // The number of availability cores used by the protocol during this session.
    n_cores: u32,
    // the zeroth delay tranche width.
    zeroth_delay_tranche_width: u32,
    // The number of samples we do of relay_vrf_modulo.
    relay_vrf_modulo_samples: u32,
    // The number of delay tranches in total.
    n_delay_tranches: u32,
    // How many slots (BABE / SASSAFRAS) must pass before an assignment is considered a
    // no-show.
    no_show_slots: u32,
    /// The number of validators needed to approve a block.
    needed_approvals: u32,
}
<span class="boring">}
</span></code></pre></pre>
<p>Storage Layout:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// The earliest session for which previous session info is stored.
EarliestStoredSession: SessionIndex,
/// Session information. Should have an entry from `EarliestStoredSession..=CurrentSessionIndex`
Sessions: map SessionIndex =&gt; Option&lt;SessionInfo&gt;,
<span class="boring">}
</span></code></pre></pre>
<h2 id="session-change-10"><a class="header" href="#session-change-10">Session Change</a></h2>
<ol>
<li>Update <code>EarliestStoredSession</code> based on <code>config.dispute_period</code> and remove all entries from <code>Sessions</code> from the previous value up to the new value.</li>
<li>Create a new entry in <code>Sessions</code> with information about the current session. Use <code>shared::ActiveValidators</code> to determine the indices into the broader validator sets (validation, assignment, discovery) which are actually used for parachain validation. Only these validators should appear in the <code>SessionInfo</code>.</li>
</ol>
<h2 id="routines-8"><a class="header" href="#routines-8">Routines</a></h2>
<ul>
<li><code>earliest_stored_session() -&gt; SessionIndex</code>: Yields the earliest session for which we have information stored.</li>
<li><code>session_info(session: SessionIndex) -&gt; Option&lt;SessionInfo&gt;</code>: Yields the session info for the given session, if stored.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="runtime-apis"><a class="header" href="#runtime-apis">Runtime APIs</a></h1>
<p>Runtime APIs are the means by which the node-side code extracts information from the state of the runtime.</p>
<p>Every block in the relay-chain contains a <em>state root</em> which is the root hash of a state trie encapsulating all storage of runtime modules after execution of the block. This is a cryptographic commitment to a unique state. We use the terminology of accessing the <em>state at</em> a block to refer accessing the state referred to by the state root of that block.</p>
<p>Although Runtime APIs are often used for simple storage access, they are actually empowered to do arbitrary computation. The implementation of the Runtime APIs lives within the Runtime as Wasm code and exposes extern functions that can be invoked with arguments and have a return value. Runtime APIs have access to a variety of host functions, which are contextual functions provided by the Wasm execution context, that allow it to carry out many different types of behaviors.</p>
<p>Abilities provided by host functions includes:</p>
<ul>
<li>State Access</li>
<li>Offchain-DB Access</li>
<li>Submitting transactions to the transaction queue</li>
<li>Optimized versions of cryptographic functions</li>
<li>More</li>
</ul>
<p>So it is clear that Runtime APIs are a versatile and powerful tool to leverage the state of the chain. In general, we will use Runtime APIs for these purposes:</p>
<ul>
<li>Access of a storage item</li>
<li>Access of a bundle of related storage items</li>
<li>Deriving a value from storage based on arguments</li>
<li>Submitting misbehavior reports</li>
</ul>
<p>More broadly, we have the goal of using Runtime APIs to write Node-side code that fulfills the requirements set by the Runtime. In particular, the constraints set forth by the <a href="runtime-api/../runtime/scheduler.html">Scheduler</a> and <a href="runtime-api/../runtime/inclusion.html">Inclusion</a> modules. These modules are responsible for advancing paras with a two-phase protocol where validators are first chosen to validate and back a candidate and then required to ensure availability of referenced data. In the second phase, validators are meant to attest to those para-candidates that they have their availability chunk for. As the Node-side code needs to generate the inputs into these two phases, the runtime API needs to transmit information from the runtime that is aware of the Availability Cores model instantiated by the Scheduler and Inclusion modules.</p>
<p>Node-side code is also responsible for detecting and reporting misbehavior performed by other validators, and the set of Runtime APIs needs to provide methods for observing live disputes and submitting reports as transactions.</p>
<p>The next sections will contain information on specific runtime APIs. The format is this:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Fetch the value of the runtime API at the block.
///
/// Definitionally, the `at` parameter cannot be any block that is not in the chain.
/// Thus the return value is unconditional. However, for in-practice implementations
/// it may be possible to provide an `at` parameter as a hash, which may not refer to a
/// valid block or one which implements the runtime API. In those cases it would be
/// best for the implementation to return an error indicating the failure mode.
fn some_runtime_api(at: Block, arg1: Type1, arg2: Type2, ...) -&gt; ReturnValue;
<span class="boring">}
</span></code></pre></pre>
<p>Certain runtime APIs concerning the state of a para require the caller to provide an <code>OccupiedCoreAssumption</code>. This indicates how the result of the runtime API should be computed if there is a candidate from the para occupying an availability core in the <a href="runtime-api/../runtime/inclusion.html">Inclusion Module</a>.</p>
<p>The choices of assumption are whether the candidate occupying that core should be assumed to have been made available and included or timed out and discarded, along with a third option to assert that the core was not occupied. This choice affects everything from the parent head-data, the validation code, and the state of message-queues. Typically, users will take the assumption that either the core was free or that the occupying candidate was included, as timeouts are expected only in adversarial circumstances and even so, only in a small minority of blocks directly following validator set rotations.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// An assumption being made about the state of an occupied core.
enum OccupiedCoreAssumption {
    /// The candidate occupying the core was made available and included to free the core.
    Included,
    /// The candidate occupying the core timed out and freed the core without advancing the para.
    TimedOut,
    /// The core was not occupied to begin with.
    Free,
}
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="validators"><a class="header" href="#validators">Validators</a></h1>
<p>Yields the validator-set at the state of a given block. This validator set is always the one responsible for backing parachains in the child of the provided block.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn validators(at: Block) -&gt; Vec&lt;ValidatorId&gt;;
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="validator-groups-1"><a class="header" href="#validator-groups-1">Validator Groups</a></h1>
<p>Yields the validator groups used during the current session. The validators in the groups are referred to by their index into the validator-set and this is assumed to be as-of the child of the block whose state is being queried.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// A helper data-type for tracking validator-group rotations.
struct GroupRotationInfo {
    session_start_block: BlockNumber,
    group_rotation_frequency: BlockNumber,
    now: BlockNumber, // The successor of the block in whose state this runtime API is queried.
}

impl GroupRotationInfo {
    /// Returns the index of the group needed to validate the core at the given index,
    /// assuming the given amount of cores/groups.
    fn group_for_core(&amp;self, core_index, cores) -&gt; GroupIndex;

    /// Returns the block number of the next rotation after the current block. If the current block
    /// is 10 and the rotation frequency is 5, this should return 15.
    fn next_rotation_at(&amp;self) -&gt; BlockNumber;

    /// Returns the block number of the last rotation before or including the current block. If the
    /// current block is 10 and the rotation frequency is 5, this should return 10.
    fn last_rotation_at(&amp;self) -&gt; BlockNumber;
}

/// Returns the validator groups and rotation info localized based on the block whose state
/// this is invoked on. Note that `now` in the `GroupRotationInfo` should be the successor of
/// the number of the block.
fn validator_groups(at: Block) -&gt; (Vec&lt;Vec&lt;ValidatorIndex&gt;&gt;, GroupRotationInfo);
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="availability-cores-1"><a class="header" href="#availability-cores-1">Availability Cores</a></h1>
<p>Yields information on all availability cores. Cores are either free or occupied. Free cores can have paras assigned to them. Occupied cores don't, but they can become available part-way through a block due to bitfields and then have something scheduled on them. To allow optimistic validation of candidates, the occupied cores are accompanied by information on what is upcoming. This information can be leveraged when validators perceive that there is a high likelihood of a core becoming available based on bitfields seen, and then optimistically validate something that would become scheduled based on that, although there is no guarantee on what the block producer will actually include in the block. </p>
<p>See also the <a href="runtime-api/../runtime/scheduler.html">Scheduler Module</a> for a high-level description of what an availability core is and why it exists.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn availability_cores(at: Block) -&gt; Vec&lt;CoreState&gt;;
<span class="boring">}
</span></code></pre></pre>
<p>This is all the information that a validator needs about scheduling for the current block. It includes all information on <a href="runtime-api/../runtime/scheduler.html">Scheduler</a> core-assignments and <a href="runtime-api/../runtime/inclusion.html">Inclusion</a> state of blocks occupying availability cores. It includes data necessary to determine not only which paras are assigned now, but which cores are likely to become freed after processing bitfields, and exactly which bitfields would be necessary to make them so.  The implementation of this runtime API should invoke <code>Scheduler::clear</code> and <code>Scheduler::schedule(Vec::new(), current_block_number + 1)</code> to ensure that scheduling is accurate.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct OccupiedCore {
    // NOTE: this has no ParaId as it can be deduced from the candidate descriptor.
    /// If this core is freed by availability, this is the assignment that is next up on this
    /// core, if any. None if there is nothing queued for this core.
    next_up_on_available: Option&lt;ScheduledCore&gt;,
    /// The relay-chain block number this began occupying the core at.
    occupied_since: BlockNumber,
    /// The relay-chain block this will time-out at, if any.
    time_out_at: BlockNumber,
    /// If this core is freed by being timed-out, this is the assignment that is next up on this
    /// core. None if there is nothing queued for this core or there is no possibility of timing
    /// out.
    next_up_on_time_out: Option&lt;ScheduledCore&gt;,
    /// A bitfield with 1 bit for each validator in the set. `1` bits mean that the corresponding
    /// validators has attested to availability on-chain. A 2/3+ majority of `1` bits means that
    /// this will be available.
    availability: Bitfield,
    /// The group assigned to distribute availability pieces of this candidate.
    group_responsible: GroupIndex,
    /// The hash of the candidate occupying the core.
    candidate_hash: CandidateHash,
    /// The descriptor of the candidate occupying the core.
    candidate_descriptor: CandidateDescriptor,
}

struct ScheduledCore {
    /// The ID of a para scheduled.
    para_id: ParaId,
    /// The collator required to author the block, if any.
    collator: Option&lt;CollatorId&gt;,
}

enum CoreState {
    /// The core is currently occupied.
    Occupied(OccupiedCore),
    /// The core is currently free, with a para scheduled and given the opportunity
    /// to occupy.
    ///
    /// If a particular Collator is required to author this block, that is also present in this
    /// variant.
    Scheduled(ScheduledCore),
    /// The core is currently free and there is nothing scheduled. This can be the case for parathread
    /// cores when there are no parathread blocks queued. Parachain cores will never be left idle.
    Free,
}
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="persisted-validation-data"><a class="header" href="#persisted-validation-data">Persisted Validation Data</a></h1>
<p>Yields the <a href="runtime-api/../types/candidate.html#persistedvalidationdata"><code>PersistedValidationData</code></a> for the given <a href="runtime-api/../types/candidate.html#paraid"><code>ParaId</code></a> along with an assumption that should be used if the para currently occupies a core:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Returns the persisted validation data for the given para and occupied core assumption.
///
/// Returns `None` if either the para is not registered or the assumption is `Freed`
/// and the para already occupies a core.
fn persisted_validation_data(at: Block, ParaId, OccupiedCoreAssumption) -&gt; Option&lt;PersistedValidationData&gt;;
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="session-index"><a class="header" href="#session-index">Session Index</a></h1>
<p>Get the session index that is expected at the child of a block.</p>
<p>In the <a href="runtime-api/../runtime/initializer.html"><code>Initializer</code></a> module, session changes are buffered by one block. The session index of the child of any relay block is always predictable by that block's state.</p>
<p>This session index can be used to derive a <a href="runtime-api/../types/candidate.html#signing-context"><code>SigningContext</code></a>.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Returns the session index expected at a child of the block.
fn session_index_for_child(at: Block) -&gt; SessionIndex;
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="validation-code"><a class="header" href="#validation-code">Validation Code</a></h1>
<p>Fetch the validation code used by a para, making the given <code>OccupiedCoreAssumption</code>.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn validation_code(at: Block, ParaId, OccupiedCoreAssumption) -&gt; Option&lt;ValidationCode&gt;;
<span class="boring">}
</span></code></pre></pre>
<p>Fetch the validation code (past, present or future) by its hash.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn validation_code_by_hash(at: Block, ValidationCodeHash) -&gt; Option&lt;ValidationCode&gt;;
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="candidate-pending-availability"><a class="header" href="#candidate-pending-availability">Candidate Pending Availability</a></h1>
<p>Get the receipt of a candidate pending availability. This returns <code>Some</code> for any paras assigned to occupied cores in <code>availability_cores</code> and <code>None</code> otherwise.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn candidate_pending_availability(at: Block, ParaId) -&gt; Option&lt;CommittedCandidateReceipt&gt;;
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="candidate-events"><a class="header" href="#candidate-events">Candidate Events</a></h1>
<p>Yields a vector of events concerning candidates that occurred within the given block.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum CandidateEvent {
	/// This candidate receipt was backed in the most recent block.
	CandidateBacked(CandidateReceipt, HeadData, CoreIndex, GroupIndex),
	/// This candidate receipt was included and became a parablock at the most recent block.
	CandidateIncluded(CandidateReceipt, HeadData, CoreIndex, GroupIndex),
	/// This candidate receipt was not made available in time and timed out.
	CandidateTimedOut(CandidateReceipt, HeadData, CoreIndex),
}

fn candidate_events(at: Block) -&gt; Vec&lt;CandidateEvent&gt;;
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="disputes-info"><a class="header" href="#disputes-info">Disputes Info</a></h1>
<p>Get information about all disputes known by the chain as well as information about which validators the disputes subsystem will accept disputes from. These disputes may be either live or concluded. The <a href="runtime-api/../types/disputes.html#disputestate"><code>DisputeState</code></a> can be used to determine whether the dispute still accepts votes, as well as which validators' votes may be included.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct Dispute {
    session: SessionIndex,
    candidate: CandidateHash,
    dispute_state: DisputeState,
    local: bool,
}

struct SpamSlotsInfo {
    max_spam_slots: u32,
    session_spam_slots: Vec&lt;(SessionIndex, Vec&lt;u32&gt;)&gt;,
}

struct DisputesInfo {
    disputes: Vec&lt;Dispute&gt;,
    spam_slots: SpamSlotsInfo,
}

fn disputes_info() -&gt; DisputesInfo;
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="candidates-included"><a class="header" href="#candidates-included">Candidates Included</a></h1>
<p>This runtime API is for checking which candidates have been included within the chain, locally.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Input and output have the same length.
fn candidates_included(Vec&lt;(SessionIndex, CandidateHash)&gt;) -&gt; Vec&lt;bool&gt;;
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="node-architecture"><a class="header" href="#node-architecture">Node Architecture</a></h1>
<h2 id="design-goals"><a class="header" href="#design-goals">Design Goals</a></h2>
<ul>
<li>Modularity: Components of the system should be as self-contained as possible. Communication boundaries between components should be well-defined and mockable. This is key to creating testable, easily reviewable code.</li>
<li>Minimizing side effects: Components of the system should aim to minimize side effects and to communicate with other components via message-passing.</li>
<li>Operational Safety: The software will be managing signing keys where conflicting messages can lead to large amounts of value to be slashed. Care should be taken to ensure that no messages are signed incorrectly or in conflict with each other.</li>
</ul>
<p>The architecture of the node-side behavior aims to embody the Rust principles of ownership and message-passing to create clean, isolatable code. Each resource should have a single owner, with minimal sharing where unavoidable.</p>
<p>Many operations that need to be carried out involve the network, which is asynchronous. This asynchrony affects all core subsystems that rely on the network as well. The approach of hierarchical state machines is well-suited to this kind of environment.</p>
<p>We introduce</p>
<h2 id="components"><a class="header" href="#components">Components</a></h2>
<p>The node architecture consists of the following components:</p>
<ul>
<li>The Overseer (and subsystems): A hierarchy of state machines where an overseer supervises subsystems. Subsystems can contain their own internal hierarchy of jobs. This is elaborated on in the next section on Subsystems.</li>
<li>A block proposer: Logic triggered by the consensus algorithm of the chain when the node should author a block.</li>
<li>A GRANDPA voting rule: A strategy for selecting chains to vote on in the GRANDPA algorithm to ensure that only valid parachain candidates appear in finalized relay-chain blocks.</li>
</ul>
<h2 id="assumptions"><a class="header" href="#assumptions">Assumptions</a></h2>
<p>The Node-side code comes with a set of assumptions that we build upon. These assumptions encompass most of the fundamental blockchain functionality.</p>
<p>We assume the following constraints regarding provided basic functionality:</p>
<ul>
<li>The underlying <strong>consensus</strong> algorithm, whether it is BABE or SASSAFRAS is implemented.</li>
<li>There is a <strong>chain synchronization</strong> protocol which will search for and download the longest available chains at all times.</li>
<li>The <strong>state</strong> of all blocks at the head of the chain is available. There may be <strong>state pruning</strong> such that state of the last <code>k</code> blocks behind the last finalized block are available, as well as the state of all their descendents. This assumption implies that the state of all active leaves and their last <code>k</code> ancestors are all available. The underlying implementation is expected to support <code>k</code> of a few hundred blocks, but we reduce this to a very conservative <code>k=5</code> for our purposes.</li>
<li>There is an underlying <strong>networking</strong> framework which provides <strong>peer discovery</strong> services which will provide us with peers and will not create &quot;loopback&quot; connections to our own node. The number of peers we will have is assumed to be bounded at 1000.</li>
<li>There is a <strong>transaction pool</strong> and a <strong>transaction propagation</strong> mechanism which maintains a set of current transactions and distributes to connected peers. Current transactions are those which are not outdated relative to some &quot;best&quot; fork of the chain, which is part of the active heads, and have not been included in the best fork.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="subsystems-and-jobs"><a class="header" href="#subsystems-and-jobs">Subsystems and Jobs</a></h1>
<p>In this section we define the notions of Subsystems and Jobs. These are guidelines for how we will employ an architecture of hierarchical state machines. We'll have a top-level state machine which oversees the next level of state machines which oversee another layer of state machines and so on. The next sections will lay out these guidelines for what we've called subsystems and jobs, since this model applies to many of the tasks that the Node-side behavior needs to encompass, but these are only guidelines and some Subsystems may have deeper hierarchies internally.</p>
<p>Subsystems are long-lived worker tasks that are in charge of performing some particular kind of work. All subsystems can communicate with each other via a well-defined protocol. Subsystems can't generally communicate directly, but must coordinate communication through an <a href="node/overseer.html">Overseer</a>, which is responsible for relaying messages, handling subsystem failures, and dispatching work signals.</p>
<p>Most work that happens on the Node-side is related to building on top of a specific relay-chain block, which is contextually known as the &quot;relay parent&quot;. We call it the relay parent to explicitly denote that it is a block in the relay chain and not on a parachain. We refer to the parent because when we are in the process of building a new block, we don't know what that new block is going to be. The parent block is our only stable point of reference, even though it is usually only useful when it is not yet a parent but in fact a leaf of the block-DAG expected to soon become a parent (because validators are authoring on top of it). Furthermore, we are assuming a forkful blockchain-extension protocol, which means that there may be multiple possible children of the relay-parent. Even if the relay parent has multiple children blocks, the parent of those children is the same, and the context in which those children is authored should be the same. The parent block is the best and most stable reference to use for defining the scope of work items and messages, and is typically referred to by its cryptographic hash.</p>
<p>Since this goal of determining when to start and conclude work relative to a specific relay-parent is common to most, if not all subsystems, it is logically the job of the Overseer to distribute those signals as opposed to each subsystem duplicating that effort, potentially being out of synchronization with each other. Subsystem A should be able to expect that subsystem B is working on the same relay-parents as it is. One of the Overseer's tasks is to provide this heartbeat, or synchronized rhythm, to the system.</p>
<p>The work that subsystems spawn to be done on a specific relay-parent is known as a job. Subsystems should set up and tear down jobs according to the signals received from the overseer. Subsystems may share or cache state between jobs.</p>
<p>Subsystems must be robust to spurious exits. The outputs of the set of subsystems as a whole comprises of signed messages and data committed to disk. Care must be taken to avoid issuing messages that are not substantiated. Since subsystems need to be safe under spurious exits, it is the expected behavior that an <code>OverseerSignal::Conclude</code> can just lead to breaking the loop and exiting directly as opposed to waiting for everything to shut down gracefully.</p>
<h2 id="subsystem-message-traffic"><a class="header" href="#subsystem-message-traffic">Subsystem Message Traffic</a></h2>
<p>Which subsystems send messages to which other subsystems.</p>
<p><strong>Note</strong>: This diagram omits the overseer for simplicity. In fact, all messages are relayed via the overseer.</p>
<p><strong>Note</strong>: Messages with a filled diamond arrowhead (&quot;♦&quot;) include a <code>oneshot::Sender</code> which communicates a response from the recipient.
Messages with an open triangle arrowhead (&quot;Δ&quot;) do not include a return sender.</p>
<p><img src="node/subsystems_and_jobs_0.generated.svg" alt="" /></p>
<h2 id="the-path-to-inclusion-node-side"><a class="header" href="#the-path-to-inclusion-node-side">The Path to Inclusion (Node Side)</a></h2>
<p>Let's contextualize that diagram a bit by following a parachain block from its creation through finalization.
Parachains can use completely arbitrary processes to generate blocks. The relay chain doesn't know or care about
the details; each parachain just needs to provide a <a href="node/collators/collation-generation.html">collator</a>.</p>
<p><strong>Note</strong>: Inter-subsystem communications are relayed via the overseer, but that step is omitted here for brevity.</p>
<p><strong>Note</strong>: Dashed lines indicate a request/response cycle, where the response is communicated asynchronously via
a oneshot channel. Adjacent dashed lines may be processed in parallel.</p>
<pre class="mermaid">sequenceDiagram
    participant Overseer
    participant CollationGeneration
    participant RuntimeApi
    participant CollatorProtocol

    Overseer -&gt;&gt; CollationGeneration: ActiveLeavesUpdate
    loop for each activated head
        CollationGeneration --&gt;&gt; RuntimeApi: Request availability cores
        CollationGeneration --&gt;&gt; RuntimeApi: Request validators

        Note over CollationGeneration: Determine an appropriate ScheduledCore &lt;br/&gt;and OccupiedCoreAssumption

        CollationGeneration --&gt;&gt; RuntimeApi: Request full validation data

        Note over CollationGeneration: Build the collation

        CollationGeneration -&gt;&gt; CollatorProtocol: DistributeCollation
    end
</pre>
<p>The <code>DistributeCollation</code> messages that <code>CollationGeneration</code> sends to the <code>CollatorProtocol</code> contains
two items: a <code>CandidateReceipt</code> and <code>PoV</code>. The <code>CollatorProtocol</code> is then responsible for distributing
that collation to interested validators. However, not all potential collations are of interest. The
<code>CandidateSelection</code> subsystem is responsible for determining which collations are interesting, before
<code>CollatorProtocol</code> actually fetches the collation.</p>
<pre class="mermaid">sequenceDiagram
    participant CollationGeneration
    participant CS as CollatorProtocol::CollatorSide
    participant NB as NetworkBridge
    participant VS as CollatorProtocol::ValidatorSide
    participant CandidateSelection

    CollationGeneration -&gt;&gt; CS: DistributeCollation
    CS --&gt;&gt; NB: ConnectToValidators

    Note over CS,NB: This connects to multiple validators.

    CS -&gt;&gt; NB: Declare
    NB -&gt;&gt; VS: Declare

    Note over CS: Ensure that the connected validator is among&lt;br/&gt;the para's validator set. Otherwise, skip it.

    CS -&gt;&gt; NB: AdvertiseCollation
    NB -&gt;&gt; VS: AdvertiseCollation

    VS -&gt;&gt; CandidateSelection: Collation

    Note over CandidateSelection: Lots of other machinery in play here,&lt;br/&gt;but there are only three outcomes from the&lt;br/&gt;perspective of the `CollatorProtocol`:

    alt happy path
        CandidateSelection --&gt;&gt; VS: FetchCollation
        Activate VS
        VS -&gt;&gt; NB: RequestCollation
        NB -&gt;&gt; CS: RequestCollation
        CS -&gt;&gt; NB: Collation
        NB -&gt;&gt; VS: Collation
        Deactivate VS

    else collation invalid or unexpected
        CandidateSelection -&gt;&gt; VS: ReportCollator
        VS -&gt;&gt; NB: ReportPeer

    else CandidateSelection already selected a different candidate
        Note over CandidateSelection: silently drop
    end
</pre>
<p>Assuming we hit the happy path, flow continues with <code>CandidateSelection</code> receiving a <code>(candidate_receipt, pov)</code> as
the return value from its
<code>FetchCollation</code> request. The only time <code>CandidateSelection</code> actively requests a collation is when
it hasn't yet seconded one for some <code>relay_parent</code>, and is ready to second.</p>
<pre class="mermaid">sequenceDiagram
    participant CS as CandidateSelection
    participant CB as CandidateBacking
    participant CV as CandidateValidation
    participant PV as Provisioner
    participant SD as StatementDistribution
    participant PD as PoVDistribution

    CS -&gt;&gt; CB: Second
    % fn validate_and_make_available
    CB --&gt;&gt; CV: ValidateFromChainState

    Note over CB,CV: There's some complication in the source, as&lt;br/&gt;candidates are actually validated in a separate task.

    alt valid
        Note over CB: This is where we transform the CandidateReceipt into a CommittedCandidateReceipt
        % CandidateBackingJob::sign_import_and_distribute_statement
        % CandidateBackingJob::import_statement
        CB -&gt;&gt; PV: ProvisionableData::BackedCandidate
        % CandidateBackingJob::issue_new_misbehaviors
        opt if there is misbehavior to report
            CB -&gt;&gt; PV: ProvisionableData::MisbehaviorReport
        end
        % CandidateBackingJob::distribute_signed_statement
        CB -&gt;&gt; SD: Share
        % CandidateBackingJob::distribute_pov
        CB -&gt;&gt; PD: DistributePoV
    else invalid
        CB -&gt;&gt; CS: Invalid
    end
</pre>
<p>At this point, you'll see that control flows in two directions: to <code>StatementDistribution</code> to distribute
the <code>SignedStatement</code>, and to <code>PoVDistribution</code> to distribute the <code>PoV</code>. However, that's largely a mirage:
while the initial implementation distributes <code>PoV</code>s by gossip, that's inefficient, and will be replaced
with a system which fetches <code>PoV</code>s only when actually necessary.</p>
<blockquote>
<p>TODO: figure out more precisely the current status and plans; write them up</p>
</blockquote>
<p>Therefore, we'll follow the <code>SignedStatement</code>. The <code>StatementDistribution</code> subsystem is largely concerned
with implementing a gossip protocol:</p>
<pre class="mermaid">sequenceDiagram
    participant SD as StatementDistribution
    participant NB as NetworkBridge

    alt On receipt of a&lt;br/&gt;SignedStatement from CandidateBacking
        % fn circulate_statement_and_dependents
        SD -&gt;&gt; NB: SendValidationMessage

        Note right of NB: Bridge sends validation message to all appropriate peers
    else On receipt of peer validation message
        NB -&gt;&gt; SD: NetworkBridgeUpdateV1

        % fn handle_incoming_message
        alt if we aren't already aware of the relay parent for this statement
            SD -&gt;&gt; NB: ReportPeer
        end

        % fn circulate_statement
        opt if we know of peers who haven't seen this message, gossip it
            SD -&gt;&gt; NB: SendValidationMessage
        end
    end
</pre>
<p>But who are these <code>Listener</code>s who've asked to be notified about incoming <code>SignedStatement</code>s?
Nobody, as yet.</p>
<p>Let's pick back up with the PoV Distribution subsystem.</p>
<pre class="mermaid">sequenceDiagram
    participant CB as CandidateBacking
    participant PD as PoVDistribution
    participant Listener
    participant NB as NetworkBridge

    CB -&gt;&gt; PD: DistributePoV

    Note over PD,Listener: Various subsystems can register listeners for when PoVs arrive

    loop for each Listener
        PD -&gt;&gt; Listener: Arc&lt;PoV&gt;
    end

    Note over PD: Gossip to connected peers

    PD -&gt;&gt; NB: SendPoV

    Note over PD,NB: On receipt of a network PoV, PovDistribution forwards it to each Listener.&lt;br/&gt;It also penalizes bad gossipers.
</pre>
<p>Unlike in the case of <code>StatementDistribution</code>, there is another subsystem which in various circumstances
already registers a listener to be notified when a new <code>PoV</code> arrives: <code>CandidateBacking</code>. Note that this
is the second time that <code>CandidateBacking</code> has gotten involved. The first instance was from the perspective
of the validator choosing to second a candidate via its <code>CandidateSelection</code> subsystem. This time, it's
from the perspective of some other validator, being informed that this foreign <code>PoV</code> has been received.</p>
<pre class="mermaid">sequenceDiagram
    participant SD as StatementDistribution
    participant CB as CandidateBacking
    participant PD as PoVDistribution
    participant AS as AvailabilityStore

    SD -&gt;&gt; CB: Statement
    % CB::maybe_validate_and_import =&gt; CB::kick_off_validation_work
    CB --&gt;&gt; PD: FetchPoV
    Note over CB,PD: This call creates the Listener from the previous diagram

    CB -&gt;&gt; AS: StoreAvailableData
</pre>
<p>At this point, things have gone a bit nonlinear. Let's pick up the thread again with <code>BitfieldSigning</code>. As
the <code>Overseer</code> activates each relay parent, it starts a <code>BitfieldSigningJob</code> which operates on an extremely
simple metric: after creation, it immediately goes to sleep for 1.5 seconds. On waking, it records the state
of the world pertaining to availability at that moment.</p>
<pre class="mermaid">sequenceDiagram
    participant OS as Overseer
    participant BS as BitfieldSigning
    participant RA as RuntimeApi
    participant AS as AvailabilityStore
    participant BD as BitfieldDistribution

    OS -&gt;&gt; BS: ActiveLeavesUpdate
    loop for each activated relay parent
        Note over BS: Wait 1.5 seconds
        BS --&gt;&gt; RA: Request::AvailabilityCores
        loop for each availability core
            BS --&gt;&gt; AS: QueryChunkAvailability
        end
        BS -&gt;&gt; BD: DistributeBitfield
    end
</pre>
<p><code>BitfieldDistribution</code> is, like the other <code>*Distribution</code> subsystems, primarily interested in implementing
a peer-to-peer gossip network propagating its particular messages. However, it also serves as an essential
relay passing the message along.</p>
<pre class="mermaid">sequenceDiagram
    participant BS as BitfieldSigning
    participant BD as BitfieldDistribution
    participant NB as NetworkBridge
    participant PV as Provisioner

    BS -&gt;&gt; BD: DistributeBitfield
    BD -&gt;&gt; PV: ProvisionableData::Bitfield
    BD -&gt;&gt; NB: SendValidationMessage::BitfieldDistribution::Bitfield
</pre>
<p>We've now seen the message flow to the <code>Provisioner</code>: both <code>CandidateBacking</code> and <code>BitfieldDistribution</code>
contribute provisionable data. Now, let's look at that subsystem.</p>
<p>Much like the <code>BitfieldSigning</code> subsystem, the <code>Provisioner</code> creates a new job for each newly-activated
leaf, and starts a timer. Unlike <code>BitfieldSigning</code>, we won't depict that part of the process, because
the <code>Provisioner</code> also has other things going on.</p>
<pre class="mermaid">sequenceDiagram
    participant A as Arbitrary
    participant PV as Provisioner
    participant CB as CandidateBacking
    participant BD as BitfieldDistribution
    participant RA as RuntimeApi
    participant PI as ParachainsInherentDataProvider

    alt receive provisionable data
        alt
            CB -&gt;&gt; PV: ProvisionableData
        else
            BD -&gt;&gt; PV: ProvisionableData
        end

        loop over stored Senders
            PV -&gt;&gt; A: ProvisionableData
        end

        Note over PV: store bitfields and backed candidates
    else receive request for inherent data
        PI -&gt;&gt; PV: RequestInherentData
        alt we have already constructed the inherent data
            PV -&gt;&gt; PI: send the inherent data
        else we have not yet constructed the inherent data
            Note over PV,PI: Store the return sender without sending immediately
        end
    else timer times out
        note over PV: Waited 2 seconds
        PV --&gt;&gt; RA: RuntimeApiRequest::AvailabilityCores
        Note over PV: construct and store the inherent data
        loop over stored inherent data requests
            PV -&gt;&gt; PI: (SignedAvailabilityBitfields, BackedCandidates)
        end
    end
</pre>
<p>In principle, any arbitrary subsystem could send a <code>RequestInherentData</code> to the <code>Provisioner</code>. In practice,
only the <code>ParachainsInherentDataProvider</code> does so.</p>
<p>The tuple <code>(SignedAvailabilityBitfields, BackedCandidates, ParentHeader)</code> is injected by the <code>ParachainsInherentDataProvider</code>
into the inherent data. From that point on, control passes from the node to the runtime.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="overseer"><a class="header" href="#overseer">Overseer</a></h1>
<p>The overseer is responsible for these tasks:</p>
<ol>
<li>Setting up, monitoring, and handing failure for overseen subsystems.</li>
<li>Providing a &quot;heartbeat&quot; of which relay-parents subsystems should be working on.</li>
<li>Acting as a message bus between subsystems.</li>
</ol>
<p>The hierarchy of subsystems:</p>
<pre><code class="language-text">+--------------+      +------------------+    +--------------------+
|              |      |                  |----&gt;   Subsystem A      |
| Block Import |      |                  |    +--------------------+
|    Events    |------&gt;                  |    +--------------------+
+--------------+      |                  |----&gt;   Subsystem B      |
                      |   Overseer       |    +--------------------+
+--------------+      |                  |    +--------------------+
|              |      |                  |----&gt;   Subsystem C      |
| Finalization |------&gt;                  |    +--------------------+
|    Events    |      |                  |    +--------------------+
|              |      |                  |----&gt;   Subsystem D      |
+--------------+      +------------------+    +--------------------+

</code></pre>
<p>The overseer determines work to do based on block import events and block finalization events. It does this by keeping track of the set of relay-parents for which work is currently being done. This is known as the &quot;active leaves&quot; set. It determines an initial set of active leaves on startup based on the data on-disk, and uses events about blockchain import to update the active leaves. Updates lead to <a href="node/../types/overseer-protocol.html#overseer-signal"><code>OverseerSignal</code></a><code>::ActiveLeavesUpdate</code> being sent according to new relay-parents, as well as relay-parents to stop considering. Block import events inform the overseer of leaves that no longer need to be built on, now that they have children, and inform us to begin building on those children. Block finalization events inform us when we can stop focusing on blocks that appear to have been orphaned.</p>
<p>The overseer is also responsible for tracking the freshness of active leaves. Leaves are fresh when they're encountered for the first time, and stale when they're encountered for subsequent times. This can occur after chain reversions or when the fork-choice rule abandons some chain. This distinction is used to manage <strong>Reversion Safety</strong>. Consensus messages are often localized to a specific relay-parent, and it is often a misbehavior to equivocate or sign two conflicting messages. When reverting the chain, we may begin work on a leaf that subsystems have already signed messages for. Subsystems which need to account for reversion safety should avoid performing work on stale leaves.</p>
<p>The overseer's logic can be described with these functions:</p>
<h2 id="on-startup"><a class="header" href="#on-startup">On Startup</a></h2>
<ul>
<li>Start all subsystems</li>
<li>Determine all blocks of the blockchain that should be built on. This should typically be the head of the best fork of the chain we are aware of. Sometimes add recent forks as well.</li>
<li>Send an <code>OverseerSignal::ActiveLeavesUpdate</code> to all subsystems with <code>activated</code> containing each of these blocks.</li>
<li>Begin listening for block import and finality events</li>
</ul>
<h2 id="on-block-import-event"><a class="header" href="#on-block-import-event">On Block Import Event</a></h2>
<ul>
<li>Apply the block import event to the active leaves. A new block should lead to its addition to the active leaves set and its parent being deactivated.</li>
<li>Mark any stale leaves as stale. The overseer should track all leaves it activates to determine whether leaves are fresh or stale.</li>
<li>Send an <code>OverseerSignal::ActiveLeavesUpdate</code> message to all subsystems containing all activated and deactivated leaves.</li>
<li>Ensure all <code>ActiveLeavesUpdate</code> messages are flushed before resuming activity as a message router.</li>
</ul>
<blockquote>
<p>TODO: in the future, we may want to avoid building on too many sibling blocks at once. the notion of a &quot;preferred head&quot; among many competing sibling blocks would imply changes in our &quot;active leaves&quot; update rules here</p>
</blockquote>
<h2 id="on-finalization-event"><a class="header" href="#on-finalization-event">On Finalization Event</a></h2>
<ul>
<li>Note the height <code>h</code> of the newly finalized block <code>B</code>.</li>
<li>Prune all leaves from the active leaves which have height <code>&lt;= h</code> and are not <code>B</code>.</li>
<li>Issue <code>OverseerSignal::ActiveLeavesUpdate</code> containing all deactivated leaves.</li>
</ul>
<h2 id="on-subsystem-failure"><a class="header" href="#on-subsystem-failure">On Subsystem Failure</a></h2>
<p>Subsystems are essential tasks meant to run as long as the node does. Subsystems can spawn ephemeral work in the form of jobs, but the subsystems themselves should not go down. If a subsystem goes down, it will be because of a critical error that should take the entire node down as well.</p>
<h2 id="communication-between-subsystems"><a class="header" href="#communication-between-subsystems">Communication Between Subsystems</a></h2>
<p>When a subsystem wants to communicate with another subsystem, or, more typically, a job within a subsystem wants to communicate with its counterpart under another subsystem, that communication must happen via the overseer. Consider this example where a job on subsystem A wants to send a message to its counterpart under subsystem B. This is a realistic scenario, where you can imagine that both jobs correspond to work under the same relay-parent.</p>
<pre><code class="language-text">     +--------+                                                           +--------+
     |        |                                                           |        |
     |Job A-1 | (sends message)                       (receives message)  |Job B-1 |
     |        |                                                           |        |
     +----|---+                                                           +----^---+
          |                  +------------------------------+                  ^
          v                  |                              |                  |
+---------v---------+        |                              |        +---------|---------+
|                   |        |                              |        |                   |
| Subsystem A       |        |       Overseer / Message     |        | Subsystem B       |
|                   --------&gt;&gt;                  Bus         --------&gt;&gt;                   |
|                   |        |                              |        |                   |
+-------------------+        |                              |        +-------------------+
                             |                              |
                             +------------------------------+
</code></pre>
<p>First, the subsystem that spawned a job is responsible for handling the first step of the communication. The overseer is not aware of the hierarchy of tasks within any given subsystem and is only responsible for subsystem-to-subsystem communication. So the sending subsystem must pass on the message via the overseer to the receiving subsystem, in such a way that the receiving subsystem can further address the communication to one of its internal tasks, if necessary.</p>
<p>This communication prevents a certain class of race conditions. When the Overseer determines that it is time for subsystems to begin working on top of a particular relay-parent, it will dispatch a <code>ActiveLeavesUpdate</code> message to all subsystems to do so, and those messages will be handled asynchronously by those subsystems. Some subsystems will receive those messsages before others, and it is important that a message sent by subsystem A after receiving <code>ActiveLeavesUpdate</code> message will arrive at subsystem B after its <code>ActiveLeavesUpdate</code> message. If subsystem A maintaned an independent channel with subsystem B to communicate, it would be possible for subsystem B to handle the side message before the <code>ActiveLeavesUpdate</code> message, but it wouldn't have any logical course of action to take with the side message - leading to it being discarded or improperly handled. Well-architectured state machines should have a single source of inputs, so that is what we do here.</p>
<p>One exception is reasonable to make for responses to requests. A request should be made via the overseer in order to ensure that it arrives after any relevant <code>ActiveLeavesUpdate</code> message. A subsystem issuing a request as a result of a <code>ActiveLeavesUpdate</code> message can safely receive the response via a side-channel for two reasons:</p>
<ol>
<li>It's impossible for a request to be answered before it arrives, it is provable that any response to a request obeys the same ordering constraint.</li>
<li>The request was sent as a result of handling a <code>ActiveLeavesUpdate</code> message. Then there is no possible future in which the <code>ActiveLeavesUpdate</code> message has not been handled upon the receipt of the response.</li>
</ol>
<p>So as a single exception to the rule that all communication must happen via the overseer we allow the receipt of responses to requests via a side-channel, which may be established for that purpose. This simplifies any cases where the outside world desires to make a request to a subsystem, as the outside world can then establish a side-channel to receive the response on.</p>
<p>It's important to note that the overseer is not aware of the internals of subsystems, and this extends to the jobs that they spawn. The overseer isn't aware of the existence or definition of those jobs, and is only aware of the outer subsystems with which it interacts. This gives subsystem implementations leeway to define internal jobs as they see fit, and to wrap a more complex hierarchy of state machines than having a single layer of jobs for relay-parent-based work. Likewise, subsystems aren't required to spawn jobs. Certain types of subsystems, such as those for shared storage or networking resources, won't perform block-based work but would still benefit from being on the Overseer's message bus. These subsystems can just ignore the overseer's signals for block-based work.</p>
<p>Furthermore, the protocols by which subsystems communicate with each other should be well-defined irrespective of the implementation of the subsystem. In other words, their interface should be distinct from their implementation. This will prevent subsystems from accessing aspects of each other that are beyond the scope of the communication boundary.</p>
<h2 id="on-shutdown"><a class="header" href="#on-shutdown">On shutdown</a></h2>
<p>Send an <code>OverseerSignal::Conclude</code> message to each subsystem and wait some time for them to conclude before hard-exiting.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="grandpa-voting-rule"><a class="header" href="#grandpa-voting-rule">GRANDPA Voting Rule</a></h1>
<p>Specifics on the motivation and types of constraints we apply to the GRANDPA voting logic as well as the definitions of <strong>viable</strong> and <strong>finalizable</strong> blocks can be found in the <a href="node/../protocol-chain-selection.html">Chain Selection Protocol</a> section.
The subsystem which provides us with viable leaves is the <a href="node/utility/chain-selection.html">Chain Selection Subsystem</a>. </p>
<p>GRANDPA's regular voting rule is for each validator to select the longest chain they are aware of. GRANDPA proceeds in rounds, collecting information from all online validators and determines the blocks that a supermajority of validators all have in common with each other.</p>
<p>The low-level GRANDPA logic will provide us with a <strong>required block</strong>. We can find the best leaf containing that block in its chain with the <a href="node/../types/overseer-protocol.html#chain-selection-message"><code>ChainSelectionMessage::BestLeafContaining</code></a>. If the result is <code>None</code>, then we will simply cast a vote on the required block.</p>
<p>The <strong>viable</strong> leaves provided from the chain selection subsystem are not necessarily <strong>finalizable</strong>, so we need to perform further work to discover the finalizable ancestor of the block. The first constraint is to avoid voting on any unapproved block. The highest approved ancestor of a given block can be determined by querying the Approval Voting subsystem via the <a href="node/../types/overseer-protocol.html#approval-voting"><code>ApprovalVotingMessage::ApprovedAncestor</code></a> message. If the response is <code>Some</code>, we continue and apply the second constraint. The second constraint is to avoid voting on any block containing a candidate undergoing an active dispute. The list of block hashes and candidates returned from <code>ApprovedAncestor</code> should be reversed, and passed to the <a href="node/../types/overseer-protocol.html#dispute-coordinator-message"><code>DisputeCoordinatorMessage::DetermineUndisputedChain</code></a> to determine the <strong>finalizable</strong> block which will be our eventual vote.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="collators"><a class="header" href="#collators">Collators</a></h1>
<p>Collators are special nodes which bridge a parachain to the relay chain. They are simultaneously full nodes of the parachain, and at least light clients of the relay chain. Their overall contribution to the system is the generation of Proofs of Validity for parachain candidates.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="collation-generation"><a class="header" href="#collation-generation">Collation Generation</a></h1>
<p>The collation generation subsystem is executed on collator nodes and produces candidates to be distributed to validators. If configured to produce collations for a para, it produces collations and then feeds them to the <a href="node/collators/collator-protocol.html">Collator Protocol</a> subsystem, which handles the networking.</p>
<h2 id="protocol"><a class="header" href="#protocol">Protocol</a></h2>
<p>Input: <code>CollationGenerationMessage</code></p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum CollationGenerationMessage {
  Initialize(CollationGenerationConfig),
}
<span class="boring">}
</span></code></pre></pre>
<p>No more than one initialization message should ever be sent to the collation generation subsystem.</p>
<p>Output: <code>CollationDistributionMessage</code></p>
<h2 id="functionality"><a class="header" href="#functionality">Functionality</a></h2>
<p>The process of generating a collation for a parachain is very parachain-specific. As such, the details of how to do so are left beyond the scope of this description. The subsystem should be implemented as an abstract wrapper, which is aware of this configuration:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Collation {
  /// Messages destined to be interpreted by the Relay chain itself.
  pub upward_messages: Vec&lt;UpwardMessage&gt;,
  /// New validation code.
  pub new_validation_code: Option&lt;ValidationCode&gt;,
  /// The head-data produced as a result of execution.
  pub head_data: HeadData,
  /// Proof to verify the state transition of the parachain.
  pub proof_of_validity: PoV,
}

/// Result of the [`CollatorFn`] invocation.
pub struct CollationResult {
	/// The collation that was build.
	collation: Collation,
	/// An optional result sender that should be informed about a successfully seconded collation.
	///
	/// There is no guarantee that this sender is informed ever about any result, it is completly okay to just drop it.
	/// However, if it is called, it should be called with the signed statement of a parachain validator seconding the
	/// collation.
	result_sender: Option&lt;oneshot::Sender&lt;SignedFullStatement&gt;&gt;,
}

/// Collation function.
///
/// Will be called with the hash of the relay chain block the parachain block should be build on and the
/// [`ValidationData`] that provides information about the state of the parachain on the relay chain.
///
/// Returns an optional [`CollationResult`].
pub type CollatorFn = Box&lt;
	dyn Fn(Hash, &amp;PersistedValidationData) -&gt; Pin&lt;Box&lt;dyn Future&lt;Output = Option&lt;CollationResult&gt;&gt; + Send&gt;&gt;
		+ Send
		+ Sync,
&gt;;

struct CollationGenerationConfig {
  key: CollatorPair,
  /// Collate will be called with the relay chain hash the parachain should build
  /// a block on and the `ValidationData` that provides information about the state
  /// of the parachain on the relay chain.
  collator: CollatorFn,
  para_id: ParaId,
}
<span class="boring">}
</span></code></pre></pre>
<p>The configuration should be optional, to allow for the case where the node is not run with the capability to collate.</p>
<p>On <code>ActiveLeavesUpdate</code>:</p>
<ul>
<li>If there is no collation generation config, ignore.</li>
<li>Otherwise, for each <code>activated</code> head in the update:
<ul>
<li>
<p>Determine if the para is scheduled on any core by fetching the <code>availability_cores</code> Runtime API.</p>
<blockquote>
<p>TODO: figure out what to do in the case of occupied cores; see <a href="https://github.com/paritytech/polkadot/issues/1573">this issue</a>.</p>
</blockquote>
</li>
<li>
<p>Determine an occupied core assumption to make about the para. Scheduled cores can make <code>OccupiedCoreAssumption::Free</code>.</p>
</li>
<li>
<p>Use the Runtime API subsystem to fetch the full validation data.</p>
</li>
<li>
<p>Invoke the <code>collator</code>, and use its outputs to produce a <code>CandidateReceipt</code>, signed with the configuration's <code>key</code>.</p>
</li>
<li>
<p>Dispatch a <a href="node/collators/../../types/overseer-protocol.html#collatorprotocolmessage"><code>CollatorProtocolMessage</code></a><code>::DistributeCollation(receipt, pov)</code>.</p>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="collator-protocol"><a class="header" href="#collator-protocol">Collator Protocol</a></h1>
<p>The Collator Protocol implements the network protocol by which collators and validators communicate. It is used by collators to distribute collations to validators and used by validators to accept collations by collators.</p>
<p>Collator-to-Validator networking is more difficult than Validator-to-Validator networking because the set of possible collators for any given para is unbounded, unlike the validator set. Validator-to-Validator networking protocols can easily be implemented as gossip because the data can be bounded, and validators can authenticate each other by their <code>PeerId</code>s for the purposes of instantiating and accepting connections.</p>
<p>Since, at least at the level of the para abstraction, the collator-set for any given para is unbounded, validators need to make sure that they are receiving connections from capable and honest collators and that their bandwidth and time are not being wasted by attackers. Communicating across this trust-boundary is the most difficult part of this subsystem.</p>
<p>Validation of candidates is a heavy task, and furthermore, the <a href="node/collators/../../types/availability.html#proofofvalidity"><code>PoV</code></a> itself is a large piece of data. Empirically, <code>PoV</code>s are on the order of 10MB.</p>
<blockquote>
<p>TODO: note the incremental validation function Ximin proposes at https://github.com/paritytech/polkadot/issues/1348</p>
</blockquote>
<p>As this network protocol serves as a bridge between collators and validators, it communicates primarily with one subsystem on behalf of each. As a collator, this will receive messages from the <a href="node/collators/collation-generation.html"><code>CollationGeneration</code></a> subsystem. As a validator, this will communicate only with the <a href="node/collators/../backing/candidate-backing.html"><code>CandidateBacking</code></a>.</p>
<h2 id="protocol-1"><a class="header" href="#protocol-1">Protocol</a></h2>
<p>Input: <a href="node/collators/../../types/overseer-protocol.html#collator-protocol-message"><code>CollatorProtocolMessage</code></a></p>
<p>Output:</p>
<ul>
<li><a href="node/collators/../../types/overseer-protocol.html#runtime-api-message"><code>RuntimeApiMessage</code></a></li>
<li><a href="node/collators/../../types/overseer-protocol.html#network-bridge-message"><code>NetworkBridgeMessage</code></a></li>
<li><a href="node/collators/../../types/overseer-protocol.html#candidate-backing-mesage"><code>CandidateBackingMessage</code></a></li>
</ul>
<h2 id="functionality-1"><a class="header" href="#functionality-1">Functionality</a></h2>
<p>This network protocol uses the <code>Collation</code> peer-set of the <a href="node/collators/../utility/network-bridge.html"><code>NetworkBridge</code></a>.</p>
<p>It uses the <a href="node/collators/../../types/network.html#collator-protocol"><code>CollatorProtocolV1Message</code></a> as its <code>WireMessage</code></p>
<p>Since this protocol functions both for validators and collators, it is easiest to go through the protocol actions for each of them separately.</p>
<p>Validators and collators.</p>
<p><img src="node/collators/collator_protocol_0.generated.svg" alt="" /></p>
<h3 id="collators-1"><a class="header" href="#collators-1">Collators</a></h3>
<p>It is assumed that collators are only collating on a single parachain. Collations are generated by the <a href="node/collators/collation-generation.html">Collation Generation</a> subsystem. We will keep up to one local collation per relay-parent, based on <code>DistributeCollation</code> messages. If the para is not scheduled or next up on any core, at the relay-parent, or the relay-parent isn't in the active-leaves set, we ignore the message as it must be invalid in that case - although this indicates a logic error elsewhere in the node.</p>
<p>We keep track of the Para ID we are collating on as a collator. This starts as <code>None</code>, and is updated with each <code>CollateOn</code> message received. If the <code>ParaId</code> of a collation requested to be distributed does not match the one we expect, we ignore the message.</p>
<p>As with most other subsystems, we track the active leaves set by following <code>ActiveLeavesUpdate</code> signals.</p>
<p>For the purposes of actually distributing a collation, we need to be connected to the validators who are interested in collations on that <code>ParaId</code> at this point in time. We assume that there is a discovery API for connecting to a set of validators.</p>
<p>As seen in the <a href="node/collators/../../runtime/scheduler.html">Scheduler Module</a> of the runtime, validator groups are fixed for an entire session and their rotations across cores are predictable. Collators will want to do these things when attempting to distribute collations at a given relay-parent:</p>
<ul>
<li>Determine which core the para collated-on is assigned to.</li>
<li>Determine the group on that core and the next group on that core.</li>
<li>Issue a discovery request for the validators of the current group and the next group with<a href="node/collators/../../types/overseer-protocol.html#network-bridge-message"><code>NetworkBridgeMessage</code></a><code>::ConnectToValidators</code>.</li>
</ul>
<p>Once connected to the relevant peers for the current group assigned to the core (transitively, the para), advertise the collation to any of them which advertise the relay-parent in their view (as provided by the <a href="node/collators/../utility/network-bridge.html">Network Bridge</a>). If any respond with a request for the full collation, provide it. Upon receiving a view update from any of these peers which includes a relay-parent for which we have a collation that they will find relevant, advertise the collation to them if we haven't already.</p>
<h3 id="validators-1"><a class="header" href="#validators-1">Validators</a></h3>
<p>On the validator side of the protocol, validators need to accept incoming connections from collators. They should keep some peer slots open for accepting new speculative connections from collators and should disconnect from collators who are not relevant.</p>
<p><img src="node/collators/collator_protocol_1.generated.svg" alt="" /></p>
<p>When peers connect to us, they can <code>Declare</code> that they represent a collator with given public key and intend to collate on a specific para ID. Once they've declared that, and we checked their signature, they can begin to send advertisements of collations. The peers should not send us any advertisements for collations that are on a relay-parent outside of our view or for a para outside of the one they've declared.</p>
<p>The protocol tracks advertisements received and the source of the advertisement. The advertisement source is the <code>PeerId</code> of the peer who sent the message. We accept one advertisement per collator per source per relay-parent.</p>
<p>As a validator, we will handle requests from other subsystems to fetch a collation on a specific <code>ParaId</code> and relay-parent. These requests are made with the request response protocol <code>CollationFetchingRequest</code> request. To do so, we need to first check if we have already gathered a collation on that <code>ParaId</code> and relay-parent. If not, we need to select one of the advertisements and issue a request for it. If we've already issued a request, we shouldn't issue another one until the first has returned.</p>
<p>When acting on an advertisement, we issue a <code>Requests::CollationFetching</code>. If the request times out, we need to note the collator as being unreliable and reduce its priority relative to other collators.</p>
<p>As a validator, once the collation has been fetched some other subsystem will inspect and do deeper validation of the collation. The subsystem will report to this subsystem with a <a href="node/collators/../../types/overseer-protocol.html#collator-protocol-message"><code>CollatorProtocolMessage</code></a><code>::ReportCollator</code>. In that case, if we are connected directly to the collator, we apply a cost to the <code>PeerId</code> associated with the collator and potentially disconnect or blacklist it. If the collation is seconded, we notify the collator and apply a benefit to the <code>PeerId</code> associated with the collator.</p>
<h3 id="interaction-with-a-hrefnodecollatorsbackingcandidate-backinghtmlcandidate-backinga"><a class="header" href="#interaction-with-a-hrefnodecollatorsbackingcandidate-backinghtmlcandidate-backinga">Interaction with <a href="node/collators/../backing/candidate-backing.html">Candidate Backing</a></a></h3>
<p>As collators advertise the availability, a validator will simply second the first valid parablock candidate per relay head by sending a <a href="node/collators/../../types/overseer-protocol.html#candidate-backing-mesage"><code>CandidateBackingMessage</code></a><code>::Second</code>. Note that this message contains the relay parent of the advertised collation, the candidate receipt and the <a href="node/collators/../../types/availability.html#proofofvalidity">PoV</a>.</p>
<p>Subsequently, once a valid parablock candidate has been seconded, the <a href="node/collators/../backing/candidate-backing.html"><code>CandidateBacking</code></a> subsystem will send a <a href="node/collators/../../types/overseer-protocol.html#collator-protocol-message"><code>CollatorProtocolMessage</code></a><code>::Seconded</code>, which will trigger this subsystem to notify the collator at the <code>PeerId</code> that first advertised the parablock on the seconded relay head of their successful seconding.</p>
<h2 id="future-work-1"><a class="header" href="#future-work-1">Future Work</a></h2>
<p>Several approaches have been discussed, but all have some issues:</p>
<ul>
<li>The current approach is very straightforward. However, that protocol is vulnerable to a single collator which, as an attack or simply through chance, gets its block candidate to the node more often than its fair share of the time.</li>
<li>If collators produce blocks via Aura, BABE or in future Sassafrass, it may be possible to choose an &quot;Official&quot; collator for the round, but it may be tricky to ensure that the PVF logic is enforced at collator leader election.</li>
<li>We could use relay-chain BABE randomness to generate some delay <code>D</code> on the order of 1 second, +- 1 second. The collator would then second the first valid parablock which arrives after <code>D</code>, or in case none has arrived by <code>2*D</code>, the last valid parablock which has arrived. This makes it very hard for a collator to game the system to always get its block nominated, but it reduces the maximum throughput of the system by introducing delay into an already tight schedule.</li>
<li>A variation of that scheme would be to have a fixed acceptance window <code>D</code> for parablock candidates and keep track of count <code>C</code>: the number of parablock candidates received. At the end of the period <code>D</code>, we choose a random number I in the range [0, C) and second the block at Index I. Its drawback is the same: it must wait the full <code>D</code> period before seconding any of its received candidates, reducing throughput.</li>
<li>In order to protect against DoS attacks, it may be prudent to run throw out collations from collators that have behaved poorly (whether recently or historically) and subsequently only verify the PoV for the most suitable of collations.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="backing-subsystems"><a class="header" href="#backing-subsystems">Backing Subsystems</a></h1>
<p>The backing subsystems, when conceived as a black box, receive an arbitrary quantity of parablock candidates and associated proofs of validity from arbitrary untrusted collators. From these, they produce a bounded quantity of backable candidates which relay chain block authors may choose to include in a subsequent block.</p>
<p>In broad strokes, the flow operates like this:</p>
<ul>
<li><strong>Candidate Selection</strong> winnows the field of parablock candidates, selecting up to one of them to second.</li>
<li><strong>Candidate Backing</strong> ensures that a seconding candidate is valid, then generates the appropriate <code>Statement</code>. It also keeps track of which candidates have received the backing of a quorum of other validators.</li>
<li><strong>Statement Distribution</strong> is the networking component which ensures that all validators receive each others' statements.</li>
<li><strong>PoV Distribution</strong> is the networking component which ensures that validators considering a candidate can get the appropriate PoV.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="candidate-backing"><a class="header" href="#candidate-backing">Candidate Backing</a></h1>
<p>The Candidate Backing subsystem ensures every parablock considered for relay block inclusion has been seconded by at least one validator, and approved by a quorum. Parablocks for which no validator will assert correctness are discarded. If the block later proves invalid, the initial backers are slashable; this gives polkadot a rational threat model during subsequent stages.</p>
<p>Its role is to produce backable candidates for inclusion in new relay-chain blocks. It does so by issuing signed <a href="node/backing/../../types/backing.html#statement-type"><code>Statement</code>s</a> and tracking received statements signed by other validators. Once enough statements are received, they can be combined into backing for specific candidates.</p>
<p>Note that though the candidate backing subsystem attempts to produce as many backable candidates as possible, it does <em>not</em> attempt to choose a single authoritative one. The choice of which actually gets included is ultimately up to the block author, by whatever metrics it may use; those are opaque to this subsystem.</p>
<p>Once a sufficient quorum has agreed that a candidate is valid, this subsystem notifies the <a href="node/backing/../utility/provisioner.html">Provisioner</a>, which in turn engages block production mechanisms to include the parablock.</p>
<h2 id="protocol-2"><a class="header" href="#protocol-2">Protocol</a></h2>
<p>Input: <a href="node/backing/../../types/overseer-protocol.html#candidate-backing-message"><code>CandidateBackingMessage</code></a></p>
<p>Output:</p>
<ul>
<li><a href="node/backing/../../types/overseer-protocol.html#validation-request-type"><code>CandidateValidationMessage</code></a></li>
<li><a href="node/backing/../../types/overseer-protocol.html#runtime-api-message"><code>RuntimeApiMessage</code></a></li>
<li><a href="node/backing/../../types/overseer-protocol.html#collator-protocol-message"><code>CollatorProtocolMessage</code></a></li>
<li><a href="node/backing/../../types/overseer-protocol.html#provisioner-message"><code>ProvisionerMessage</code></a></li>
<li><a href="node/backing/../../types/overseer-protocol.html#availability-distribution-message"><code>AvailabilityDistributionMessage</code></a></li>
<li><a href="node/backing/../../types/overseer-protocol.html#statement-distribution-message"><code>StatementDistributionMessage</code></a></li>
</ul>
<h2 id="functionality-2"><a class="header" href="#functionality-2">Functionality</a></h2>
<p>The <a href="node/backing/../collators/collator-protocol.html">Collator Protocol</a> subsystem is the primary source of non-overseer messages into this subsystem. That subsystem generates appropriate <a href="node/backing/../../types/overseer-protocol.html#candidate-backing-message"><code>CandidateBackingMessage</code>s</a> and passes them to this subsystem.</p>
<p>This subsystem requests validation from the <a href="node/backing/../utility/candidate-validation.html">Candidate Validation</a> and generates an appropriate <a href="node/backing/../../types/backing.html#statement-type"><code>Statement</code></a>. All <code>Statement</code>s are then passed on to the <a href="node/backing/statement-distribution.html">Statement Distribution</a> subsystem to be gossiped to peers. When <a href="node/backing/../utility/candidate-validation.html">Candidate Validation</a> decides that a candidate is invalid, and it was recommended to us to second by our own <a href="node/backing/../collators/collator-protocol.html">Collator Protocol</a> subsystem, a message is sent to the <a href="node/backing/../collators/collator-protocol.html">Collator Protocol</a> subsystem with the candidate's hash so that the collator which recommended it can be penalized.</p>
<p>The subsystem should maintain a set of handles to Candidate Backing Jobs that are currently live, as well as the relay-parent to which they correspond.</p>
<h3 id="on-overseer-signal"><a class="header" href="#on-overseer-signal">On Overseer Signal</a></h3>
<ul>
<li>If the signal is an <a href="node/backing/../../types/overseer-protocol.html#overseer-signal"><code>OverseerSignal</code></a><code>::ActiveLeavesUpdate</code>:
<ul>
<li>spawn a Candidate Backing Job for each <code>activated</code> head referring to a fresh leaf, storing a bidirectional channel with the Candidate Backing Job in the set of handles.</li>
<li>cease the Candidate Backing Job for each <code>deactivated</code> head, if any.</li>
</ul>
</li>
<li>If the signal is an <a href="node/backing/../../types/overseer-protocol.html#overseer-signal"><code>OverseerSignal</code></a><code>::Conclude</code>: Forward conclude messages to all jobs, wait a small amount of time for them to join, and then exit.</li>
</ul>
<h3 id="on-receiving-candidatebackingmessage"><a class="header" href="#on-receiving-candidatebackingmessage">On Receiving <code>CandidateBackingMessage</code></a></h3>
<ul>
<li>If the message is a <a href="node/backing/../../types/overseer-protocol.html#candidate-backing-message"><code>CandidateBackingMessage</code></a><code>::GetBackedCandidates</code>, get all backable candidates from the statement table and send them back.</li>
<li>If the message is a <a href="node/backing/../../types/overseer-protocol.html#candidate-backing-message"><code>CandidateBackingMessage</code></a><code>::Second</code>, sign and dispatch a <code>Seconded</code> statement only if we have not seconded any other candidate and have not signed a <code>Valid</code> statement for the requested candidate. Signing both a <code>Seconded</code> and <code>Valid</code> message is a double-voting misbehavior with a heavy penalty, and this could occur if another validator has seconded the same candidate and we've received their message before the internal seconding request.</li>
<li>If the message is a <a href="node/backing/../../types/overseer-protocol.html#candidate-backing-message"><code>CandidateBackingMessage</code></a><code>::Statement</code>, count the statement to the quorum. If the statement in the message is <code>Seconded</code> and it contains a candidate that belongs to our assignment, request the corresponding <code>PoV</code> from the backing node via <code>AvailabilityDistribution</code> and launch validation. Issue our own <code>Valid</code> or <code>Invalid</code> statement as a result.</li>
</ul>
<p>If the seconding node did not provide us with the <code>PoV</code> we will retry fetching from other backing validators.</p>
<blockquote>
<p>big TODO: &quot;contextual execution&quot;</p>
<ul>
<li>At the moment we only allow inclusion of <em>new</em> parachain candidates validated by <em>current</em> validators.</li>
<li>Allow inclusion of <em>old</em> parachain candidates validated by <em>current</em> validators.</li>
<li>Allow inclusion of <em>old</em> parachain candidates validated by <em>old</em> validators.</li>
</ul>
<p>This will probably blur the lines between jobs, will probably require inter-job communication and a short-term memory of recently backable, but not backed candidates.</p>
</blockquote>
<h2 id="candidate-backing-job"><a class="header" href="#candidate-backing-job">Candidate Backing Job</a></h2>
<p>The Candidate Backing Job represents the work a node does for backing candidates with respect to a particular relay-parent.</p>
<p>The goal of a Candidate Backing Job is to produce as many backable candidates as possible. This is done via signed <a href="node/backing/../../types/backing.html#statement-type"><code>Statement</code>s</a> by validators. If a candidate receives a majority of supporting Statements from the Parachain Validators currently assigned, then that candidate is considered backable.</p>
<h3 id="on-startup-1"><a class="header" href="#on-startup-1">On Startup</a></h3>
<ul>
<li>Fetch current validator set, validator -&gt; parachain assignments from <a href="node/backing/../utility/runtime-api.html"><code>Runtime API</code></a> subsystem using <a href="node/backing/../../types/overseer-protocol.html#runtime-api-message"><code>RuntimeApiRequest::Validators</code></a> and <a href="node/backing/../../types/overseer-protocol.html#runtime-api-message"><code>RuntimeApiRequest::ValidatorGroups</code></a></li>
<li>Determine if the node controls a key in the current validator set. Call this the local key if so.</li>
<li>If the local key exists, extract the parachain head and validation function from the <a href="node/backing/../utility/runtime-api.html"><code>Runtime API</code></a> for the parachain the local key is assigned to by issuing a <a href="node/backing/../../types/overseer-protocol.html#runtime-api-message"><code>RuntimeApiRequest::Validators</code></a></li>
<li>Issue a <a href="node/backing/../../types/overseer-protocol.html#runtime-api-message"><code>RuntimeApiRequest::SigningContext</code></a> message to get a context that will later be used upon signing.</li>
</ul>
<h3 id="on-receiving-new-candidate-backing-message"><a class="header" href="#on-receiving-new-candidate-backing-message">On Receiving New Candidate Backing Message</a></h3>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>match msg {
  GetBackedCandidates(hashes, tx) =&gt; {
    // Send back a set of backable candidates.
  }
  CandidateBackingMessage::Second(hash, candidate) =&gt; {
    if candidate is unknown and in local assignment {
      if spawn_validation_work(candidate, parachain head, validation function).await == Valid {
        send(DistributePoV(pov))
      }
    }
  }
  CandidateBackingMessage::Statement(hash, statement) =&gt; {
    // count to the votes on this candidate
    if let Statement::Seconded(candidate) = statement {
      if candidate.parachain_id == our_assignment {
        spawn_validation_work(candidate, parachain head, validation function)
      }
    }
  }
}
<span class="boring">}
</span></code></pre></pre>
<p>Add <code>Seconded</code> statements and <code>Valid</code> statements to a quorum. If quorum reaches validator-group majority, send a <a href="node/backing/../../types/overseer-protocol.html#provisioner-message"><code>ProvisionerMessage</code></a><code>::ProvisionableData(ProvisionableData::BackedCandidate(CandidateReceipt))</code> message.
<code>Invalid</code> statements that conflict with already witnessed <code>Seconded</code> and <code>Valid</code> statements for the given candidate, statements that are double-votes, self-contradictions and so on, should result in issuing a <a href="node/backing/../../types/overseer-protocol.html#provisioner-message"><code>ProvisionerMessage</code></a><code>::MisbehaviorReport</code> message for each newly detected case of this kind.</p>
<p>On each incoming statement, <a href="node/backing/../../types/overseer-protocol.html#dispute-coordinator-message"><code>DisputeCoordinatorMessage::ImportStatement</code></a> should be issued.</p>
<h3 id="validating-candidates"><a class="header" href="#validating-candidates">Validating Candidates.</a></h3>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn spawn_validation_work(candidate, parachain head, validation function) {
  asynchronously {
    let pov = (fetch pov block).await

    let valid = (validate pov block).await;
    if valid {
      // make PoV available for later distribution. Send data to the availability store to keep.
      // sign and dispatch `valid` statement to network if we have not seconded the given candidate.
    } else {
      // sign and dispatch `invalid` statement to network.
    }
  }
}
<span class="boring">}
</span></code></pre></pre>
<h3 id="fetch-pov-block"><a class="header" href="#fetch-pov-block">Fetch Pov Block</a></h3>
<p>Create a <code>(sender, receiver)</code> pair.
Dispatch a <a href="node/backing/../../types/overseer-protocol.html#availability-distribution-message"><code>AvailabilityDistributionMessage</code></a><code>::FetchPoV{ validator_index, pov_hash, candidate_hash, tx, } and listen on the passed receiver for a response. Availability distribution will send the request to the validator specified by </code>validator_index`, which might not be serving it for whatever reasons, therefore we need to retry with other backing validators in that case.</p>
<h3 id="validate-pov-block"><a class="header" href="#validate-pov-block">Validate PoV Block</a></h3>
<p>Create a <code>(sender, receiver)</code> pair.
Dispatch a <code>CandidateValidationMessage::Validate(validation function, candidate, pov, sender)</code> and listen on the receiver for a response.</p>
<h3 id="distribute-signed-statement"><a class="header" href="#distribute-signed-statement">Distribute Signed Statement</a></h3>
<p>Dispatch a <a href="node/backing/../../types/overseer-protocol.html#statement-distribution-message"><code>StatementDistributionMessage</code></a><code>::Share(relay_parent, SignedFullStatement)</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="statement-distribution"><a class="header" href="#statement-distribution">Statement Distribution</a></h1>
<p>The Statement Distribution Subsystem is responsible for distributing statements about seconded candidates between validators.</p>
<h2 id="protocol-3"><a class="header" href="#protocol-3">Protocol</a></h2>
<p><code>PeerSet</code>: <code>Validation</code></p>
<p>Input:</p>
<ul>
<li>NetworkBridgeUpdate(update)</li>
<li>StatementDistributionMessage</li>
</ul>
<p>Output:</p>
<ul>
<li>NetworkBridge::SendMessage(<code>[PeerId]</code>, message)</li>
<li>NetworkBridge::SendRequests (StatementFetching)</li>
<li>NetworkBridge::ReportPeer(PeerId, cost_or_benefit)</li>
</ul>
<h2 id="functionality-3"><a class="header" href="#functionality-3">Functionality</a></h2>
<p>Implemented as a gossip protocol. Handle updates to our view and peers' views. Neighbor packets are used to inform peers which chain heads we are interested in data for.</p>
<p>It is responsible for distributing signed statements that we have generated and forwarding them, and for detecting a variety of Validator misbehaviors for reporting to <a href="node/backing/../utility/misbehavior-arbitration.html">Misbehavior Arbitration</a>. During the Backing stage of the inclusion pipeline, it's the main point of contact with peer nodes. On receiving a signed statement from a peer in the same backing group, assuming the peer receipt state machine is in an appropriate state, it sends the Candidate Receipt to the <a href="node/backing/candidate-backing.html">Candidate Backing subsystem</a> to handle the validator's statement. On receiving <code>StatementDistributionMessage::Share</code> we make sure to send messages to our backing group in addition to random other peers, to ensure a fast backing process and getting all statements quickly for distribtution.</p>
<p>Track equivocating validators and stop accepting information from them. Establish a data-dependency order:</p>
<ul>
<li>In order to receive a <code>Seconded</code> message we have the corresponding chain head in our view</li>
<li>In order to receive an <code>Valid</code> message we must have received the corresponding <code>Seconded</code> message.</li>
</ul>
<p>And respect this data-dependency order from our peers by respecting their views. This subsystem is responsible for checking message signatures.</p>
<p>The Statement Distribution subsystem sends statements to peer nodes.</p>
<h2 id="peer-receipt-state-machine"><a class="header" href="#peer-receipt-state-machine">Peer Receipt State Machine</a></h2>
<p>There is a very simple state machine which governs which messages we are willing to receive from peers. Not depicted in the state machine: on initial receipt of any <a href="node/backing/../../types/backing.html#signed-statement-type"><code>SignedFullStatement</code></a>, validate that the provided signature does in fact sign the included data. Note that each individual parablock candidate gets its own instance of this state machine; it is perfectly legal to receive a <code>Valid(X)</code> before a <code>Seconded(Y)</code>, as long as a <code>Seconded(X)</code> has been received.</p>
<p>A: Initial State. Receive <code>SignedFullStatement(Statement::Second)</code>: extract <code>Statement</code>, forward to Candidate Backing, proceed to B. Receive any other <code>SignedFullStatement</code> variant: drop it.</p>
<p>B: Receive any <code>SignedFullStatement</code>: check signature and determine whether the statement is new to us. if new, forward to Candidate Backing and circulate to other peers. Receive <code>OverseerMessage::StopWork</code>: proceed to C.</p>
<p>C: Receive any message for this block: drop it.</p>
<p>For large statements (see below), we also keep track of the total received large
statements per peer and have a hard limit on that number for flood protection.
This is necessary as in the current code we only forward statements once we have
all the data, therefore flood protection for large statement is a bit more
subtle. This will become an obsolete problem once <a href="https://github.com/paritytech/polkadot/issues/2979">off chain code
upgrades</a> are implemented.</p>
<h2 id="peer-knowledge-tracking"><a class="header" href="#peer-knowledge-tracking">Peer Knowledge Tracking</a></h2>
<p>The peer receipt state machine implies that for parsimony of network resources, we should model the knowledge of our peers, and help them out. For example, let's consider a case with peers A, B, and C, validators X and Y, and candidate M. A sends us a <code>Statement::Second(M)</code> signed by X. We've double-checked it, and it's valid. While we're checking it, we receive a copy of X's <code>Statement::Second(M)</code> from <code>B</code>, along with a <code>Statement::Valid(M)</code> signed by Y.</p>
<p>Our response to A is just the <code>Statement::Valid(M)</code> signed by Y. However, we haven't heard anything about this from C. Therefore, we send it everything we have: first a copy of X's <code>Statement::Second</code>, then Y's <code>Statement::Valid</code>.</p>
<p>This system implies a certain level of duplication of messages--we received X's <code>Statement::Second</code> from both our peers, and C may experience the same--but it minimizes the degree to which messages are simply dropped.</p>
<p>And respect this data-dependency order from our peers. This subsystem is responsible for checking message signatures.</p>
<p>No jobs. We follow view changes from the <a href="node/backing/../utility/network-bridge.html"><code>NetworkBridge</code></a>, which in turn is updated by the overseer.</p>
<h2 id="equivocations-and-flood-protection"><a class="header" href="#equivocations-and-flood-protection">Equivocations and Flood Protection</a></h2>
<p>An equivocation is a double-vote by a validator. The <a href="node/backing/candidate-backing.html">Candidate Backing</a> Subsystem is better-suited than this one to detect equivocations as it adds votes to quorum trackers.</p>
<p>At this level, we are primarily concerned about flood-protection, and to some extent, detecting equivocations is a part of that. In particular, we are interested in detecting equivocations of <code>Seconded</code> statements. Since every other statement is dependent on <code>Seconded</code> statements, ensuring that we only ever hold a bounded number of <code>Seconded</code> statements is sufficient for flood-protection.</p>
<p>The simple approach is to say that we only receive up to two <code>Seconded</code> statements per validator per chain head. However, the marginal cost of equivocation, conditional on having already equivocated, is close to 0, since a single double-vote offence is counted as all double-vote offences for a particular chain-head. Even if it were not, there is some amount of equivocations that can be done such that the marginal cost of issuing further equivocations is close to 0, as there would be an amount of equivocations necessary to be completely and totally obliterated by the slashing algorithm. We fear the validator with nothing left to lose.</p>
<p>With that in mind, this simple approach has a caveat worth digging deeper into.</p>
<p>First: We may be aware of two equivocated <code>Seconded</code> statements issued by a validator. A totally honest peer of ours can also be aware of one or two different <code>Seconded</code> statements issued by the same validator. And yet another peer may be aware of one or two <em>more</em> <code>Seconded</code> statements. And so on. This interacts badly with pre-emptive sending logic. Upon sending a <code>Seconded</code> statement to a peer, we will want to pre-emptively follow up with all statements relative to that candidate. Waiting for acknowledgement introduces latency at every hop, so that is best avoided. What can happen is that upon receipt of the <code>Seconded</code> statement, the peer will discard it as it falls beyond the bound of 2 that it is allowed to store. It cannot store anything in memory about discarded candidates as that would introduce a DoS vector. Then, the peer would receive from us all of the statements pertaining to that candidate, which, from its perspective, would be undesired - they are data-dependent on the <code>Seconded</code> statement we sent them, but they have erased all record of that from their memory. Upon receiving a potential flood of undesired statements, this 100% honest peer may choose to disconnect from us. In this way, an adversary may be able to partition the network with careful distribution of equivocated <code>Seconded</code> statements.</p>
<p>The fix is to track, per-peer, the hashes of up to 4 candidates per validator (per relay-parent) that the peer is aware of. It is 4 because we may send them 2 and they may send us 2 different ones. We track the data that they are aware of as the union of things we have sent them and things they have sent us. If we receive a 1st or 2nd <code>Seconded</code> statement from a peer, we note it in the peer's known candidates even if we do disregard the data locally. And then, upon receipt of any data dependent on that statement, we do not reduce that peer's standing in our eyes, as the data was not undesired.</p>
<p>There is another caveat to the fix: we don't want to allow the peer to flood us because it has set things up in a way that it knows we will drop all of its traffic.
We also track how many statements we have received per peer, per candidate, and per chain-head. This is any statement concerning a particular candidate: <code>Seconded</code>, <code>Valid</code>, or <code>Invalid</code>. If we ever receive a statement from a peer which would push any of these counters beyond twice the amount of validators at the chain-head, we begin to lower the peer's standing and eventually disconnect. This bound is a massive overestimate and could be reduced to twice the number of validators in the corresponding validator group. It is worth noting that the goal at the time of writing is to ensure any finite bound on the amount of stored data, as any equivocation results in a large slash.</p>
<h2 id="large-statements"><a class="header" href="#large-statements">Large statements</a></h2>
<p>Seconded statements can become quite large on parachain runtime upgrades for
example. For this reason, there exists a <code>LargeStatement</code> constructor for the
<code>StatementDistributionMessage</code> wire message, which only contains light metadata
of a statement. The actual candidate data is not included. This message type is
used whenever a message is deemed large. The receiver of such a message needs to
request the actual payload via request/response by means of a
<code>StatementFetching</code> request.</p>
<p>This is necessary as distribution of a large payload (mega bytes) via gossip
would make the network collapse and timely distribution of statements would no
longer be possible. By using request/response it is ensured that each peer only
transferes large data once. We only take good care to detect an overloaded
peer early and immediately move on to a different peer for fetching the data.
This mechanism should result in a good load distribution and therefore a rather
optimal distribution path.</p>
<p>With these optimizations, distribution of payloads in the size of up to 3 to 4
MB should work with Kusama validator specifications. For scaling up even more,
runtime upgrades and message passing should be done off chain at some point.</p>
<p>Flood protection considerations: For making DoS attacks slightly harder on this
subsystem, nodes will only respond to large statement requests, when they
previously notified that peer via gossip about that statement. So, it is not
possible to DoS nodes at scale, by requesting candidate data over and over
again.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="availability-subsystems"><a class="header" href="#availability-subsystems">Availability Subsystems</a></h1>
<p>The availability subsystems are responsible for ensuring that Proofs of Validity of backed candidates are widely available within the validator set, without requiring every node to retain a full copy. They accomplish this by broadly distributing erasure-coded chunks of the PoV, keeping track of which validator has which chunk by means of signed bitfields. They are also responsible for reassembling a complete PoV when required, e.g. when a fisherman reports a potentially invalid block.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="availability-distribution"><a class="header" href="#availability-distribution">Availability Distribution</a></h1>
<p>This subsystem is responsible for distribution availability data to peers.
Availability data are chunks, <code>PoV</code>s and <code>AvailableData</code> (which is <code>PoV</code> +
<code>PersistedValidationData</code>). It does so via request response protocols.</p>
<p>In particular this subsystem is responsible for:</p>
<ul>
<li>Respond to network requests requesting availability data by querying the
<a href="node/availability/../utility/availability-store.html">Availability Store</a>.</li>
<li>Request chunks from backing validators to put them in the local <code>Availability Store</code> whenever we find an occupied core on the chain,
this is to ensure availability by at least 2/3+ of all validators, this
happens after a candidate is backed.</li>
<li>Fetch <code>PoV</code> from validators, when requested via <code>FetchPoV</code> message from
backing (pov_requester module).</li>
<li></li>
</ul>
<p>The backing subsystem is responsible of making available data available in the
local <code>Availability Store</code> upon validation. This subsystem will serve any
network requests by querying that store.</p>
<h2 id="protocol-4"><a class="header" href="#protocol-4">Protocol</a></h2>
<p>This subsystem does not handle any peer set messages, but the <code>pov_requester</code>
does  connecto to validators of the same backing group on the validation peer
set, to ensure fast propagation of statements between those validators and for
ensuring already established connections for requesting <code>PoV</code>s. Other than that
this subsystem drives request/response protocols.</p>
<p>Input:</p>
<ul>
<li>OverseerSignal::ActiveLeaves(<code>[ActiveLeavesUpdate]</code>)</li>
<li>AvailabilityDistributionMessage{msg: ChunkFetchingRequest}</li>
<li>AvailabilityDistributionMessage{msg: PoVFetchingRequest}</li>
<li>AvailabilityDistributionMessage{msg: FetchPoV}</li>
</ul>
<p>Output:</p>
<ul>
<li>NetworkBridgeMessage::SendRequests(<code>[Requests]</code>, IfDisconnected::TryConnect)</li>
<li>AvailabilityStore::QueryChunk(candidate_hash, index, response_channel)</li>
<li>AvailabilityStore::StoreChunk(candidate_hash, chunk)</li>
<li>AvailabilityStore::QueryAvailableData(candidate_hash, response_channel)</li>
<li>RuntimeApiRequest::SessionIndexForChild</li>
<li>RuntimeApiRequest::SessionInfo</li>
<li>RuntimeApiRequest::AvailabilityCores</li>
</ul>
<h2 id="functionality-4"><a class="header" href="#functionality-4">Functionality</a></h2>
<h3 id="pov-requester"><a class="header" href="#pov-requester">PoV Requester</a></h3>
<p>The PoV requester in the <code>pov_requester</code> module takes care of staying connected
to validators of the current backing group of this very validator on the <code>Validation</code>
peer set and it will handle <code>FetchPoV</code> requests by issuing network requests to
those validators. It will check the hash of the received <code>PoV</code>, but will not do any
further validation. That needs to be done by the original <code>FetchPoV</code> sender
(backing subsystem).</p>
<h3 id="chunk-requester"><a class="header" href="#chunk-requester">Chunk Requester</a></h3>
<p>After a candidate is backed, the availability of the PoV block must be confirmed
by 2/3+ of all validators. The chunk requester is responsible of making that
availability a reality.</p>
<p>It does that by querying checking occupied cores for all active leaves. For each
occupied core it will spawn a task fetching the erasure chunk which has the
<code>ValidatorIndex</code> of the node. For this an <code>ChunkFetchingRequest</code> is issued, via
substrate's generic request/response protocol.</p>
<p>The spawned task will start trying to fetch the chunk from validators in
responsible group of the occupied core, in a random order. For ensuring that we
use already open TCP connections wherever possible, the requester maintains a
cache and preserves that random order for the entire session.</p>
<p>Note however that, because not all validators in a group have to be actual
backers, not all of them are required to have the needed chunk. This in turn
could lead to low throughput, as we have to wait for fetches to fail,
before reaching a validator finally having our chunk. We do rank back validators
not delivering our chunk, but as backers could vary from block to block on a
perfectly legitimate basis, this is still not ideal. See issues <a href="https://github.com/paritytech/polkadot/issues/2509">2509</a> and <a href="https://github.com/paritytech/polkadot/issues/2512">2512</a>
for more information.</p>
<p>The current implementation also only fetches chunks for occupied cores in blocks
in active leaves. This means though, if active leaves skips a block or we are
particularly slow in fetching our chunk, we might not fetch our chunk if
availability reached 2/3 fast enough (slot becomes free). This is not desirable
as we would like as many validators as possible to have their chunk. See this
<a href="https://github.com/paritytech/polkadot/issues/2513">issue</a> for more details.</p>
<h3 id="serving"><a class="header" href="#serving">Serving</a></h3>
<p>On the other side the subsystem will listen for incoming <code>ChunkFetchingRequest</code>s
and <code>PoVFetchingRequest</code>s from the network bridge and will respond to queries,
by looking the requested chunks and <code>PoV</code>s up in the availability store, this
happens in the <code>responder</code> module.</p>
<p>We rely on the backing subsystem to make available data available locally in the
<code>Availability Store</code> after it has validated it.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="availability-recovery"><a class="header" href="#availability-recovery">Availability Recovery</a></h1>
<p>This subsystem is the inverse of the <a href="node/availability/availability-distribution.html">Availability Distribution</a> subsystem: validators will serve the availability chunks kept in the availability store to nodes who connect to them. And the subsystem will also implement the other side: the logic for nodes to connect to validators, request availability pieces, and reconstruct the <code>AvailableData</code>.</p>
<p>This version of the availability recovery subsystem is based off of direct connections to validators. In order to recover any given <code>AvailableData</code>, we must recover at least <code>f + 1</code> pieces from validators of the session. Thus, we will connect to and query randomly chosen validators until we have received <code>f + 1</code> pieces.</p>
<h2 id="protocol-5"><a class="header" href="#protocol-5">Protocol</a></h2>
<p><code>PeerSet</code>: <code>Validation</code></p>
<p>Input:</p>
<ul>
<li>NetworkBridgeUpdateV1(update)</li>
<li>AvailabilityRecoveryMessage::RecoverAvailableData(candidate, session, backing_group, response)</li>
</ul>
<p>Output:</p>
<ul>
<li>NetworkBridge::SendValidationMessage</li>
<li>NetworkBridge::ReportPeer</li>
<li>AvailabilityStore::QueryChunk</li>
</ul>
<h2 id="functionality-5"><a class="header" href="#functionality-5">Functionality</a></h2>
<p>We hold a state which tracks the current recovery interactions we have live, as well as which request IDs correspond to which interactions. An interaction is a structure encapsulating all interaction with the network necessary to recover the available data.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct State {
    /// Each interaction is implemented as its own remote async task, and these handles are remote
    /// for it.
    interactions: FuturesUnordered&lt;InteractionHandle&gt;,
    /// A multiplexer over receivers from live interactions.
    interaction_receivers: FuturesUnordered&lt;ResponseReceiver&lt;Concluded&gt;&gt;,
    /// A recent block hash for which state should be available.
    live_block_hash: Hash,
    // An LRU cache of recently recovered data.
    availability_lru: LruCache&lt;CandidateHash, Result&lt;AvailableData, RecoveryError&gt;&gt;,
}

/// This is a future, which concludes either when a response is received from the interaction,
/// or all the `awaiting` channels have closed.
struct InteractionHandle {
    candidate_hash: CandidateHash,
    interaction_response: RemoteHandle&lt;Concluded&gt;,
    awaiting: Vec&lt;ResponseChannel&lt;Result&lt;AvailableData, RecoveryError&gt;&gt;&gt;,
}

struct Unavailable;
struct Concluded(CandidateHash, Result&lt;AvailableData, RecoveryError&gt;);

struct InteractionParams {
    validator_authority_keys: Vec&lt;AuthorityId&gt;,
    validators: Vec&lt;ValidatorId&gt;,
    // The number of pieces needed.
    threshold: usize, 
    candidate_hash: Hash,
    erasure_root: Hash,
}

enum InteractionPhase {
    RequestFromBackers {
        // a random shuffling of the validators from the backing group which indicates the order
        // in which we connect to them and request the chunk.
        shuffled_backers: Vec&lt;ValidatorIndex&gt;,
    }
    RequestChunks {
        // a random shuffling of the validators which indicates the order in which we connect to the validators and
        // request the chunk from them.
        shuffling: Vec&lt;ValidatorIndex&gt;, 
        received_chunks: Map&lt;ValidatorIndex, ErasureChunk&gt;,
        requesting_chunks: FuturesUnordered&lt;Receiver&lt;ErasureChunkRequestResponse&gt;&gt;,
    }
}

struct Interaction {
    to_subsystems: SubsystemSender,
    params: InteractionParams,
    phase: InteractionPhase,
}
<span class="boring">}
</span></code></pre></pre>
<h3 id="signal-handling"><a class="header" href="#signal-handling">Signal Handling</a></h3>
<p>On <code>ActiveLeavesUpdate</code>, if <code>activated</code> is non-empty, set <code>state.live_block_hash</code> to the first block in <code>Activated</code>.</p>
<p>Ignore <code>BlockFinalized</code> signals.</p>
<p>On <code>Conclude</code>, shut down the subsystem.</p>
<h4 id="availabilityrecoverymessagerecoveravailabledatareceipt-session-optionbacking_group_index-response"><a class="header" href="#availabilityrecoverymessagerecoveravailabledatareceipt-session-optionbacking_group_index-response"><code>AvailabilityRecoveryMessage::RecoverAvailableData(receipt, session, Option&lt;backing_group_index&gt;, response)</code></a></h4>
<ol>
<li>Check the <code>availability_lru</code> for the candidate and return the data if so.</li>
<li>Check if there is already an interaction handle for the request. If so, add the response handle to it.</li>
<li>Otherwise, load the session info for the given session under the state of <code>live_block_hash</code>, and initiate an interaction with <em>launch_interaction</em>. Add an interaction handle to the state and add the response channel to it.</li>
<li>If the session info is not available, return <code>RecoveryError::Unavailable</code> on the response channel.</li>
</ol>
<h3 id="from-interaction-logic"><a class="header" href="#from-interaction-logic">From-interaction logic</a></h3>
<h4 id="frominteractionconcluded"><a class="header" href="#frominteractionconcluded"><code>FromInteraction::Concluded</code></a></h4>
<ol>
<li>Load the entry from the <code>interactions</code> map. It should always exist, if not for logic errors. Send the result to each member of <code>awaiting</code>.</li>
<li>Add the entry to the availability_lru.</li>
</ol>
<h3 id="interaction-logic"><a class="header" href="#interaction-logic">Interaction logic</a></h3>
<h4 id="launch_interactionsession_index-session_info-candidate_receipt-candidate_hash-optionbacking_group_index"><a class="header" href="#launch_interactionsession_index-session_info-candidate_receipt-candidate_hash-optionbacking_group_index"><code>launch_interaction(session_index, session_info, candidate_receipt, candidate_hash, Option&lt;backing_group_index&gt;)</code></a></h4>
<ol>
<li>Compute the threshold from the session info. It should be <code>f + 1</code>, where <code>n = 3f + k</code>, where <code>k in {1, 2, 3}</code>, and <code>n</code> is the number of validators.</li>
<li>Set the various fields of <code>InteractionParams</code> based on the validator lists in <code>session_info</code> and information about the candidate.</li>
<li>If the <code>backing_group_index</code> is <code>Some</code>, start in the <code>RequestFromBackers</code> phase with a shuffling of the backing group validator indices and a <code>None</code> requesting value.</li>
<li>Otherwise, start in the <code>RequestChunks</code> phase with <code>received_chunks</code>,<code>requesting_chunks</code>, and <code>next_shuffling</code> all empty.</li>
<li>Set the <code>to_subsystems</code> sender to be equal to a clone of the <code>SubsystemContext</code>'s sender.</li>
<li>Initialize <code>received_chunks</code> to an empty set, as well as <code>requesting_chunks</code>.</li>
</ol>
<p>Launch the interaction as a background task running <code>interaction_loop(interaction)</code>.</p>
<h4 id="interaction_loopinteraction---resultavailabledata-recoeryerror"><a class="header" href="#interaction_loopinteraction---resultavailabledata-recoeryerror"><code>interaction_loop(interaction) -&gt; Result&lt;AvailableData, RecoeryError&gt;</code></a></h4>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// How many parallel requests to have going at once.
const N_PARALLEL: usize = 50;
<span class="boring">}
</span></code></pre></pre>
<ul>
<li>
<p>Request <code>AvailabilityStoreMessage::QueryAvailableData</code>. If it exists, return that.</p>
</li>
<li>
<p>If the phase is <code>InteractionPhase::RequestFromBackers</code></p>
<ul>
<li>Loop:
<ul>
<li>If the <code>requesting_pov</code> is <code>Some</code>, poll for updates on it. If it concludes, set <code>requesting_pov</code> to <code>None</code>. </li>
<li>If the <code>requesting_pov</code> is <code>None</code>, take the next backer off the <code>shuffled_backers</code>.
<ul>
<li>If the backer is <code>Some</code>, issue a <code>NetworkBridgeMessage::Requests</code> with a network request for the <code>AvailableData</code> and wait for the response.</li>
<li>If it concludes with a <code>None</code> result, return to beginning. </li>
<li>If it concludes with available data, attempt a re-encoding. 
<ul>
<li>If it has the correct erasure-root, break and issue a <code>Ok(available_data)</code>. </li>
<li>If it has an incorrect erasure-root, return to beginning.</li>
</ul>
</li>
<li>If the backer is <code>None</code>, set the phase to <code>InteractionPhase::RequestChunks</code> with a random shuffling of validators and empty <code>next_shuffling</code>, <code>received_chunks</code>, and <code>requesting_chunks</code> and break the loop.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>If the phase is <code>InteractionPhase::RequestChunks</code>:</p>
<ul>
<li>Request <code>AvailabilityStoreMessage::QueryAllChunks</code>. For each chunk that exists, add it to <code>received_chunks</code> and remote the validator from <code>shuffling</code>.</li>
<li>Loop:
<ul>
<li>If <code>received_chunks + requesting_chunks + shuffling</code> lengths are less than the threshold, break and return <code>Err(Unavailable)</code>.</li>
<li>Poll for new updates from <code>requesting_chunks</code>. Check merkle proofs of any received chunks. If the request simply fails due to network issues, insert into the front of <code>shuffling</code> to be retried.</li>
<li>If <code>received_chunks</code> has more than <code>threshold</code> entries, attempt to recover the data. If that fails, or a re-encoding produces an incorrect erasure-root, break and issue a <code>Err(RecoveryError::Invalid)</code>. If correct, break and issue <code>Ok(available_data)</code>.</li>
<li>While there are fewer than <code>N_PARALLEL</code> entries in <code>requesting_chunks</code>,
<ul>
<li>Pop the next item from <code>shuffling</code>. If it's empty and <code>requesting_chunks</code> is empty, return <code>Err(RecoveryError::Unavailable)</code>.</li>
<li>Issue a <code>NetworkBridgeMessage::Requests</code> and wait for the response in <code>requesting_chunks</code>.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bitfield-distribution"><a class="header" href="#bitfield-distribution">Bitfield Distribution</a></h1>
<p>Validators vote on the availability of a backed candidate by issuing signed bitfields, where each bit corresponds to a single candidate. These bitfields can be used to compactly determine which backed candidates are available or not based on a 2/3+ quorum.</p>
<h2 id="protocol-6"><a class="header" href="#protocol-6">Protocol</a></h2>
<p><code>PeerSet</code>: <code>Validation</code></p>
<p>Input:
<a href="node/availability/../../types/overseer-protocol.html#bitfield-distribution-message"><code>BitfieldDistributionMessage</code></a> which are gossiped to all peers, no matter if validator or not.</p>
<p>Output:</p>
<ul>
<li><code>NetworkBridge::SendValidationMessage([PeerId], message)</code> gossip a verified incoming bitfield on to interested subsystems within this validator node.</li>
<li><code>NetworkBridge::ReportPeer(PeerId, cost_or_benefit)</code> improve or penalize the reputation of peers based on the messages that are received relative to the current view.</li>
<li><code>ProvisionerMessage::ProvisionableData(ProvisionableData::Bitfield(relay_parent, SignedAvailabilityBitfield))</code> pass
on the bitfield to the other submodules via the overseer.</li>
</ul>
<h2 id="functionality-6"><a class="header" href="#functionality-6">Functionality</a></h2>
<p>This is implemented as a gossip system.</p>
<p>It is necessary to track peer connection, view change, and disconnection events, in order to maintain an index of which peers are interested in which relay parent bitfields.</p>
<p>Before gossiping incoming bitfields, they must be checked to be signed by one of the validators
of the validator set relevant to the current relay parent.
Only accept bitfields relevant to our current view and only distribute bitfields to other peers when relevant to their most recent view.
Accept and distribute only one bitfield per validator.</p>
<p>When receiving a bitfield either from the network or from a <code>DistributeBitfield</code> message, forward it along to the block authorship (provisioning) subsystem for potential inclusion in a block.</p>
<p>Peers connecting after a set of valid bitfield gossip messages was received, those messages must be cached and sent upon connection of new peers or re-connecting peers.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bitfield-signing"><a class="header" href="#bitfield-signing">Bitfield Signing</a></h1>
<p>Validators vote on the availability of a backed candidate by issuing signed bitfields, where each bit corresponds to a single candidate. These bitfields can be used to compactly determine which backed candidates are available or not based on a 2/3+ quorum.</p>
<h2 id="protocol-7"><a class="header" href="#protocol-7">Protocol</a></h2>
<p>Input:</p>
<p>There is no dedicated input mechanism for bitfield signing. Instead, Bitfield Signing produces a bitfield representing the current state of availability on <code>StartWork</code>.</p>
<p>Output:</p>
<ul>
<li>BitfieldDistribution::DistributeBitfield: distribute a locally signed bitfield</li>
<li>AvailabilityStore::QueryChunk(CandidateHash, validator_index, response_channel)</li>
</ul>
<h2 id="functionality-7"><a class="header" href="#functionality-7">Functionality</a></h2>
<p>Upon receipt of an <code>ActiveLeavesUpdate</code>, launch bitfield signing job for each <code>activated</code> head referring to a fresh leaf. Stop the job for each <code>deactivated</code> head.</p>
<h2 id="bitfield-signing-job"><a class="header" href="#bitfield-signing-job">Bitfield Signing Job</a></h2>
<p>Localized to a specific relay-parent <code>r</code>
If not running as a validator, do nothing.</p>
<ul>
<li>Begin by waiting a fixed period of time so availability distribution has the chance to make candidates available.</li>
<li>Determine our validator index <code>i</code>, the set of backed candidates pending availability in <code>r</code>, and which bit of the bitfield each corresponds to.</li>
<li>Start with an empty bitfield. For each bit in the bitfield, if there is a candidate pending availability, query the <a href="node/availability/../utility/availability-store.html">Availability Store</a> for whether we have the availability chunk for our validator index. The <code>OccupiedCore</code> struct contains the candidate hash so the full candidate does not need to be fetched from runtime.</li>
<li>For all chunks we have, set the corresponding bit in the bitfield.</li>
<li>Sign the bitfield and dispatch a <code>BitfieldDistribution::DistributeBitfield</code> message.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="approval-subsystems"><a class="header" href="#approval-subsystems">Approval Subsystems</a></h1>
<p>The approval subsystems implement the node-side of the <a href="node/approval/../../protocol-approval.html">Approval Protocol</a>.</p>
<p>We make a divide between the <a href="node/approval/approval-voting.html">assignment/voting logic</a> and the <a href="node/approval/approval-distribution.html">distribution logic</a> that distributes assignment certifications and approval votes. The logic in the assignment and voting also informs the GRANDPA voting rule on how to vote.</p>
<p>These subsystems are intended to flag issues and begin <a href="node/approval/../disputes/dispute-participation.html">participating in live disputes</a>. Dispute subsystems also track all observed votes (backing, approval, and dispute-specific) by all validators on all candidates.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="approval-voting"><a class="header" href="#approval-voting">Approval Voting</a></h1>
<p>Reading the <a href="node/approval/../../protocol-approval.html">section on the approval protocol</a> will likely be necessary to understand the aims of this subsystem.</p>
<p>Approval votes are split into two parts: Assignments and Approvals. Validators first broadcast their assignment to indicate intent to check a candidate. Upon successfully checking, they broadcast an approval vote. If a validator doesn't broadcast their approval vote shortly after issuing an assignment, this is an indication that they are being prevented from recovering or validating the block data and that more validators should self-select to check the candidate. This is known as a &quot;no-show&quot;.</p>
<p>The core of this subsystem is a Tick-based timer loop, where Ticks are 500ms. We also reason about time in terms of DelayTranches, which measure the number of ticks elapsed since a block was produced. We track metadata for all un-finalized but included candidates. We compute our local assignments to check each candidate, as well as which DelayTranche those assignments may be minimally triggered at. As the same candidate may appear in more than one block, we must produce our potential assignments for each (Block, Candidate) pair. The timing loop is based on waiting for assignments to become no-shows or waiting to broadcast and begin our own assignment to check.</p>
<p>Another main component of this subsystem is the logic for determining when a (Block, Candidate) pair has been approved and when to broadcast and trigger our own assignment. Once a (Block, Candidate) pair has been approved, we mark a corresponding bit in the BlockEntry that indicates the candidate has been approved under the block. When we trigger our own assignment, we broadcast it via Approval Distribution, begin fetching the data from Availability Recovery, and then pass it through to the Candidate Validation. Once these steps are successful, we issue our approval vote. If any of these steps fail, we don't issue any vote and will &quot;no-show&quot; from the perspective of other validators. In the future we will initiate disputes as well.</p>
<p>Where this all fits into Polkadot is via block finality. Our goal is to not finalize any block containing a candidate that is not approved. We provide a hook for a custom GRANDPA voting rule - GRANDPA makes requests of the form (target, minimum) consisting of a target block (i.e. longest chain) that it would like to finalize, and a minimum block which, due to the rules of GRANDPA, must be voted on. The minimum is typically the last finalized block, but may be beyond it, in the case of having a last-round-estimate beyond the last finalized. Thus, our goal is to inform GRANDPA of some block between target and minimum which we believe can be finalized safely. We do this by iterating backwards from the target to the minimum and finding the longest continuous chain from minimum where all candidates included by those blocks have been approved.</p>
<h2 id="protocol-8"><a class="header" href="#protocol-8">Protocol</a></h2>
<p>Input:</p>
<ul>
<li><code>ApprovalVotingMessage::CheckAndImportAssignment</code></li>
<li><code>ApprovalVotingMessage::CheckAndImportApproval</code></li>
<li><code>ApprovalVotingMessage::ApprovedAncestor</code></li>
</ul>
<p>Output:</p>
<ul>
<li><code>ApprovalDistributionMessage::DistributeAssignment</code></li>
<li><code>ApprovalDistributionMessage::DistributeApproval</code></li>
<li><code>RuntimeApiMessage::Request</code></li>
<li><code>ChainApiMessage</code></li>
<li><code>AvailabilityRecoveryMessage::Recover</code></li>
<li><code>CandidateExecutionMessage::ValidateFromExhaustive</code></li>
</ul>
<h2 id="functionality-8"><a class="header" href="#functionality-8">Functionality</a></h2>
<p>The approval voting subsystem is responsible for casting votes and determining approval of candidates and as a result, blocks.</p>
<p>This subsystem wraps a database which is used to store metadata about unfinalized blocks and the candidates within them. Candidates may appear in multiple blocks, and assignment criteria are chosen differently based on the hash of the block they appear in.</p>
<h2 id="database-schema"><a class="header" href="#database-schema">Database Schema</a></h2>
<p>The database schema is designed with the following goals in mind:</p>
<ol>
<li>To provide an easy index from unfinalized blocks to candidates</li>
<li>To provide a lookup from candidate hash to approval status</li>
<li>To be easy to clear on start-up. What has happened while we were offline is unimportant.</li>
<li>To be fast to clear entries outdated by finality</li>
</ol>
<p>Structs:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct TrancheEntry {
    tranche: DelayTranche,
    // assigned validators who have not yet approved, and the instant we received
    // their assignment.
    assignments: Vec&lt;(ValidatorIndex, Tick)&gt;,
}

struct OurAssignment {
  cert: AssignmentCert,
  tranche: DelayTranche,
  validator_index: ValidatorIndex,
  triggered: bool,
}

struct ApprovalEntry {
    tranches: Vec&lt;TrancheEntry&gt;, // sorted ascending by tranche number.
    backing_group: GroupIndex,
    our_assignment: Option&lt;OurAssignment&gt;,
    our_approval_sig: Option&lt;ValidatorSignature&gt;,
    assignments: Bitfield, // n_validators bits
    approved: bool,
}

struct CandidateEntry {
    candidate: CandidateReceipt,
    session: SessionIndex,
    // Assignments are based on blocks, so we need to track assignments separately
    // based on the block we are looking at.
    block_assignments: HashMap&lt;Hash, ApprovalEntry&gt;,
    approvals: Bitfield, // n_validators bits
}

struct BlockEntry {
    block_hash: Hash,
    session: SessionIndex,
    slot: Slot,
    // random bytes derived from the VRF submitted within the block by the block
    // author as a credential and used as input to approval assignment criteria.
    relay_vrf_story: [u8; 32],
    // The candidates included as-of this block and the index of the core they are
    // leaving. Sorted ascending by core index.
    candidates: Vec&lt;(CoreIndex, Hash)&gt;,
    // A bitfield where the i'th bit corresponds to the i'th candidate in `candidates`.
    // The i'th bit is `true` iff the candidate has been approved in the context of
    // this block. The block can be considered approved has all bits set to 1
    approved_bitfield: Bitfield,
    children: Vec&lt;Hash&gt;,
}

// slot_duration * 2 + DelayTranche gives the number of delay tranches since the
// unix epoch.
type Tick = u64;

struct StoredBlockRange(BlockNumber, BlockNumber);
<span class="boring">}
</span></code></pre></pre>
<p>In the schema, we map</p>
<pre><code>&quot;StoredBlocks&quot; =&gt; StoredBlockRange
BlockNumber =&gt; Vec&lt;BlockHash&gt;
BlockHash =&gt; BlockEntry
CandidateHash =&gt; CandidateEntry
</code></pre>
<h2 id="logic"><a class="header" href="#logic">Logic</a></h2>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>const APPROVAL_SESSIONS: SessionIndex = 6;
<span class="boring">}
</span></code></pre></pre>
<p>In-memory state:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct ApprovalVoteRequest {
  validator_index: ValidatorIndex,
  block_hash: Hash,
  candidate_index: CandidateIndex,
}

// Requests that background work (approval voting tasks) may need to make of the main subsystem
// task.
enum BackgroundRequest {
  ApprovalVote(ApprovalVoteRequest),
  // .. others, unspecified as per implementation.
}

// This is the general state of the subsystem. The actual implementation may split this
// into further pieces.
struct State {
    earliest_session: SessionIndex,
    session_info: Vec&lt;SessionInfo&gt;,
    babe_epoch: Option&lt;BabeEpoch&gt;, // information about a cached BABE epoch.
    keystore: KeyStore,

    // A scheduler which keeps at most one wakeup per hash, candidate hash pair and
    // maps such pairs to `Tick`s.
    wakeups: Wakeups, 

    // These are connected to each other.
    background_tx: mpsc::Sender&lt;BackgroundRequest&gt;,
    background_rx: mpsc::Receiver&lt;BackgroundRequest&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<p>This guide section makes no explicit references to writes to or reads from disk. Instead, it handles them implicitly, with the understanding that updates to block, candidate, and approval entries are persisted to disk.</p>
<p><a href="node/approval/../../runtime/session_info.html"><code>SessionInfo</code></a></p>
<p>On start-up, we clear everything currently stored by the database. This is done by loading the <code>StoredBlockRange</code>, iterating through each block number, iterating through each block hash, and iterating through each candidate referenced by each block. Although this is <code>O(o*n*p)</code>, we don't expect to have more than a few unfinalized blocks at any time and in extreme cases, a few thousand. The clearing operation should be relatively fast as a result.</p>
<p>Main loop:</p>
<ul>
<li>Each iteration, select over all of
<ul>
<li>The next <code>Tick</code> in <code>wakeups</code>: trigger <code>wakeup_process</code> for each <code>(Hash, Hash)</code> pair scheduled under the <code>Tick</code> and then remove all entries under the <code>Tick</code>.</li>
<li>The next message from the overseer: handle the message as described in the <a href="node/approval/approval-voting.html#incoming-messages">Incoming Messages section</a></li>
<li>The next approval vote request from <code>background_rx</code>
<ul>
<li>If this is an <code>ApprovalVoteRequest</code>, <a href="node/approval/approval-voting.html#issue-approval-vote">Issue an approval vote</a>.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="incoming-messages"><a class="header" href="#incoming-messages">Incoming Messages</a></h3>
<h4 id="overseersignalblockfinalized"><a class="header" href="#overseersignalblockfinalized"><code>OverseerSignal::BlockFinalized</code></a></h4>
<p>On receiving an <code>OverseerSignal::BlockFinalized(h)</code>, we fetch the block number <code>b</code> of that block from the ChainApi subsystem. We update our <code>StoredBlockRange</code> to begin at <code>b+1</code>. Additionally, we remove all block entries and candidates referenced by them up to and including <code>b</code>. Lastly, we prune out all descendents of <code>h</code> transitively: when we remove a <code>BlockEntry</code> with number <code>b</code> that is not equal to <code>h</code>, we recursively delete all the <code>BlockEntry</code>s referenced as children. We remove the <code>block_assignments</code> entry for the block hash and if <code>block_assignments</code> is now empty, remove the <code>CandidateEntry</code>. We also update each of the <code>BlockNumber -&gt; Vec&lt;Hash&gt;</code> keys in the database to reflect the blocks at that height, clearing if empty.</p>
<h4 id="overseersignalactiveleavesupdate"><a class="header" href="#overseersignalactiveleavesupdate"><code>OverseerSignal::ActiveLeavesUpdate</code></a></h4>
<p>On receiving an <code>OverseerSignal::ActiveLeavesUpdate(update)</code>:</p>
<ul>
<li>We determine the set of new blocks that were not in our previous view. This is done by querying the ancestry of all new items in the view and contrasting against the stored <code>BlockNumber</code>s. Typically, there will be only one new block. We fetch the headers and information on these blocks from the ChainApi subsystem. Stale leaves in the update can be ignored.</li>
<li>We update the <code>StoredBlockRange</code> and the <code>BlockNumber</code> maps.</li>
<li>We use the RuntimeApiSubsystem to determine information about these blocks. It is generally safe to assume that runtime state is available for recent, unfinalized blocks. In the case that it isn't, it means that we are catching up to the head of the chain and needn't worry about assignments to those blocks anyway, as the security assumption of the protocol tolerates nodes being temporarily offline or out-of-date.
<ul>
<li>We fetch the set of candidates included by each block by dispatching a <code>RuntimeApiRequest::CandidateEvents</code> and checking the <code>CandidateIncluded</code> events.</li>
<li>We fetch the session of the block by dispatching a <code>session_index_for_child</code> request with the parent-hash of the block.</li>
<li>If the <code>session index - APPROVAL_SESSIONS &gt; state.earliest_session</code>, then bump <code>state.earliest_sessions</code> to that amount and prune earlier sessions.</li>
<li>If the session isn't in our <code>state.session_info</code>, load the session info for it and for all sessions since the earliest-session, including the earliest-session, if that is missing. And it can be, just after pruning, if we've done a big jump forward, as is the case when we've just finished chain synchronization.</li>
<li>If any of the runtime API calls fail, we just warn and skip the block.</li>
</ul>
</li>
<li>We use the RuntimeApiSubsystem to determine the set of candidates included in these blocks and use BABE logic to determine the slot number and VRF of the blocks.</li>
<li>We also note how late we appear to have received the block. We create a <code>BlockEntry</code> for each block and a <code>CandidateEntry</code> for each candidate obtained from <code>CandidateIncluded</code> events after making a <code>RuntimeApiRequest::CandidateEvents</code> request.</li>
<li>For each candidate, if the amount of needed approvals is more than the validators remaining after the backing group of the candidate is subtracted, then the candidate is insta-approved as approval would be impossible otherwise. If all candidates in the block are insta-approved, or there are no candidates in the block, then the block is insta-approved. If the block is insta-approved, a <a href="node/approval/../../types/overseer-protocol.html#chainselectionmessage">`ChainSelectionMessage::Approvedl</a> should be sent for the block.</li>
<li>Ensure that the <code>CandidateEntry</code> contains a <code>block_assignments</code> entry for the block, with the correct backing group set.</li>
<li>If a validator in this session, compute and assign <code>our_assignment</code> for the <code>block_assignments</code>
<ul>
<li>Only if not a member of the backing group.</li>
<li>Run <code>RelayVRFModulo</code> and <code>RelayVRFDelay</code> according to the <a href="node/approval/../../protocol-approval.html#assignment-criteria">the approvals protocol section</a>. Ensure that the assigned core derived from the output is covered by the auxiliary signature aggregated in the <code>VRFPRoof</code>.</li>
</ul>
</li>
<li><a href="node/approval/approval-voting.html#handle-wakeup">Handle Wakeup</a> for each new candidate in each new block - this will automatically broadcast a 0-tranche assignment, kick off approval work, and schedule the next delay.</li>
<li>Dispatch an <code>ApprovalDistributionMessage::NewBlocks</code> with the meta information filled out for each new block.</li>
</ul>
<h4 id="approvalvotingmessagecheckandimportassignment"><a class="header" href="#approvalvotingmessagecheckandimportassignment"><code>ApprovalVotingMessage::CheckAndImportAssignment</code></a></h4>
<p>On receiving a <code>ApprovalVotingMessage::CheckAndImportAssignment</code> message, we check the assignment cert against the block entry. The cert itself contains information necessary to determine the candidate that is being assigned-to. In detail:</p>
<ul>
<li>Load the <code>BlockEntry</code> for the relay-parent referenced by the message. If there is none, return <code>AssignmentCheckResult::Bad</code>.</li>
<li>Fetch the <code>SessionInfo</code> for the session of the block</li>
<li>Determine the assignment key of the validator based on that.</li>
<li>Determine the claimed core index by looking up the candidate with given index in <code>block_entry.candidates</code>. Return <code>AssignmentCheckResult::Bad</code> if missing.</li>
<li>Check the assignment cert
<ul>
<li>If the cert kind is <code>RelayVRFModulo</code>, then the certificate is valid as long as <code>sample &lt; session_info.relay_vrf_samples</code> and the VRF is valid for the validator's key with the input <code>block_entry.relay_vrf_story ++ sample.encode()</code> as described with <a href="node/approval/../../protocol-approval.html#assignment-criteria">the approvals protocol section</a>. We set <code>core_index = vrf.make_bytes().to_u32() % session_info.n_cores</code>. If the <code>BlockEntry</code> causes inclusion of a candidate at <code>core_index</code>, then this is a valid assignment for the candidate at <code>core_index</code> and has delay tranche 0. Otherwise, it can be ignored.</li>
<li>If the cert kind is <code>RelayVRFDelay</code>, then we check if the VRF is valid for the validator's key with the input <code>block_entry.relay_vrf_story ++ cert.core_index.encode()</code> as described in <a href="node/approval/../../protocol-approval.html#assignment-criteria">the approvals protocol section</a>. The cert can be ignored if the block did not cause inclusion of a candidate on that core index. Otherwise, this is a valid assignment for the included candidate. The delay tranche for the assignment is determined by reducing <code>(vrf.make_bytes().to_u64() % (session_info.n_delay_tranches + session_info.zeroth_delay_tranche_width)).saturating_sub(session_info.zeroth_delay_tranche_width)</code>.</li>
<li>We also check that the core index derived by the output is covered by the <code>VRFProof</code> by means of an auxiliary signature.</li>
<li>If the delay tranche is too far in the future, return <code>AssignmentCheckResult::TooFarInFuture</code>.</li>
</ul>
</li>
<li>Import the assignment.
<ul>
<li>Load the candidate in question and access the <code>approval_entry</code> for the block hash the cert references.</li>
<li>Ignore if we already observe the validator as having been assigned.</li>
<li>Ensure the validator index is not part of the backing group for the candidate.</li>
<li>Ensure the validator index is not present in the approval entry already.</li>
<li>Create a tranche entry for the delay tranche in the approval entry and note the assignment within it.</li>
<li>Note the candidate index within the approval entry.</li>
</ul>
</li>
<li><a href="node/approval/approval-voting.html#schedule-wakeup">Schedule a wakeup</a> for this block, candidate pair.</li>
<li>return the appropriate <code>AssignmentCheckResult</code> on the response channel.</li>
</ul>
<h4 id="approvalvotingmessagecheckandimportapproval"><a class="header" href="#approvalvotingmessagecheckandimportapproval"><code>ApprovalVotingMessage::CheckAndImportApproval</code></a></h4>
<p>On receiving a <code>CheckAndImportApproval(indirect_approval_vote, response_channel)</code> message:</p>
<ul>
<li>Fetch the <code>BlockEntry</code> from the indirect approval vote's <code>block_hash</code>. If none, return <code>ApprovalCheckResult::Bad</code>.</li>
<li>Fetch the <code>CandidateEntry</code> from the indirect approval vote's <code>candidate_index</code>. If the block did not trigger inclusion of enough candidates, return <code>ApprovalCheckResult::Bad</code>.</li>
<li>Construct a <code>SignedApprovalVote</code> using the candidate hash and check against the validator's approval key, based on the session info of the block. If invalid or no such validator, return <code>ApprovalCheckResult::Bad</code>.</li>
<li>Send <code>ApprovalCheckResult::Accepted</code></li>
<li>Dispatch a <a href="node/approval/../../types/overseer-protocol.html#dispute-coordinator-message"><code>DisputeCoordinatorMessage::ImportStatement</code></a> with the approval statement.</li>
<li><a href="node/approval/approval-voting.html#import-checked-approval">Import the checked approval vote</a></li>
</ul>
<h4 id="approvalvotingmessageapprovedancestor"><a class="header" href="#approvalvotingmessageapprovedancestor"><code>ApprovalVotingMessage::ApprovedAncestor</code></a></h4>
<p>On receiving an <code>ApprovedAncestor(Hash, BlockNumber, response_channel)</code>:</p>
<ul>
<li>Iterate over the ancestry of the hash all the way back to block number given, starting from the provided block hash. Load the <code>CandidateHash</code>es from each block entry.</li>
<li>Keep track of an <code>all_approved_max: Option&lt;(Hash, BlockNumber, Vec&lt;(Hash, Vec&lt;CandidateHash&gt;))&gt;</code>.</li>
<li>For each block hash encountered, load the <code>BlockEntry</code> associated. If any are not found, return <code>None</code> on the response channel and conclude.</li>
<li>If the block entry's <code>approval_bitfield</code> has all bits set to 1 and <code>all_approved_max == None</code>, set <code>all_approved_max = Some((current_hash, current_number))</code>.</li>
<li>If the block entry's <code>approval_bitfield</code> has any 0 bits, set <code>all_approved_max = None</code>.</li>
<li>If <code>all_approved_max</code> is <code>Some</code>, push the current block hash and candidate hashes onto the list of blocks and candidates <code>all_approved_max</code>.</li>
<li>After iterating all ancestry, return <code>all_approved_max</code>.</li>
</ul>
<h3 id="updates-and-auxiliary-logic"><a class="header" href="#updates-and-auxiliary-logic">Updates and Auxiliary Logic</a></h3>
<h4 id="import-checked-approval"><a class="header" href="#import-checked-approval">Import Checked Approval</a></h4>
<ul>
<li>Import an approval vote which we can assume to have passed signature checks and correspond to an imported assignment.</li>
<li>Requires <code>(BlockEntry, CandidateEntry, ValidatorIndex)</code></li>
<li>Set the corresponding bit of the <code>approvals</code> bitfield in the <code>CandidateEntry</code> to <code>1</code>. If already <code>1</code>, return.</li>
<li>Checks the approval state of a candidate under a specific block, and updates the block and candidate entries accordingly.</li>
<li>Checks the <code>ApprovalEntry</code> for the block.
<ul>
<li><a href="node/approval/approval-voting.html#determine-required-tranches">determine the tranches to inspect</a> of the candidate,</li>
<li><a href="node/approval/approval-voting.html#check-approval">the candidate is approved under the block</a>, set the corresponding bit in the <code>block_entry.approved_bitfield</code>.</li>
<li>If the block is now fully approved and was not before, send a <a href="node/approval/../../types/overseer-protocol.html#chainselectionmessage"><code>ChainSelectionMessage::Approved</code></a>.</li>
<li>Otherwise, <a href="node/approval/approval-voting.html#schedule-wakeup">schedule a wakeup of the candidate</a></li>
</ul>
</li>
<li>If the approval vote originates locally, set the <code>our_approval_sig</code> in the candidate entry.</li>
</ul>
<h4 id="handling-wakeup"><a class="header" href="#handling-wakeup">Handling Wakeup</a></h4>
<ul>
<li>Handle a previously-scheduled wakeup of a candidate under a specific block.</li>
<li>Requires <code>(relay_block, candidate_hash)</code></li>
<li>Load the <code>BlockEntry</code> and <code>CandidateEntry</code> from disk. If either is not present, this may have lost a race with finality and can be ignored. Also load the <code>ApprovalEntry</code> for the block and candidate.</li>
<li><a href="node/approval/approval-voting.html#determine-required-tranches">determine the <code>RequiredTranches</code> of the candidate</a>.</li>
<li>Determine if we should trigger our assignment.
<ul>
<li>If we've already triggered or <code>OurAssignment</code> is <code>None</code>, we do not trigger.</li>
<li>If we have  <code>RequiredTranches::All</code>, then we trigger if the candidate is <a href="node/approval/approval-voting.html#check-approval">not approved</a>. We have no next wakeup as we assume that other validators are doing the same and we will be implicitly woken up by handling new votes.</li>
<li>If we have <code>RequiredTranches::Pending { considered, next_no_show, uncovered, maximum_broadcast, clock_drift }</code>, then we trigger if our assignment's tranche is less than or equal to <code>maximum_broadcast</code> and the current tick, with <code>clock_drift</code> applied, is at least the tick of our tranche. </li>
<li>If we have <code>RequiredTranches::Exact { .. }</code> then we do not trigger, because this value indicates that no new assignments are needed at the moment.</li>
</ul>
</li>
<li>If we should trigger our assignment
<ul>
<li>Import the assignment to the <code>ApprovalEntry</code></li>
<li>Broadcast on network with an <code>ApprovalDistributionMessage::DistributeAssignment</code>.</li>
<li><a href="node/approval/approval-voting.html#launch-approval-work">Launch approval work</a> for the candidate.</li>
</ul>
</li>
<li><a href="node/approval/approval-voting.html#schedule-wakeup">Schedule a new wakeup</a> of the candidate.</li>
</ul>
<h4 id="schedule-wakeup"><a class="header" href="#schedule-wakeup">Schedule Wakeup</a></h4>
<ul>
<li>Requires <code>(approval_entry, candidate_entry)</code> which effectively denotes a <code>(Block Hash, Candidate Hash)</code> pair - the candidate, along with the block it appears in.</li>
<li>Also requires <code>RequiredTranches</code></li>
<li>If the <code>approval_entry</code> is approved, this doesn't need to be woken up again.</li>
<li>If <code>RequiredTranches::All</code> - no wakeup. We assume other incoming votes will trigger wakeup and potentially re-schedule.</li>
<li>If <code>RequiredTranches::Pending { considered, next_no_show, uncovered, maximum_broadcast, clock_drift }</code> - schedule at the lesser of the next no-show tick, or the tick, offset positively by <code>clock_drift</code> of the next non-empty tranche we are aware of after <code>considered</code>, including any tranche containing our own unbroadcast assignment. This can lead to no wakeup in the case that we have already broadcast our assignment and there are no pending no-shows; that is, we have approval votes for every assignment we've received that is not already a no-show. In this case, we will be re-triggered by other validators broadcasting their assignments.</li>
<li>If `RequiredTranches::Exact { next_no_show, .. } - set a wakeup for the next no-show tick.</li>
</ul>
<h4 id="launch-approval-work"><a class="header" href="#launch-approval-work">Launch Approval Work</a></h4>
<ul>
<li>Requires <code>(SessionIndex, SessionInfo, CandidateReceipt, ValidatorIndex, backing_group, block_hash, candidate_index)</code></li>
<li>Extract the public key of the <code>ValidatorIndex</code> from the <code>SessionInfo</code> for the session.</li>
<li>Issue an <code>AvailabilityRecoveryMessage::RecoverAvailableData(candidate, session_index, Some(backing_group), response_sender)</code></li>
<li>Load the historical validation code of the parachain by dispatching a <code>RuntimeApiRequest::ValidationCodeByHash(</code>descriptor.validation_code_hash<code>)</code> against the state of <code>block_hash</code>.</li>
<li>Spawn a background task with a clone of <code>background_tx</code>
<ul>
<li>Wait for the available data</li>
<li>Issue a <code>CandidateValidationMessage::ValidateFromExhaustive</code> message</li>
<li>Wait for the result of validation</li>
<li>Check that the result of validation, if valid, matches the commitments in the receipt.</li>
<li>If valid, issue a message on <code>background_tx</code> detailing the request.</li>
<li>If any of the data, the candidate, or the commitments are invalid, issue on <code>background_tx</code> a <a href="node/approval/../../types/overseer-protocol.html#dispute-coordinator-message"><code>DisputeCoordinatorMessage::IssueLocalStatement</code></a> with <code>valid = false</code> to initiate a dispute.</li>
</ul>
</li>
</ul>
<h4 id="issue-approval-vote"><a class="header" href="#issue-approval-vote">Issue Approval Vote</a></h4>
<ul>
<li>Fetch the block entry and candidate entry. Ignore if <code>None</code> - we've probably just lost a race with finality.</li>
<li>Construct a <code>SignedApprovalVote</code> with the validator index for the session.</li>
<li><a href="node/approval/approval-voting.html#import-checked-approval">Import the checked approval vote</a>. It is &quot;checked&quot; as we've just issued the signature.</li>
<li>Construct a <code>IndirectSignedApprovalVote</code> using the information about the vote.</li>
<li>Dispatch <code>ApprovalDistributionMessage::DistributeApproval</code>.</li>
</ul>
<h3 id="determining-approval-of-candidate"><a class="header" href="#determining-approval-of-candidate">Determining Approval of Candidate</a></h3>
<h4 id="determine-required-tranches"><a class="header" href="#determine-required-tranches">Determine Required Tranches</a></h4>
<p>This logic is for inspecting an approval entry that tracks the assignments received, along with information on which assignments have corresponding approval votes. Inspection also involves the current time and expected requirements and is used to help the higher-level code determine the following:</p>
<ul>
<li>Whether to broadcast the local assignment</li>
<li>Whether to check that the candidate entry has been completely approved.</li>
<li>If the candidate is waiting on approval, when to schedule the next wakeup of the <code>(candidate, block)</code> pair at a point where the state machine could be advanced.</li>
</ul>
<p>These routines are pure functions which only depend on the environmental state. The expectation is that this determination is re-run every time we attempt to update an approval entry: either when we trigger a wakeup to advance the state machine based on a no-show or our own broadcast, or when we receive further assignments or approvals from the network.</p>
<p>Thus it may be that at some point in time, we consider that tranches 0..X is required to be considered, but as we receive more information, we might require fewer tranches. Or votes that we perceived to be missing and require replacement are filled in and change our view.</p>
<p>Requires <code>(approval_entry, approvals_received, tranche_now, block_tick, no_show_duration, needed_approvals)</code></p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum RequiredTranches {
  // All validators appear to be required, based on tranches already taken and remaining no-shows.
  All,
  // More tranches required - We're awaiting more assignments.
  Pending {
    /// The highest considered delay tranche when counting assignments.
    considered: DelayTranche,
    /// The tick at which the next no-show, of the assignments counted, would occur.
    next_no_show: Option&lt;Tick&gt;,
    /// The highest tranche to consider when looking to broadcast own assignment.
    /// This should be considered along with the clock drift to avoid broadcasting
    /// assignments that are before the local time.
    maximum_broadcast: DelayTranche,
    /// The clock drift, in ticks, to apply to the local clock when determining whether
    /// to broadcast an assignment or when to schedule a wakeup. The local clock should be treated
    /// as though it is `clock_drift` ticks earlier.
    clock_drift: Tick,
  },
  // An exact number of required tranches and a number of no-shows. This indicates that the amount of `needed_approvals` are assigned and additionally all no-shows are covered.
  Exact {
    /// The tranche to inspect up to.
    needed: DelayTranche,
    /// The amount of missing votes that should be tolerated.
    tolerated_missing: usize,
    /// When the next no-show would be, if any. This is used to schedule the next wakeup in the
    /// event that there are some assignments that don't have corresponding approval votes. If this
    /// is `None`, all assignments have approvals.
    next_no_show: Option&lt;Tick&gt;,
  }
}
<span class="boring">}
</span></code></pre></pre>
<p><strong>Clock-drift and Tranche-taking</strong></p>
<p>Our vote-counting procedure depends heavily on how we interpret time based on the presence of no-shows - assignments which have no corresponding approval after some time.</p>
<p>We have this is because of how we handle no-shows: we keep track of the depth of no-shows we are covering. </p>
<p>As an example: there may be initial no-shows in tranche 0. It'll take <code>no_show_duration</code> ticks before those are considered no-shows. Then, we don't want to immediately take <code>no_show_duration</code> more tranches. Instead, we want to take one tranche for each uncovered no-show. However, as we take those tranches, there may be further no-shows. Since these depth-1 no-shows should have only been triggered after the depth-0 no-shows were already known to be no-shows, we need to discount the local clock by <code>no_show_duration</code> to  see whether these should be considered no-shows or not. There may be malicious parties who broadcast their assignment earlier than they were meant to, who shouldn't be counted as instant no-shows. We continue onwards to cover all depth-1 no-shows which may lead to depth-2 no-shows and so on.</p>
<p>Likewise, when considering how many tranches to take, the no-show depth should be used to apply a depth-discount or clock drift to the <code>tranche_now</code>.</p>
<p><strong>Procedure</strong></p>
<ul>
<li>Start with <code>depth = 0</code>.</li>
<li>Set a clock drift of <code>depth * no_show_duration</code></li>
<li>Take tranches up to <code>tranche_now - clock_drift</code> until all needed assignments are met.</li>
<li>Keep track of the <code>next_no_show</code> according to the clock drift, as we go.</li>
<li>If running out of tranches before then, return <code>Pending { considered, next_no_show, maximum_broadcast, clock_drift }</code></li>
<li>If there are no no-shows, return <code>Exact { needed, tolerated_missing, next_no_show }</code></li>
<li><code>maximum_broadcast</code> is either <code>DelayTranche::max_value()</code> at tranche 0 or otherwise by the last considered tranche + the number of uncovered no-shows at this point.</li>
<li>If there are no-shows, return to the beginning, incrementing <code>depth</code> and attempting to cover the number of no-shows. Each no-show must be covered by a non-empty tranche, which are tranches that have at least one assignment. Each non-empty tranche covers exactly one no-show.</li>
<li>If at any point, it seems that all validators are required, do an early return with <code>RequiredTranches::All</code> which indicates that everyone should broadcast.</li>
</ul>
<h4 id="check-approval"><a class="header" href="#check-approval">Check Approval</a></h4>
<ul>
<li>Check whether a candidate is approved under a particular block.</li>
<li>Requires <code>(block_entry, candidate_entry, approval_entry, n_tranches)</code></li>
<li>If we have <code>3 * n_approvals &gt; n_validators</code>, return true. This is because any set with f+1 validators must have at least one honest validator, who has approved the candidate.</li>
<li>If <code>n_tranches</code> is <code>RequiredTranches::Pending</code>, return false</li>
<li>If <code>n_tranches</code> is <code>RequiredTranches::All</code>, return false.</li>
<li>If <code>n_tranches</code> is <code>RequiredTranches::Exact { tranche, tolerated_missing, .. }</code>, then we return whether all assigned validators up to <code>tranche</code> less <code>tolerated_missing</code> have approved. e.g. if we had 5 tranches and 1 tolerated missing, we would accept only if all but 1 of assigned validators in tranches 0..=5 have approved. In that example, we also accept all validators in tranches 0..=5 having approved, but that would indicate that the <code>RequiredTranches</code> value was incorrectly constructed, so it is not realistic. <code>tolerated_missing</code> actually represents covered no-shows. If there are more missing approvals than there are tolerated missing, that indicates that there are some assignments which are not yet no-shows, but may become no-shows, and we should wait for the validators to either approve or become no-shows.</li>
</ul>
<h3 id="time"><a class="header" href="#time">Time</a></h3>
<h4 id="current-tranche"><a class="header" href="#current-tranche">Current Tranche</a></h4>
<ul>
<li>Given the slot number of a block, and the current time, this informs about the current tranche.</li>
<li>Convert <code>time.saturating_sub(slot_number.to_time())</code> to a delay tranches value</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="approval-distribution"><a class="header" href="#approval-distribution">Approval Distribution</a></h1>
<p>A subsystem for the distribution of assignments and approvals for approval checks on candidates over the network.</p>
<p>The <a href="node/approval/approval-voting.html">Approval Voting</a> subsystem is responsible for active participation in a protocol designed to select a sufficient number of validators to check each and every candidate which appears in the relay chain. Statements of participation in this checking process are divided into two kinds:</p>
<ul>
<li><strong>Assignments</strong> indicate that validators have been selected to do checking</li>
<li><strong>Approvals</strong> indicate that validators have checked and found the candidate satisfactory.</li>
</ul>
<p>The <a href="node/approval/approval-voting.html">Approval Voting</a> subsystem handles all the issuing and tallying of this protocol, but this subsystem is responsible for the disbursal of statements among the validator-set.</p>
<p>The inclusion pipeline of candidates concludes after availability, and only after inclusion do candidates actually get pushed into the approval checking pipeline. As such, this protocol deals with the candidates <em>made available by</em> particular blocks, as opposed to the candidates which actually appear within those blocks, which are the candidates <em>backed by</em> those blocks. Unless stated otherwise, whenever we reference a candidate partially by block hash, we are referring to the set of candidates <em>made available by</em> those blocks.</p>
<p>We implement this protocol as a gossip protocol, and like other parachain-related gossip protocols our primary concerns are about ensuring fast message propagation while maintaining an upper bound on the number of messages any given node must store at any time.</p>
<p>Approval messages should always follow assignments, so we need to be able to discern two pieces of information based on our <a href="node/approval/../../types/network.html#universal-types">View</a>:</p>
<ol>
<li>Is a particular assignment relevant under a given <code>View</code>?</li>
<li>Is a particular approval relevant to any assignment in a set?</li>
</ol>
<p>For our own local view, these two queries  must not yield false negatives. When applied to our peers' views, it is acceptable for them to yield false negatives. The reason for that is that our peers' views may be beyond ours, and we are not capable of fully evaluating them. Once we have caught up, we can check again for false negatives to continue distributing.</p>
<p>For assignments, what we need to be checking is whether we are aware of the (block, candidate) pair that the assignment references. For approvals, we need to be aware of an assignment by the same validator which references the candidate being approved.</p>
<p>However, awareness on its own of a (block, candidate) pair would imply that even ancient candidates all the way back to the genesis are relevant. We are actually not interested in anything before finality.</p>
<h2 id="protocol-9"><a class="header" href="#protocol-9">Protocol</a></h2>
<p>Input:</p>
<ul>
<li><code>ApprovalDistributionMessage::NewBlocks</code></li>
<li><code>ApprovalDistributionMessage::DistributeAssignment</code></li>
<li><code>ApprovalDistributionMessage::DistributeApproval</code></li>
<li><code>ApprovalDistributionMessage::NetworkBridgeUpdateV1</code></li>
<li><code>OverseerSignal::BlockFinalized</code></li>
</ul>
<p>Output:</p>
<ul>
<li><code>ApprovalVotingMessage::CheckAndImportAssignment</code></li>
<li><code>ApprovalVotingMessage::CheckAndImportApproval</code></li>
<li><code>NetworkBridgeMessage::SendValidationMessage::ApprovalDistribution</code></li>
</ul>
<h2 id="functionality-9"><a class="header" href="#functionality-9">Functionality</a></h2>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>type BlockScopedCandidate = (Hash, CandidateHash);

enum PendingMessage {
  Assignment(IndirectAssignmentCert, CoreIndex),
  Approval(IndirectSignedApprovalVote),
}

/// The `State` struct is responsible for tracking the overall state of the subsystem.
///
/// It tracks metadata about our view of the unfinalized chain, which assignments and approvals we have seen, and our peers' views.
struct State {
  // These two fields are used in conjunction to construct a view over the unfinalized chain.
  blocks_by_number: BTreeMap&lt;BlockNumber, Vec&lt;Hash&gt;&gt;,
  blocks: HashMap&lt;Hash, BlockEntry&gt;,

  /// Our view updates to our peers can race with `NewBlocks` updates. We store messages received
  /// against the directly mentioned blocks in our view in this map until `NewBlocks` is received.
  ///
  /// As long as the parent is already in the `blocks` map and `NewBlocks` messages aren't delayed
  /// by more than a block length, this strategy will work well for mitigating the race. This is
  /// also a race that occurs typically on local networks.
  pending_known: HashMap&lt;Hash, Vec&lt;(PeerId, PendingMessage&gt;)&gt;&gt;,

  // Peer view data is partially stored here, and partially inline within the `BlockEntry`s
  peer_views: HashMap&lt;PeerId, View&gt;,
}

enum MessageFingerprint {
  Assigment(Hash, u32, ValidatorIndex),
  Approval(Hash, u32, ValidatorIndex),
}

struct Knowledge {
  known_messages: HashSet&lt;MessageFingerprint&gt;,
}

struct PeerKnowledge {
  /// The knowledge we've sent to the peer.
  sent: Knowledge,
  /// The knowledge we've received from the peer.
  received: Knowledge,
}

/// Information about blocks in our current view as well as whether peers know of them.
struct BlockEntry {
  // Peers who we know are aware of this block and thus, the candidates within it. This maps to their knowledge of messages.
  known_by: HashMap&lt;PeerId, PeerKnowledge&gt;,
  // The number of the block.
  number: BlockNumber,
  // The parent hash of the block.
  parent_hash: Hash,
  // Our knowledge of messages.
  knowledge: Knowledge,
  // A votes entry for each candidate.
  candidates: IndexMap&lt;CandidateHash, CandidateEntry&gt;,
}

enum ApprovalState {
  Assigned(AssignmentCert),
  Approved(AssignmentCert, ApprovalSignature),
}

/// Information about candidates in the context of a particular block they are included in. In other words,
/// multiple `CandidateEntry`s may exist for the same candidate, if it is included by multiple blocks - this is likely the case
/// when there are forks.
struct CandidateEntry {
  approvals: HashMap&lt;ValidatorIndex, ApprovalState&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<h3 id="network-updates"><a class="header" href="#network-updates">Network updates</a></h3>
<h4 id="networkbridgeeventpeerconnected"><a class="header" href="#networkbridgeeventpeerconnected"><code>NetworkBridgeEvent::PeerConnected</code></a></h4>
<p>Add a blank view to the <code>peer_views</code> state.</p>
<h4 id="networkbridgeeventpeerdisconnected"><a class="header" href="#networkbridgeeventpeerdisconnected"><code>NetworkBridgeEvent::PeerDisconnected</code></a></h4>
<p>Remove the view under the associated <code>PeerId</code> from <code>State::peer_views</code>.</p>
<p>Iterate over every <code>BlockEntry</code> and remove <code>PeerId</code> from it.</p>
<h4 id="networkbridgeeventourviewchange"><a class="header" href="#networkbridgeeventourviewchange"><code>NetworkBridgeEvent::OurViewChange</code></a></h4>
<p>Remove entries in <code>pending_known</code> for all hashes not present in the view.
Ensure a vector is present in <code>pending_known</code> for each hash in the view that does not have an entry in <code>blocks</code>.</p>
<h4 id="networkbridgeeventpeerviewchange"><a class="header" href="#networkbridgeeventpeerviewchange"><code>NetworkBridgeEvent::PeerViewChange</code></a></h4>
<p>Invoke <code>unify_with_peer(peer, view)</code> to catch them up to messages we have.</p>
<p>We also need to use the <code>view.finalized_number</code> to remove the <code>PeerId</code> from any blocks that it won't be wanting information about anymore. Note that we have to be on guard for peers doing crazy stuff like jumping their 'finalized_number` forward 10 trillion blocks to try and get us stuck in a loop for ages.</p>
<p>One of the safeguards we can implement is to reject view updates from peers where the new <code>finalized_number</code> is less than the previous.</p>
<p>We augment that by defining <code>constrain(x)</code> to output the x bounded by the first and last numbers in <code>state.blocks_by_number</code>.</p>
<p>From there, we can loop backwards from <code>constrain(view.finalized_number)</code> until <code>constrain(last_view.finalized_number)</code> is reached, removing the <code>PeerId</code> from all <code>BlockEntry</code>s referenced at that height. We can break the loop early if we ever exit the bound supplied by the first block in <code>state.blocks_by_number</code>.</p>
<h4 id="networkbridgeeventpeermessage"><a class="header" href="#networkbridgeeventpeermessage"><code>NetworkBridgeEvent::PeerMessage</code></a></h4>
<p>If the block hash referenced by the message exists in <code>pending_known</code>, add it to the vector of pending messages and return.</p>
<p>If the message is of type <code>ApprovalDistributionV1Message::Assignment(assignment_cert, claimed_index)</code>, then call <code>import_and_circulate_assignment(MessageSource::Peer(sender), assignment_cert, claimed_index)</code></p>
<p>If the message is of type <code>ApprovalDistributionV1Message::Approval(approval_vote)</code>, then call <code>import_and_circulate_approval(MessageSource::Peer(sender), approval_vote)</code></p>
<h3 id="subsystem-updates"><a class="header" href="#subsystem-updates">Subsystem Updates</a></h3>
<h4 id="approvaldistributionmessagenewblocks"><a class="header" href="#approvaldistributionmessagenewblocks"><code>ApprovalDistributionMessage::NewBlocks</code></a></h4>
<p>Create <code>BlockEntry</code> and <code>CandidateEntries</code> for all blocks.</p>
<p>For all entries in <code>pending_known</code>:</p>
<ul>
<li>If there is now an entry under <code>blocks</code> for the block hash, drain all messages and import with <code>import_and_circulate_assignment</code> and <code>import_and_circulate_approval</code>.</li>
</ul>
<p>For all peers:</p>
<ul>
<li>Compute <code>view_intersection</code> as the intersection of the peer's view blocks with the hashes of the new blocks.</li>
<li>Invoke <code>unify_with_peer(peer, view_intersection)</code>.</li>
</ul>
<h4 id="approvaldistributionmessagedistributeasignment"><a class="header" href="#approvaldistributionmessagedistributeasignment"><code>ApprovalDistributionMessage::DistributeAsignment</code></a></h4>
<p>Call <code>import_and_circulate_assignment</code> with <code>MessageSource::Local</code>.</p>
<h4 id="approvaldistributionmessagedistributeapproval"><a class="header" href="#approvaldistributionmessagedistributeapproval"><code>ApprovalDistributionMessage::DistributeApproval</code></a></h4>
<p>Call <code>import_and_circulate_approval</code> with <code>MessageSource::Local</code>.</p>
<h4 id="overseersignalblockfinalized-1"><a class="header" href="#overseersignalblockfinalized-1"><code>OverseerSignal::BlockFinalized</code></a></h4>
<p>Prune all lists from <code>blocks_by_number</code> with number less than or equal to <code>finalized_number</code>. Prune all the <code>BlockEntry</code>s referenced by those lists.</p>
<h3 id="utility"><a class="header" href="#utility">Utility</a></h3>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum MessageSource {
  Peer(PeerId),
  Local,
}
<span class="boring">}
</span></code></pre></pre>
<h4 id="import_and_circulate_assignmentsource-messagesource-assignment-indirectassignmentcert-claimed_candidate_index-candidateindex"><a class="header" href="#import_and_circulate_assignmentsource-messagesource-assignment-indirectassignmentcert-claimed_candidate_index-candidateindex"><code>import_and_circulate_assignment(source: MessageSource, assignment: IndirectAssignmentCert, claimed_candidate_index: CandidateIndex)</code></a></h4>
<p>Imports an assignment cert referenced by block hash and candidate index. As a postcondition, if the cert is valid, it will have distributed the cert to all peers who have the block in their view, with the exclusion of the peer referenced by the <code>MessageSource</code>.</p>
<p>We maintain a few invariants:</p>
<ul>
<li>we only send an assignment to a peer after we add its fingerprint to our knowledge</li>
<li>we add a fingerprint of an assignment to our knowledge only if it's valid and hasn't been added before</li>
</ul>
<p>The algorithm is the following:</p>
<ul>
<li>Load the BlockEntry using <code>assignment.block_hash</code>. If it does not exist, report the source if it is <code>MessageSource::Peer</code> and return.</li>
<li>Compute a fingerprint for the <code>assignment</code> using <code>claimed_candidate_index</code>.</li>
<li>If the source is <code>MessageSource::Peer(sender)</code>:
<ul>
<li>check if <code>peer</code> appears under <code>known_by</code> and whether the fingerprint is in the knowledge of the peer. If the peer does not know the block, report for providing data out-of-view and proceed. If the peer does know the block and the <code>sent</code> knowledge contains the fingerprint, report for providing replicate data and return, otherwise, insert into the <code>received</code> knowledge and return.</li>
<li>If the message fingerprint appears under the <code>BlockEntry</code>'s <code>Knowledge</code>, give the peer a small positive reputation boost,
add the fingerprint to the peer's knowledge only if it knows about the block and return.
Note that we must do this after checking for out-of-view and if the peers knows about the block to avoid being spammed.
If we did this check earlier, a peer could provide data out-of-view repeatedly and be rewarded for it.</li>
<li>Dispatch <code>ApprovalVotingMessage::CheckAndImportAssignment(assignment)</code> and wait for the response.</li>
<li>If the result is <code>AssignmentCheckResult::Accepted</code>
<ul>
<li>If the vote was accepted but not duplicate, give the peer a positive reputation boost</li>
<li>add the fingerprint to both our and the peer's knowledge in the <code>BlockEntry</code>. Note that we only doing this after making sure we have the right fingerprint.</li>
</ul>
</li>
<li>If the result is <code>AssignmentCheckResult::AcceptedDuplicate</code>, add the fingerprint to the peer's knowledge if it knows about the block and return.</li>
<li>If the result is <code>AssignmentCheckResult::TooFarInFuture</code>, mildly punish the peer and return.</li>
<li>If the result is <code>AssignmentCheckResult::Bad</code>, punish the peer and return.</li>
</ul>
</li>
<li>If the source is <code>MessageSource::Local(CandidateIndex)</code>
<ul>
<li>check if the fingerprint appears under the <code>BlockEntry's</code> knowledge. If not, add it.</li>
</ul>
</li>
<li>Load the candidate entry for the given candidate index. It should exist unless there is a logic error in the approval voting subsystem.</li>
<li>Set the approval state for the validator index to <code>ApprovalState::Assigned</code> unless the approval state is set already. This should not happen as long as the approval voting subsystem instructs us to ignore duplicate assignments.</li>
<li>Dispatch a <code>ApprovalDistributionV1Message::Assignment(assignment, candidate_index)</code> to all peers in the <code>BlockEntry</code>'s <code>known_by</code> set, excluding the peer in the <code>source</code>, if <code>source</code> has kind <code>MessageSource::Peer</code>. Add the fingerprint of the assignment to the knowledge of each peer.</li>
</ul>
<h4 id="import_and_circulate_approvalsource-messagesource-approval-indirectsignedapprovalvote"><a class="header" href="#import_and_circulate_approvalsource-messagesource-approval-indirectsignedapprovalvote"><code>import_and_circulate_approval(source: MessageSource, approval: IndirectSignedApprovalVote)</code></a></h4>
<p>Imports an approval signature referenced by block hash and candidate index:</p>
<ul>
<li>Load the BlockEntry using <code>approval.block_hash</code> and the candidate entry using <code>approval.candidate_entry</code>. If either does not exist, report the source if it is <code>MessageSource::Peer</code> and return.</li>
<li>Compute a fingerprint for the approval.</li>
<li>Compute a fingerprint for the corresponding assignment. If the <code>BlockEntry</code>'s knowledge does not contain that fingerprint, then report the source if it is <code>MessageSource::Peer</code> and return. All references to a fingerprint after this refer to the approval's, not the assignment's.</li>
<li>If the source is <code>MessageSource::Peer(sender)</code>:
<ul>
<li>check if <code>peer</code> appears under <code>known_by</code> and whether the fingerprint is in the knowledge of the peer. If the peer does not know the block, report for providing data out-of-view and proceed. If the peer does know the block and the <code>sent</code> knowledge contains the fingerprint, report for providing replicate data and return, otherwise, insert into the <code>received</code> knowledge and return.</li>
<li>If the message fingerprint appears under the <code>BlockEntry</code>'s <code>Knowledge</code>, give the peer a small positive reputation boost,
add the fingerprint to the peer's knowledge only if it knows about the block and return.
Note that we must do this after checking for out-of-view to avoid being spammed. If we did this check earlier, a peer could provide data out-of-view repeatedly and be rewarded for it.</li>
<li>Dispatch <code>ApprovalVotingMessage::CheckAndImportApproval(approval)</code> and wait for the response.</li>
<li>If the result is <code>VoteCheckResult::Accepted(())</code>:
<ul>
<li>Give the peer a positive reputation boost and add the fingerprint to both our and the peer's knowledge.</li>
</ul>
</li>
<li>If the result is <code>VoteCheckResult::Bad</code>:
<ul>
<li>Report the peer and return.</li>
</ul>
</li>
</ul>
</li>
<li>Load the candidate entry for the given candidate index. It should exist unless there is a logic error in the approval voting subsystem.</li>
<li>Set the approval state for the validator index to <code>ApprovalState::Approved</code>. It should already be in the <code>Assigned</code> state as our <code>BlockEntry</code> knowledge contains a fingerprint for the assignment.</li>
<li>Dispatch a <code>ApprovalDistributionV1Message::Approval(approval)</code> to all peers in the <code>BlockEntry</code>'s <code>known_by</code> set, excluding the peer in the <code>source</code>, if <code>source</code> has kind <code>MessageSource::Peer</code>. Add the fingerprint of the assignment to the knowledge of each peer. Note that this obeys the politeness conditions:
<ul>
<li>We guarantee elsewhere that all peers within <code>known_by</code> are aware of all assignments relative to the block.</li>
<li>We've checked that this specific approval has a corresponding assignment within the <code>BlockEntry</code>.</li>
<li>Thus, all peers are aware of the assignment or have a message to them in-flight which will make them so.</li>
</ul>
</li>
</ul>
<h4 id="unify_with_peerpeer-peerid-view"><a class="header" href="#unify_with_peerpeer-peerid-view"><code>unify_with_peer(peer: PeerId, view)</code>:</a></h4>
<ol>
<li>Initialize a set <code>fresh_blocks = {}</code></li>
</ol>
<p>For each block in the view:</p>
<ol start="2">
<li>
<p>Load the <code>BlockEntry</code> for the block. If the block is unknown, or the number is less than or equal to the view's finalized number go to step 6.</p>
</li>
<li>
<p>Inspect the <code>known_by</code> set of the <code>BlockEntry</code>. If the peer is already present, go to step 6.</p>
</li>
<li>
<p>Add the peer to <code>known_by</code> with a cloned version of <code>block_entry.knowledge</code>. and add the hash of the block to <code>fresh_blocks</code>.</p>
</li>
<li>
<p>Return to step 2 with the ancestor of the block.</p>
</li>
<li>
<p>For each block in <code>fresh_blocks</code>, send all assignments and approvals for all candidates in those blocks to the peer.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="disputes-subsystems"><a class="header" href="#disputes-subsystems">Disputes Subsystems</a></h1>
<p>This section is for the node-side subsystems that lead to participation in disputes. There are five major roles that validator nodes must participate in when it comes to disputes</p>
<ul>
<li>Detection. Detect bad parablocks, either during candidate backing or approval checking, and initiate a dispute.</li>
<li>Participation. Participate in active disputes. When a node becomes aware of a dispute, it should recover the data for the disputed block, check the validity of the parablock, and issue a statement regarding the validity of the parablock.</li>
<li>Distribution. Validators should notify each other of active disputes and relevant statements over the network.</li>
<li>Submission. When authoring a block, a validator should inspect the state of the parent block and provide any information about disputes that the chain needs as part of the ParaInherent. This should initialize new disputes on-chain as necessary.</li>
<li>Fork-choice and Finality. When observing a block issuing a DisputeRollback digest in the header, that branch of the relay chain should be abandoned all the way back to the indicated block. When voting on chains in GRANDPA, no chains that contain blocks that are or have been disputed should be voted on.</li>
</ul>
<h2 id="components-1"><a class="header" href="#components-1">Components</a></h2>
<h3 id="dispute-coordinator"><a class="header" href="#dispute-coordinator">Dispute Coordinator</a></h3>
<p>This component is responsible for coordinating other subsystems around disputes.</p>
<p>This component should track all statements received from all validators over some window of sessions. This includes backing statements, approval votes, and statements made specifically for disputes. This will be used to identify disagreements or instances where validators conflict with themselves.</p>
<p>This is responsible for tracking and initiating disputes. Disputes will be initiated either externally by another subsystem which has identified an issue with a parablock or internally upon observing two statements which conflict with each other on the validity of a parablock.</p>
<p>No more than one statement by each validator on each side of the dispute needs to be stored. That is, validators are allowed to participate on both sides of the dispute, although we won't write code to do so. Such behavior has negative EV in the runtime.</p>
<p>This will notify the dispute participation subsystem of a new dispute if the local validator has not issued any statements on the disputed candidate already.</p>
<p>Locally authored statements related to disputes will be forwarded to the dispute distribution subsystem.</p>
<p>This subsystem also provides two further behaviors for the interactions between disputes and fork-choice</p>
<ul>
<li>Enhancing the finality voting rule. Given description of a chain and candidates included at different heights in that chain, it returns the BlockHash corresponding to the highest BlockNumber that there are no disputes before. I expect that we will slightly change ApprovalVoting::ApprovedAncestor to return this set and then the target block to vote on will be further constrained by this function.</li>
<li>Chain roll-backs. Whenever importing new blocks, the header should be scanned for a roll-back digest. If there is one, the chain should be rolled back according to the digest. I expect this would be implemented with a ChainApi function and possibly an ApprovalVoting function to clean up the approval voting DB.</li>
</ul>
<h3 id="dispute-participation-1"><a class="header" href="#dispute-participation-1">Dispute Participation</a></h3>
<p>This subsystem ties together the dispute tracker, availability recovery, candidate validation, and dispute distribution subsystems. When notified of a new dispute by the Dispute Tracker, the data should be recovered, the validation code loaded from the relay chain, and the candidate is executed.</p>
<p>A statement depending on the outcome of the execution is produced, signed, and imported to the dispute tracker.</p>
<h3 id="dispute-distribution"><a class="header" href="#dispute-distribution">Dispute Distribution</a></h3>
<p>This is a networking component by which validators notify each other of live disputes and statements on those disputes.</p>
<p>Validators will in the future distribute votes to each other via the network, but at the moment learn of disputes just from watching the chain.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dispute-coordinator-1"><a class="header" href="#dispute-coordinator-1">Dispute Coordinator</a></h1>
<p>This is the central subsystem of the node-side components which participate in disputes. This subsystem wraps a database which tracks all statements observed by all validators over some window of sessions. Votes older than this session window are pruned.</p>
<p>This subsystem will be the point which produce dispute votes, either positive or negative, based on locally-observed validation results as well as a sink for votes received by other subsystems. When importing a dispute vote from another node, this will trigger the <a href="node/disputes/dispute-participation.html">dispute participation</a> subsystem to recover and validate the block and call back to this subsystem.</p>
<h2 id="database-schema-1"><a class="header" href="#database-schema-1">Database Schema</a></h2>
<p>We use an underlying Key-Value database where we assume we have the following operations available:</p>
<ul>
<li><code>write(key, value)</code></li>
<li><code>read(key) -&gt; Option&lt;value&gt;</code></li>
<li><code>iter_with_prefix(prefix) -&gt; Iterator&lt;(key, value)&gt;</code> - gives all keys and values in lexicographical order where the key starts with <code>prefix</code>.</li>
</ul>
<p>We use this database to encode the following schema:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>(&quot;candidate-votes&quot;, SessionIndex, CandidateHash) -&gt; Option&lt;CandidateVotes&gt;
&quot;active-disputes&quot; -&gt; ActiveDisputes
&quot;earliest-session&quot; -&gt; Option&lt;SessionIndex&gt;
<span class="boring">}
</span></code></pre></pre>
<p>The meta information that we track per-candidate is defined as the <code>CandidateVotes</code> struct.
This draws on the <a href="node/disputes/../../types/disputes.html">dispute statement types</a></p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct CandidateVotes {
    // The receipt of the candidate itself.
    candidate_receipt: CandidateReceipt,
    // Sorted by validator index.
    valid: Vec&lt;(ValidDisputeStatementKind, ValidatorIndex, ValidatorSignature)&gt;,
    // Sorted by validator index.
    invalid: Vec&lt;(InvalidDisputeStatementKind, ValidatorIndex, ValidatorSignature)&gt;,
}

struct ActiveDisputes {
    // sorted by session index and then by candidate hash.
    disputed: Vec&lt;(SessionIndex, CandidateHash)&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="protocol-10"><a class="header" href="#protocol-10">Protocol</a></h2>
<p>Input: <a href="node/disputes/../../types/overseer-protocol.html#dispute-coordinator-message"><code>DisputeCoordinatorMessage</code></a></p>
<p>Output:</p>
<ul>
<li><a href="node/disputes/../../types/overseer-protocol.html#runtime-api-message"><code>RuntimeApiMessage</code></a></li>
<li><a href="node/disputes/../../types/overseer-protocol.html#dispute-participation-message"><code>DisputeParticipationMessage</code></a></li>
</ul>
<h2 id="functionality-10"><a class="header" href="#functionality-10">Functionality</a></h2>
<p>This assumes a constant <code>DISPUTE_WINDOW: SessionIndex</code>. This should correspond to at least 1 day.</p>
<p>Ephemeral in-memory state:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct State {
    keystore: KeyStore,
    highest_session: SessionIndex,
}
<span class="boring">}
</span></code></pre></pre>
<h3 id="on-overseersignalactiveleavesupdate"><a class="header" href="#on-overseersignalactiveleavesupdate">On <code>OverseerSignal::ActiveLeavesUpdate</code></a></h3>
<p>For each leaf in the leaves update:</p>
<ul>
<li>Fetch the session index for the child of the block with a <a href="node/disputes/../../types/overseer-protocol.html#runtime-api-message"><code>RuntimeApiMessage::SessionIndexForChild</code></a>.</li>
<li>If the session index is higher than <code>state.highest_session</code>:
<ul>
<li>update <code>state.highest_session</code></li>
<li>remove everything with session index less than <code>state.highest_session - DISPUTE_WINDOW</code> from the <code>&quot;active-disputes&quot;</code> in the DB.</li>
<li>Use <code>iter_with_prefix</code> to remove everything from <code>&quot;earliest-session&quot;</code> up to <code>state.highest_session - DISPUTE_WINDOW</code> from the DB under <code>&quot;candidate-votes&quot;</code>.</li>
<li>Update <code>&quot;earliest-session&quot;</code> to be equal to <code>state.highest_session - DISPUTE_WINDOW</code>.</li>
</ul>
</li>
<li>For each new block, explicitly or implicitly, under the new leaf, scan for a dispute digest which indicates a rollback. If a rollback is detected, use the ChainApi subsystem to blacklist the chain.</li>
</ul>
<h3 id="on-overseersignalconclude"><a class="header" href="#on-overseersignalconclude">On <code>OverseerSignal::Conclude</code></a></h3>
<p>Exit gracefully.</p>
<h3 id="on-overseersignalblockfinalized"><a class="header" href="#on-overseersignalblockfinalized">On <code>OverseerSignal::BlockFinalized</code></a></h3>
<p>Do nothing.</p>
<h3 id="on-disputecoordinatormessageimportstatement"><a class="header" href="#on-disputecoordinatormessageimportstatement">On <code>DisputeCoordinatorMessage::ImportStatement</code></a></h3>
<ul>
<li>Deconstruct into parts <code>{ candidate_hash, candidate_receipt, session, statements }</code>.</li>
<li>If the session is earlier than <code>state.highest_session - DISPUTE_WINDOW</code>, return.</li>
<li>Load from underlying DB by querying `(&quot;candidate-votes&quot;, session, candidate_hash). If that does not exist, create fresh with the given candidate receipt.</li>
<li>If candidate votes is empty and the statements only contain dispute-specific votes, return.</li>
<li>Otherwise, if there is already an entry from the validator in the respective <code>valid</code> or <code>invalid</code> field of the <code>CandidateVotes</code>, return.</li>
<li>Add an entry to the respective <code>valid</code> or <code>invalid</code> list of the <code>CandidateVotes</code> for each statement in <code>statements</code>.</li>
<li>Write the <code>CandidateVotes</code> to the underyling DB.</li>
<li>If the both <code>valid</code> and <code>invalid</code> lists now have non-zero length where previously one or both had zero length, the candidate is now freshly disputed.</li>
<li>If freshly disputed, load <code>&quot;active-disputes&quot;</code> and add the candidate hash and session index. Also issue a <a href="node/disputes/../../types/overseer-protocol.html#dispute-participation-message"><code>DisputeParticipationMessage::Participate</code></a>.</li>
<li>If the dispute now has supermajority votes in the &quot;valid&quot; direction, according to the <code>SessionInfo</code> of the dispute candidate's session, remove from <code>&quot;active-disputes&quot;</code>.</li>
<li>If the dispute now has supermajority votes in the &quot;invalid&quot; direction, there is no need to do anything explicitly. The actual rollback will be handled during the active leaves update by observing digests from the runtime.</li>
<li>Write <code>&quot;active-disputes&quot;</code></li>
</ul>
<h3 id="on-disputecoordinatormessageactivedisputes"><a class="header" href="#on-disputecoordinatormessageactivedisputes">On <code>DisputeCoordinatorMessage::ActiveDisputes</code></a></h3>
<ul>
<li>Load <code>&quot;active-disputes&quot;</code> and return the data contained within.</li>
</ul>
<h3 id="on-disputecoordinatormessagequerycandidatevotes"><a class="header" href="#on-disputecoordinatormessagequerycandidatevotes">On <code>DisputeCoordinatorMessage::QueryCandidateVotes</code></a></h3>
<ul>
<li>Load <code>&quot;candidate-votes&quot;</code> and return the data within or <code>None</code> if missing.</li>
</ul>
<h3 id="on-disputecoordinatormessageissuelocalstatement"><a class="header" href="#on-disputecoordinatormessageissuelocalstatement">On <code>DisputeCoordinatorMessage::IssueLocalStatement</code></a></h3>
<ul>
<li>Deconstruct into parts <code>{ session_index, candidate_hash, candidate_receipt, is_valid }</code>.</li>
<li>Construct a <a href="node/disputes/../../types/disputes.html#disputestatement"><code>DisputeStatement</code></a> based on <code>Valid</code> or <code>Invalid</code>, depending on the parameterization of this routine.</li>
<li>Sign the statement with each key in the <code>SessionInfo</code>'s list of parachain validation keys which is present in the keystore, except those whose indices appear in <code>voted_indices</code>. This will typically just be one key, but this does provide some future-proofing for situations where the same node may run on behalf multiple validators. At the time of writing, this is not a use-case we support as other subsystems do not invariably provide this guarantee.</li>
<li>Write statement to DB.</li>
<li>Send a <code>DisputeDistributionMessage::SendDispute</code> message to get the vote
distributed to other validators.</li>
</ul>
<h3 id="on-disputecoordinatormessagedetermineundisputedchain"><a class="header" href="#on-disputecoordinatormessagedetermineundisputedchain">On <code>DisputeCoordinatorMessage::DetermineUndisputedChain</code></a></h3>
<ul>
<li>Load <code>&quot;active-disputes&quot;</code>.</li>
<li>Deconstruct into parts <code>{ base_number, block_descriptions, rx }</code></li>
<li>Starting from the beginning of <code>block_descriptions</code>:
<ol>
<li>Check the <code>ActiveDisputes</code> for a dispute of each candidate in the block description.</li>
<li>If there is a dispute, exit the loop.</li>
</ol>
</li>
<li>For the highest index <code>i</code> reached in the <code>block_descriptions</code>, send <code>(base_number + i + 1, block_hash)</code> on the channel, unless <code>i</code> is 0, in which case <code>None</code> should be sent. The <code>block_hash</code> is determined by inspecting <code>block_descriptions[i]</code>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dispute-participation-2"><a class="header" href="#dispute-participation-2">Dispute Participation</a></h1>
<p>This subsystem is responsible for actually participating in disputes: when notified of a dispute, we need to recover the candidate data, validate the candidate, and cast our vote in the dispute.</p>
<p>Fortunately, most of that work is handled by other subsystems; this subsystem is just a small glue component for tying other subsystems together and issuing statements based on their validity.</p>
<h2 id="protocol-11"><a class="header" href="#protocol-11">Protocol</a></h2>
<p>Input: <a href="node/disputes/../../types/overseer-protocol.html#dispute-participation-message">DisputeParticipationMessage</a></p>
<p>Output:</p>
<ul>
<li><a href="node/disputes/../../types/overseer-protocol.html#runtime-api-message">RuntimeApiMessage</a></li>
<li><a href="node/disputes/../../types/overseer-protocol.html#candidate-validation-message">CandidateValidationMessage</a></li>
<li><a href="node/disputes/../../types/overseer-protocol.html#availability-recovery-message">AvailabilityRecoveryMessage</a></li>
<li><a href="node/disputes/../../types/overseer-protocol.html#chain-api-message">ChainApiMessage</a></li>
</ul>
<h2 id="functionality-11"><a class="header" href="#functionality-11">Functionality</a></h2>
<p>In-memory state:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct State {
    recent_block_hash: Option&lt;(BlockNumber, Hash)&gt;
}
<span class="boring">}
</span></code></pre></pre>
<h3 id="on-overseersignalactiveleavesupdate-1"><a class="header" href="#on-overseersignalactiveleavesupdate-1">On <code>OverseerSignal::ActiveLeavesUpdate</code></a></h3>
<p>Update <code>recent_block</code> in in-memory state according to the highest observed active leaf.</p>
<h3 id="on-overseersignalblockfinalized-1"><a class="header" href="#on-overseersignalblockfinalized-1">On <code>OverseerSignal::BlockFinalized</code></a></h3>
<p>Do nothing.</p>
<h3 id="on-overseersignalconclude-1"><a class="header" href="#on-overseersignalconclude-1">On <code>OverseerSignal::Conclude</code></a></h3>
<p>Conclude.</p>
<h3 id="on-disputeparticipationmessageparticipate"><a class="header" href="#on-disputeparticipationmessageparticipate">On <code>DisputeParticipationMessage::Participate</code></a></h3>
<ul>
<li>Decompose into parts: <code>{ candidate_hash, candidate_receipt, session, voted_indices }</code></li>
<li>Issue an <a href="node/disputes/../../types/overseer-protocol.html#availability-recovery-message"><code>AvailabilityRecoveryMessage::RecoverAvailableData</code></a></li>
<li>If the result is <code>Unavailable</code>, return.</li>
<li>If the result is <code>Invalid</code>, <a href="node/disputes/dispute-participation.html#cast-votes">cast invalid votes</a> and return.</li>
<li>If the data is recovered, dispatch a <a href="node/disputes/../../types/overseer-protocol.html#runtime-api-message"><code>RuntimeApiMessage::ValidationCodeByHash</code></a> with the parameters <code>(candidate_receipt.descriptor.validation_code_hash)</code> at <code>state.recent_block.hash</code>.</li>
<li>Dispatch a <a href="node/disputes/../../types/overseer-protocol.html#availability-store-message"><code>AvailabilityStoreMessage::StoreAvailableData</code></a> with the data.</li>
<li>If the code is not fetched from the chain, return. This should be impossible with correct relay chain configuration, at least if chain synchronization is working correctly.</li>
<li>Dispatch a <a href="node/disputes/../../types/overseer-protocol.html#candidate-validation-message"><code>CandidateValidationMessage::ValidateFromExhaustive</code></a> with the available data and the validation code.</li>
<li>If the validation result is <code>Invalid</code>, <a href="node/disputes/dispute-participation.html#cast-votes">cast invalid votes</a> and return.</li>
<li>If the validation fails, <a href="node/disputes/dispute-participation.html#cast-votes">cast invalid votes</a> and return.</li>
<li>If the validation succeeds, compute the <code>CandidateCommitments</code> based on the validation result and compare against the candidate receipt's <code>commitments_hash</code>. If they match, <a href="node/disputes/dispute-participation.html#cast-votes">cast valid votes</a> and if not, <a href="node/disputes/dispute-participation.html#cast-votes">cast invalid votes</a>.</li>
</ul>
<h3 id="cast-votes"><a class="header" href="#cast-votes">Cast Votes</a></h3>
<p>This requires the parameters <code>{ candidate_receipt, candidate_hash, session, voted_indices }</code> as well as a choice of either <code>Valid</code> or <code>Invalid</code>.</p>
<p>Invoke <a href="node/disputes/../../types/overseer-protocol.html#dispute-coordinator-message"><code>DisputeCoordinatorMessage::IssueLocalStatement</code></a> with <code>is_valid</code> according to the parametrization.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dispute-distribution-1"><a class="header" href="#dispute-distribution-1">Dispute Distribution</a></h1>
<p>Dispute distribution is responsible for ensuring all concerned validators will be aware of a dispute and have the relevant votes.</p>
<h2 id="design-goals-1"><a class="header" href="#design-goals-1">Design Goals</a></h2>
<p>This design should result in a protocol that is:</p>
<ul>
<li>resilient to nodes being temporarily unavailable</li>
<li>make sure nodes are aware of a dispute quickly</li>
<li>relatively efficient, should not cause too much stress on the network</li>
<li>be resilient when it comes to spam</li>
<li>be simple and boring: We want disputes to work when they happen</li>
</ul>
<h2 id="protocol-12"><a class="header" href="#protocol-12">Protocol</a></h2>
<h3 id="input"><a class="header" href="#input">Input</a></h3>
<p>[<code>DisputeDistributionMessage</code>][DisputeDistributionMessage]</p>
<h3 id="output"><a class="header" href="#output">Output</a></h3>
<ul>
<li><a href="node/disputes/../../types/overseer-protocol.html#dispute-participation-message"><code>DisputeCoordinatorMessage::ActiveDisputes</code></a></li>
<li><a href="node/disputes/../../types/overseer-protocol.html#dispute-participation-message"><code>DisputeCoordinatorMessage::ImportStatements</code></a></li>
<li><a href="node/disputes/../../types/overseer-protocol.html#dispute-participation-message"><code>DisputeCoordinatorMessage::QueryCandidateVotes</code></a></li>
<li><a href="node/disputes/../../types/overseer-protocol.html#runtime-api-message"><code>RuntimeApiMessage</code></a></li>
</ul>
<h3 id="wire-format"><a class="header" href="#wire-format">Wire format</a></h3>
<h4 id="disputes-1"><a class="header" href="#disputes-1">Disputes</a></h4>
<p>Protocol: &quot;/polkadot/send_dispute/1&quot;</p>
<p>Request:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct DisputeRequest {
  // Either initiating invalid vote or our own (if we voted invalid).
  invalid_vote: InvalidVote,
  // Some invalid vote (can be from backing/approval) or our own if we voted
  // valid.
  valid_vote: ValidVote,
}

struct InvalidVote {
  subject: VoteSubject,
  kind: InvalidDisputeStatementKind,
}

struct ValidVote {
  subject: VoteSubject,
  kind: ValidDisputeStatementKind,
}

struct VoteSubject {
  /// The candidate being disputed.
  candidate_hash: CandidateHash,
  /// The voting validator.
  validator_index: ValidatorIndex,
  /// The session the candidate appears in.
  candidate_session: SessionIndex,
  /// The validator signature, that can be verified when constructing a
  /// `SignedDisputeStatement`.
  validator_signature: ValidatorSignature,
}
<span class="boring">}
</span></code></pre></pre>
<p>Response:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum DisputeResponse {
  Confirmed
}
<span class="boring">}
</span></code></pre></pre>
<h4 id="vote-recovery"><a class="header" href="#vote-recovery">Vote Recovery</a></h4>
<p>Protocol: &quot;/polkadot/req_votes/1&quot;</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct IHaveVotesRequest {
  candidate_hash: CandidateHash,
  session: SessionIndex,
  valid_votes: Bitfield,
  invalid_votes: Bitfield,
}

<span class="boring">}
</span></code></pre></pre>
<p>Response:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct VotesResponse {
  /// All votes we have, but the requester was missing.
  missing: Vec&lt;(DisputeStatement, ValidatorIndex, ValidatorSignature)&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="functionality-12"><a class="header" href="#functionality-12">Functionality</a></h2>
<p>Distributing disputes needs to be a reliable protocol. We would like to make as
sure as possible that our vote got properly delivered to all concerned
validators. For this to work, this subsystem won't be gossip based, but instead
will use a request/response protocol for application level confirmations. The
request will be the payload (the actual votes/statements), the response will
be the confirmation. See [above][#wire-format].</p>
<h3 id="starting-a-dispute"><a class="header" href="#starting-a-dispute">Starting a Dispute</a></h3>
<p>A dispute is initiated once a node sends the first <code>DisputeRequest</code> wire message,
which must contain an &quot;invalid&quot; vote and a &quot;valid&quot; vote.</p>
<p>The dispute distribution subsystem can get instructed to send that message out to
all concerned validators by means of a <code>DisputeDistributionMessage::SendDispute</code>
message. That message must contain an invalid vote from the local node and some
valid one, e.g. a backing statement.</p>
<p>We include a valid vote as well, so any node regardless of whether it is synced
with the chain or not or has seen backing/approval vote can see that there are
conflicting votes available, hence we have a valid dispute. Nodes will still
need to check whether the disputing votes are somewhat current and not some
stale ones.</p>
<h3 id="participating-in-a-dispute"><a class="header" href="#participating-in-a-dispute">Participating in a Dispute</a></h3>
<p>Upon receiving a <code>DisputeRequest</code> message, a dispute distribution will trigger the
import of the received votes via the dispute coordinator
(<code>DisputeCoordinatorMessage::ImportStatements</code>). The dispute coordinator will
take care of participating in that dispute if necessary. Once it is done, the
coordinator will send a <code>DisputeDistributionMessage::SendDispute</code> message to dispute
distribution. From here, everything is the same as for starting a dispute,
except that if the local node deemed the candidate valid, the <code>SendDispute</code>
message will contain a valid vote signed by our node and will contain the
initially received <code>Invalid</code> vote.</p>
<p>Note, that we rely on the coordinator to check availability for spam protection
(see below).
In case the current node is only a potential block producer and does not
actually need to recover availability (as it is not going to participate in the
dispute), there is a potential optimization available: The coordinator could
first just check whether we have our piece and only if we don't, try to recover
availability. Our node having a piece would be proof enough of the
data to be available and thus the dispute to not be spam.</p>
<h3 id="sending-of-messages"><a class="header" href="#sending-of-messages">Sending of messages</a></h3>
<p>Starting and participating in a dispute are pretty similar from the perspective
of disptute distribution. Once we receive a <code>SendDispute</code> message we try to make
sure to get the data out. We keep track of all the parachain validators that
should see the message, which are all the parachain validators of the session
where the dispute happened as they will want to participate in the dispute.  In
addition we also need to get the votes out to all authorities of the current
session (which might be the same or not and may change during the dispute).
Those authorities will not participate in the dispute, but need to see the
statements so they can include them in blocks.</p>
<p>We keep track of connected parachain validators and authorities and will issue
warnings in the logs if connected nodes are less than two thirds of the
corresponding sets. We also only consider a message transmitted, once we
received a confirmation message. If not, we will keep retrying getting that
message out as long as the dispute is deemed alive. To determine whether a
dispute is still alive we will issue a
<code>DisputeCoordinatorMessage::ActiveDisputes</code> message before each retry run. Once
a dispute is no longer live, we will clean up the state accordingly.</p>
<h3 id="reception--spam-considerations"><a class="header" href="#reception--spam-considerations">Reception &amp; Spam Considerations</a></h3>
<p>Because we are not forwarding foreign statements, spam is not so much of an
issue as in other subsystems. Rate limiting should be implemented at the
substrate level, see
<a href="https://github.com/paritytech/substrate/issues/7750">#7750</a>. Still we should
make sure that it is not possible via spamming to prevent a dispute concluding
or worse from getting noticed.</p>
<p>Considered attack vectors:</p>
<ol>
<li>Invalid disputes (candidate does not exist) could make us
run out of resources. E.g. if we recorded every statement, we could run out
of disk space eventually.</li>
<li>An attacker can just flood us with notifications on any notification
protocol, assuming flood protection is not effective enough, our unbounded
buffers can fill up and we will run out of memory eventually.</li>
<li>Attackers could spam us at a high rate with invalid disputes. Our incoming
queue of requests could get monopolized by those malicious requests and we
won't be able to import any valid disputes and we could run out of resources,
if we tried to process them all in parallel.</li>
</ol>
<p>For tackling 1, we make sure to not occupy resources before we don't know a
candidate is available. So we will not record statements to disk until we
recovered availability for the candidate or know by some other means that the
dispute is legit.</p>
<p>For 2, we will pick up on any dispute on restart, so assuming that any realistic
memory filling attack will take some time, we should be able to participate in a
dispute under such attacks.</p>
<p>For 3, full monopolization of the incoming queue should not be possible assuming
substrate handles incoming requests in a somewhat fair way. Still we want some
defense mechanisms, at the very least we need to make sure to not exhaust
resources.</p>
<p>The dispute coordinator will notify us
via <code>DisputeDistributionMessage::ReportCandidateUnavailable</code> about unavailable
candidates and we can disconnect from such peers/decrease their reputation
drastically. This alone should get us quite far with regards to queue
monopolization, as availability recovery is expected to fail relatively quickly
for unavailable data.</p>
<p>Still if those spam messages come at a very high rate, we might still run out of
resources if we immediately call <code>DisputeCoordinatorMessage::ImportStatements</code>
on each one of them. Secondly with our assumption of 1/3 dishonest validators,
getting rid of all of them will take some time, depending on reputation timeouts
some of them might even be able to reconnect eventually.</p>
<p>To mitigate those issues we will process dispute messages with a maximum
parallelism <code>N</code>. We initiate import processes for up to <code>N</code> candidates in
parallel. Once we reached <code>N</code> parallel requests we will start back pressuring on
the incoming requests. This saves us from resource exhaustion.</p>
<p>To reduce impact of malicious nodes further, we can keep track from which nodes the
currently importing statements came from and will drop requests from nodes that
already have imports in flight.</p>
<p>Honest nodes are not expected to send dispute statements at a high rate, but
even if they did:</p>
<ul>
<li>we will import at least the first one and if it is valid it will trigger a
dispute, preventing finality.</li>
<li>Chances are good that the first sent candidate from a peer is indeed the
oldest one (if they differ in age at all).</li>
<li>for the dropped request any honest node will retry sending.</li>
<li>there will be other nodes notifying us about that dispute as well.</li>
<li>honest votes have a speed advantage on average. Apart from the very first
dispute statement for a candidate, which might cause the availability recovery
process, imports of honest votes will be super fast, while for spam imports
they will always take some time as we have to wait for availability to fail.</li>
</ul>
<p>So this general rate limit, that we drop requests from same peers if they come
faster than we can import the statements should not cause any problems for
honest nodes and is in their favour.</p>
<p>Size of <code>N</code>: The larger <code>N</code> the better we can handle distributed flood attacks
(see previous paragraph), but we also get potentially more availability recovery
processes happening at the same time, which slows down the individual processes.
And we rather want to have one finish quickly than lots slowly at the same time.
On the other hand, valid disputes are expected to be rare, so if we ever exhaust
<code>N</code> it is very likely that this is caused by spam and spam recoveries don't cost
too much bandwidth due to empty responses.</p>
<p>Considering that an attacker would need to attack many nodes in parallel to have
any effect, an <code>N</code> of 10 seems to be a good compromise. For honest requests, most
of those imports will likely concern the same candidate, and for dishonest ones
we get to disconnect from up to ten colluding adversaries at a time.</p>
<p>For the size of the channel for incoming requests: Due to dropping of repeated
requests from same nodes we can make the channel relatively large without fear
of lots of spam requests sitting there wasting our time, even after we already
blocked a peer. For valid disputes, incoming requests can become bursty. On the
other hand we will also be very quick in processing them. A channel size of 100
requests seems plenty and should be able to handle bursts adequately.</p>
<h3 id="node-startup"><a class="header" href="#node-startup">Node Startup</a></h3>
<p>On startup we need to check with the dispute coordinator for any ongoing
disputes and assume we have not yet sent our statement for those. In case we
find an explicit statement from ourselves via
<code>DisputeCoordinatorMessage::QueryCandidateVotes</code> we will pretend to just have
received a <code>SendDispute</code> message for that candidate.</p>
<h2 id="backing-and-approval-votes"><a class="header" href="#backing-and-approval-votes">Backing and Approval Votes</a></h2>
<p>Backing and approval votes get imported when they arrive/are created via the
distpute coordinator by corresponding subsystems.</p>
<p>We assume that under normal operation each node will be aware of backing and
approval votes and optimize for that case. Nevertheless we want disputes to
conclude fast and reliable, therefore if a node is not aware of backing/approval
votes it can request the missing votes from the node that informed it about the
dispute (see <a href="node/disputes/dispute-distribution.html#Resiliency%5D">Resiliency</a></p>
<h2 id="resiliency"><a class="header" href="#resiliency">Resiliency</a></h2>
<p>The above protocol should be sufficient for most cases, but there are certain
cases we also want to have covered:</p>
<ul>
<li>Non validator nodes might be interested in ongoing voting, even before it is
recorded on chain.</li>
<li>Nodes might have missed votes, especially backing or approval votes.
Recovering them from chain is difficult and expensive, due to runtime upgrades
and untyped extrinsics.</li>
</ul>
<p>To cover those cases, we introduce a second request/response protocol, which can
be handled on a lower priority basis as the one above. It consists of the
request/response messages as described in the [protocol
section][#vote-recovery].</p>
<p>Nodes may send those requests to validators, if they feel they are missing
votes. E.g. after some timeout, if no majority was reached yet in their point of
view or if they are not aware of any backing/approval votes for a received
disputed candidate.</p>
<p>The receiver of a <code>IHaveVotesRequest</code> message will do the following:</p>
<ol>
<li>See if the sender is missing votes we are aware of - if so, respond with
those votes.</li>
<li>Check whether the sender knows about any votes, we don't know about and if so
send a <code>IHaveVotesRequest</code> request back, with our knowledge.</li>
<li>Record the peer's knowledge.</li>
</ol>
<p>When to send <code>IHaveVotesRequest</code> messages:</p>
<ol>
<li>Whenever we are asked to do so via
<code>DisputeDistributionMessage::FetchMissingVotes</code>.</li>
<li>Approximately once per block to some random validator as long as the dispute
is active.</li>
</ol>
<p>Spam considerations: Nodes want to accept those messages once per validator and
per slot. They are free to drop more frequent requests or requests for stale
data. Requests coming from non validator nodes, can be handled on a best effort
basis.</p>
<h2 id="considerations"><a class="header" href="#considerations">Considerations</a></h2>
<p>Dispute distribution is critical. We should keep track of available validator
connections and issue warnings if we are not connected to a majority of
validators. We should also keep track of failed sending attempts and log
warnings accordingly. As disputes are rare and TCP is a reliable protocol,
probably each failed attempt should trigger a warning in logs and also logged
into some Prometheus metric.</p>
<h2 id="disputes-for-non-available-candidates"><a class="header" href="#disputes-for-non-available-candidates">Disputes for non available candidates</a></h2>
<p>If deemed necessary we can later on also support disputes for non available
candidates, but disputes for those cases have totally different requirements.</p>
<p>First of all such disputes are not time critical. We just want to have
some offender slashed at some point, but we have no risk of finalizing any bad
data.</p>
<p>Second, as we won't have availability for such data, the node that initiated the
dispute will be responsible for providing the disputed data initially. Then
nodes which did the check already are also providers of the data, hence
distributing load and making prevention of the dispute from concluding harder
and harder over time. Assuming an attacker can not DoS a node forever, the
dispute will succeed eventually, which is all that matters. And again, even if
an attacker managed to prevent such a dispute from happening somehow, there is
no real harm done: There was no serious attack to begin with.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="utility-subsystems"><a class="header" href="#utility-subsystems">Utility Subsystems</a></h1>
<p>The utility subsystems are an assortment which don't have a natural home in another subsystem collection.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="availability-store"><a class="header" href="#availability-store">Availability Store</a></h1>
<p>This is a utility subsystem responsible for keeping available certain data and pruning that data.</p>
<p>The two data types:</p>
<ul>
<li>Full PoV blocks of candidates we have validated</li>
<li>Availability chunks of candidates that were backed and noted available on-chain.</li>
</ul>
<p>For each of these data we have pruning rules that determine how long we need to keep that data available.</p>
<p>PoV hypothetically only need to be kept around until the block where the data was made fully available is finalized. However, disputes can revert finality, so we need to be a bit more conservative and we add a delay. We should keep the PoV until a block that finalized availability of it has been finalized for 1 day + 1 hour.</p>
<p>Availability chunks need to be kept available until the dispute period for the corresponding candidate has ended. We can accomplish this by using the same criterion as the above. This gives us a pruning condition of the block finalizing availability of the chunk being final for 1 day + 1 hour.</p>
<p>There is also the case where a validator commits to make a PoV available, but the corresponding candidate is never backed. In this case, we keep the PoV available for 1 hour.</p>
<p>There may be multiple competing blocks all ending the availability phase for a particular candidate. Until finality, it will be unclear which of those is actually the canonical chain, so the pruning records for PoVs and Availability chunks should keep track of all such blocks.</p>
<h2 id="lifetime-of-the-block-data-and-chunks-in-storage"><a class="header" href="#lifetime-of-the-block-data-and-chunks-in-storage">Lifetime of the block data and chunks in storage</a></h2>
<p><img src="node/utility/availability_store_0.generated.svg" alt="" /></p>
<h2 id="database-schema-2"><a class="header" href="#database-schema-2">Database Schema</a></h2>
<p>We use an underlying Key-Value database where we assume we have the following operations available:</p>
<ul>
<li><code>write(key, value)</code></li>
<li><code>read(key) -&gt; Option&lt;value&gt;</code></li>
<li><code>iter_with_prefix(prefix) -&gt; Iterator&lt;(key, value)&gt;</code> - gives all keys and values in lexicographical order where the key starts with <code>prefix</code>.</li>
</ul>
<p>We use this database to encode the following schema:</p>
<pre><code>(&quot;available&quot;, CandidateHash) -&gt; Option&lt;AvailableData&gt;
(&quot;chunk&quot;, CandidateHash, u32) -&gt; Option&lt;ErasureChunk&gt;
(&quot;meta&quot;, CandidateHash) -&gt; Option&lt;CandidateMeta&gt;

(&quot;unfinalized&quot;, BlockNumber, BlockHash, CandidateHash) -&gt; Option&lt;()&gt;
(&quot;prune_by_time&quot;, Timestamp, CandidateHash) -&gt; Option&lt;()&gt;
</code></pre>
<p>Timestamps are the wall-clock seconds since unix epoch. Timestamps and block numbers are both encoded as big-endian so lexicographic order is ascending.</p>
<p>The meta information that we track per-candidate is defined as the <code>CandidateMeta</code> struct</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct CandidateMeta {
  state: State,
  data_available: bool,
  chunks_stored: Bitfield,
}

enum State {
  /// Candidate data was first observed at the given time but is not available in any block.
  Unavailable(Timestamp),
  /// The candidate was first observed at the given time and was included in the given list of unfinalized blocks, which may be
  /// empty. The timestamp here is not used for pruning. Either one of these blocks will be finalized or the state will regress to
  /// `State::Unavailable`, in which case the same timestamp will be reused.
  Unfinalized(Timestamp, Vec&lt;(BlockNumber, BlockHash)&gt;),
  /// Candidate data has appeared in a finalized block and did so at the given time.
  Finalized(Timestamp)
}
<span class="boring">}
</span></code></pre></pre>
<p>We maintain the invariant that if a candidate has a meta entry, its available data exists on disk if <code>data_available</code> is true. All chunks mentioned in the meta entry are available.</p>
<p>Additionally, there is exactly one <code>prune_by_time</code> entry which holds the candidate hash unless the state is <code>Unfinalized</code>. There may be zero, one, or many &quot;unfinalized&quot; keys with the given candidate, and this will correspond to the <code>state</code> of the meta entry.</p>
<h2 id="protocol-13"><a class="header" href="#protocol-13">Protocol</a></h2>
<p>Input: <a href="node/utility/../../types/overseer-protocol.html#availability-store-message"><code>AvailabilityStoreMessage</code></a></p>
<p>Output:</p>
<ul>
<li><a href="node/utility/../../types/overseer-protocol.html#runtime-api-message"><code>RuntimeApiMessage</code></a></li>
</ul>
<h2 id="functionality-13"><a class="header" href="#functionality-13">Functionality</a></h2>
<p>For each head in the <code>activated</code> list:</p>
<ul>
<li>Load all ancestors of the head back to the finalized block so we don't miss anything if import notifications are missed. If a <code>StoreChunk</code> message is received for a candidate which has no entry, then we will prematurely lose the data.</li>
<li>Note any new candidates backed in the head. Update the <code>CandidateMeta</code> for each. If the <code>CandidateMeta</code> does not exist, create it as <code>Unavailable</code> with the current timestamp. Register a <code>&quot;prune_by_time&quot;</code> entry based on the current timestamp + 1 hour.</li>
<li>Note any new candidate included in the head. Update the <code>CandidateMeta</code> for each, performing a transition from <code>Unavailable</code> to <code>Unfinalized</code> if necessary. That includes removing the <code>&quot;prune_by_time&quot;</code> entry. Add the head hash and number to the state, if unfinalized. Add an <code>&quot;unfinalized&quot;</code> entry for the block and candidate.</li>
<li>The <code>CandidateEvent</code> runtime API can be used for this purpose.</li>
</ul>
<p>On <code>OverseerSignal::BlockFinalized(finalized)</code> events:</p>
<ul>
<li>for each key in <code>iter_with_prefix(&quot;unfinalized&quot;)</code>
<ul>
<li>Stop if the key is beyond <code>(&quot;unfinalized, finalized)</code></li>
<li>For each block number f that we encounter, load the finalized hash for that block.
<ul>
<li>The state of each <code>CandidateMeta</code> we encounter here must be <code>Unfinalized</code>, since we loaded the candidate from an <code>&quot;unfinalized&quot;</code> key.</li>
<li>For each candidate that we encounter under <code>f</code> and the finalized block hash,
<ul>
<li>Update the <code>CandidateMeta</code> to have <code>State::Finalized</code>.  Remove all <code>&quot;unfinalized&quot;</code> entries from the old <code>Unfinalized</code> state.</li>
<li>Register a <code>&quot;prune_by_time&quot;</code> entry for the candidate based on the current time + 1 day + 1 hour.</li>
</ul>
</li>
<li>For each candidate that we encounter under <code>f</code> which is not under the finalized block hash,
<ul>
<li>Remove all entries under <code>f</code> in the <code>Unfinalized</code> state.</li>
<li>If the <code>CandidateMeta</code> has state <code>Unfinalized</code> with an empty list of blocks, downgrade to <code>Unavailable</code> and re-schedule pruning under the timestamp + 1 hour. We do not prune here as the candidate still may be included in a descendent of the finalized chain.</li>
</ul>
</li>
<li>Remove all <code>&quot;unfinalized&quot;</code> keys under <code>f</code>.</li>
</ul>
</li>
</ul>
</li>
<li>Update last_finalized = finalized.</li>
</ul>
<p>This is roughly <code>O(n * m)</code> where n is the number of blocks finalized since the last update, and <code>m</code> is the number of parachains.</p>
<p>On <code>QueryAvailableData</code> message:</p>
<ul>
<li>Query <code>(&quot;available&quot;, candidate_hash)</code></li>
</ul>
<p>This is <code>O(n)</code> in the size of the data, which may be large.</p>
<p>On <code>QueryDataAvailability</code> message:</p>
<ul>
<li>Query whether <code>(&quot;meta&quot;, candidate_hash)</code> exists and <code>data_available == true</code>.</li>
</ul>
<p>This is <code>O(n)</code> in the size of the metadata which is small.</p>
<p>On <code>QueryChunk</code> message:</p>
<ul>
<li>Query <code>(&quot;chunk&quot;, candidate_hash, index)</code></li>
</ul>
<p>This is <code>O(n)</code> in the size of the data, which may be large.</p>
<p>On <code>QueryAllChunks</code> message:</p>
<ul>
<li>Query <code>(&quot;meta&quot;, candidate_hash)</code>. If <code>None</code>, send an empty response and return.</li>
<li>For all <code>1</code> bits in the <code>chunks_stored</code>, query <code>(&quot;chunk&quot;, candidate_hash, index)</code>. Ignore but warn on errors, and return a vector of all loaded chunks.</li>
</ul>
<p>On `QueryChunkAvailability message:</p>
<ul>
<li>Query whether <code>(&quot;meta&quot;, candidate_hash)</code> exists and the bit at <code>index</code> is set.</li>
</ul>
<p>This is <code>O(n)</code> in the size of the metadata which is small.</p>
<p>On <code>StoreChunk</code> message:</p>
<ul>
<li>If there is a <code>CandidateMeta</code> under the candidate hash, set the bit of the erasure-chunk in the <code>chunks_stored</code> bitfield to <code>1</code>. If it was not <code>1</code> already, write the chunk under <code>(&quot;chunk&quot;, candidate_hash, chunk_index)</code>.</li>
</ul>
<p>This is <code>O(n)</code> in the size of the chunk.</p>
<p>On <code>StoreAvailableData</code> message:</p>
<ul>
<li>If there is no <code>CandidateMeta</code> under the candidate hash, create it with <code>State::Unavailable(now)</code>. Load the <code>CandidateMeta</code> otherwise.</li>
<li>Store <code>data</code> under <code>(&quot;available&quot;, candidate_hash)</code> and set <code>data_available</code> to true.</li>
<li>Store each chunk under <code>(&quot;chunk&quot;, candidate_hash, index)</code> and set every bit in <code>chunks_stored</code> to <code>1</code>.</li>
</ul>
<p>This is <code>O(n)</code> in the size of the data as the aggregate size of the chunks is proportional to the data.</p>
<p>Every 5 minutes, run a pruning routine:</p>
<ul>
<li>for each key in <code>iter_with_prefix(&quot;prune_by_time&quot;)</code>:
<ul>
<li>If the key is beyond (&quot;prune_by_time&quot;, now), return.</li>
<li>Remove the key.</li>
<li>Extract <code>candidate_hash</code> from the key.</li>
<li>Load and remove the <code>(&quot;meta&quot;, candidate_hash)</code></li>
<li>For each erasure chunk bit set, remove <code>(&quot;chunk&quot;, candidate_hash, bit_index)</code>.</li>
<li>If <code>data_available</code>, remove `(&quot;available&quot;, candidate_hash)</li>
</ul>
</li>
</ul>
<p>This is O(n * m) in the amount of candidates and average size of the data stored. This is probably the most expensive operation but does not need
to be run very often.</p>
<h2 id="basic-scenarios-to-test"><a class="header" href="#basic-scenarios-to-test">Basic scenarios to test</a></h2>
<p>Basically we need to test the correctness of data flow through state FSMs described earlier. These tests obviously assume that some mocking of time is happening.</p>
<ul>
<li>
<p>Stored data that is never included pruned in necessary timeout</p>
<ul>
<li>A block (and/or a chunk) is added to the store.</li>
<li>We never note that the respective candidate is included.</li>
<li>Until a defined timeout the data in question is available.</li>
<li>After this timeout the data is no longer available.</li>
</ul>
</li>
<li>
<p>Stored data is kept until we are certain it is finalized.</p>
<ul>
<li>A block (and/or a chunk) is added to the store.</li>
<li>It is available.</li>
<li>Before the inclusion timeout expires notify storage that the candidate was included.</li>
<li>The data is still available.</li>
<li>Wait for an absurd amount of time (longer than 1 day).</li>
<li>Check that the data is still available.</li>
<li>Send finality notification about the block in question.</li>
<li>Wait for some time below finalized data timeout.</li>
<li>The data is still available.</li>
<li>Wait until the data should have been pruned.</li>
<li>The data is no longer available.</li>
</ul>
</li>
<li>
<p>Forkfulness of the relay chain is taken into account</p>
<ul>
<li>Block <code>B1</code> is added to the store.</li>
<li>Block <code>B2</code> is added to the store.</li>
<li>Notify the subsystem that both <code>B1</code> and <code>B2</code> were included in different leafs of relay chain.</li>
<li>Notify the subsystem that the leaf with <code>B1</code> was finalized.</li>
<li>Leaf with <code>B2</code> is never finalized.</li>
<li>Leaf with <code>B2</code> is pruned and its data is no longer available.</li>
<li>Wait until the finalized data of <code>B1</code> should have been pruned.</li>
<li><code>B1</code> is no longer available.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="candidate-validation"><a class="header" href="#candidate-validation">Candidate Validation</a></h1>
<p>This subsystem is responsible for handling candidate validation requests. It is a simple request/response server.</p>
<p>A variety of subsystems want to know if a parachain block candidate is valid. None of them care about the detailed mechanics of how a candidate gets validated, just the results. This subsystem handles those details.</p>
<h2 id="protocol-14"><a class="header" href="#protocol-14">Protocol</a></h2>
<p>Input: <a href="node/utility/../../types/overseer-protocol.html#validation-request-type"><code>CandidateValidationMessage</code></a></p>
<p>Output: Validation result via the provided response side-channel.</p>
<h2 id="functionality-14"><a class="header" href="#functionality-14">Functionality</a></h2>
<p>This subsystem answers two types of requests: one which draws out validation data from the state, and another which accepts all validation data exhaustively. The goal of both request types is to validate a candidate. There are three possible outputs of validation: either the candidate is valid, the candidate is invalid, or an internal error occurred. Whatever the end result is, it will be returned on the response channel to the requestor.</p>
<p>Parachain candidates are validated against their validation function: A piece of Wasm code that is describes the state-transition of the parachain. Validation function execution is not metered. This means that an execution which is an infinite loop or simply takes too long must be forcibly exited by some other means. For this reason, we recommend dispatching candidate validation to be done on subprocesses which can be killed if they time-out.</p>
<p>Upon receiving a validation request, the first thing the candidate validation subsystem should do is make sure it has all the necessary parameters to the validation function. These are:</p>
<ul>
<li>The Validation Function itself.</li>
<li>The <a href="node/utility/../../types/candidate.html#candidatedescriptor"><code>CandidateDescriptor</code></a>.</li>
<li>The <a href="node/utility/../../types/candidate.html#validationdata"><code>ValidationData</code></a>.</li>
<li>The <a href="node/utility/../../types/availability.html#proofofvalidity"><code>PoV</code></a>.</li>
</ul>
<h3 id="determining-parameters"><a class="header" href="#determining-parameters">Determining Parameters</a></h3>
<p>For a <a href="node/utility/../../types/overseer-protocol.html#validationrequesttype"><code>CandidateValidationMessage</code></a><code>::ValidateFromExhaustive</code>, these parameters are exhaustively provided.</p>
<p>For a <a href="node/utility/../../types/overseer-protocol.html#validationrequesttype"><code>CandidateValidationMessage</code></a><code>::ValidateFromChainState</code>, some more work needs to be done. Due to the uncertainty of Availability Cores (implemented in the <a href="node/utility/../../runtime/scheduler.html"><code>Scheduler</code></a> module of the runtime), a candidate at a particular relay-parent and for a particular para may have two different valid validation-data to be executed under depending on what is assumed to happen if the para is occupying a core at the onset of the new block. This is encoded as an <code>OccupiedCoreAssumption</code> in the runtime API.</p>
<p>The way that we can determine which assumption the candidate is meant to be executed under is simply to do an exhaustive check of both possibilities based on the state of the relay-parent. First we fetch the validation data under the assumption that the block occupying becomes available. If the <code>validation_data_hash</code> of the <code>CandidateDescriptor</code> matches this validation data, we use that. Otherwise, if the <code>validation_data_hash</code> matches the validation data fetched under the <code>TimedOut</code> assumption, we use that. Otherwise, we return a <code>ValidationResult::Invalid</code> response and conclude.</p>
<p>Then, we can fetch the validation code from the runtime based on which type of candidate this is. This gives us all the parameters. The descriptor and PoV come from the request itself, and the other parameters have been derived from the state.</p>
<blockquote>
<p>TODO: This would be a great place for caching to avoid making lots of runtime requests. That would need a job, though.</p>
</blockquote>
<h3 id="execution-of-the-parachain-wasm"><a class="header" href="#execution-of-the-parachain-wasm">Execution of the Parachain Wasm</a></h3>
<p>Once we have all parameters, we can spin up a background task to perform the validation in a way that doesn't hold up the entire event loop. Before invoking the validation function itself, this should first do some basic checks:</p>
<ul>
<li>The collator signature is valid</li>
<li>The PoV provided matches the <code>pov_hash</code> field of the descriptor</li>
</ul>
<h3 id="checking-validation-outputs"><a class="header" href="#checking-validation-outputs">Checking Validation Outputs</a></h3>
<p>If we can assume the presence of the relay-chain state (that is, during processing <a href="node/utility/../../types/overseer-protocol.html#validationrequesttype"><code>CandidateValidationMessage</code></a><code>::ValidateFromChainState</code>) we can run all the checks that the relay-chain would run at the inclusion time thus confirming that the candidate will be accepted.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="provisioner"><a class="header" href="#provisioner">Provisioner</a></h1>
<p>Relay chain block authorship authority is governed by BABE and is beyond the scope of the Overseer and the rest of the subsystems. That said, ultimately the block author needs to select a set of backable parachain candidates and other consensus data, and assemble a block from them. This subsystem is responsible for providing the necessary data to all potential block authors.</p>
<p>A major feature of the provisioner: this subsystem is responsible for ensuring that parachain block candidates are sufficiently available before sending them to potential block authors.</p>
<h2 id="provisionable-data"><a class="header" href="#provisionable-data">Provisionable Data</a></h2>
<p>There are several distinct types of provisionable data, but they share this property in common: all should eventually be included in a relay chain block.</p>
<h3 id="backed-candidates"><a class="header" href="#backed-candidates">Backed Candidates</a></h3>
<p>The block author can choose 0 or 1 backed parachain candidates per parachain; the only constraint is that each backed candidate has the appropriate relay parent. However, the choice of a backed candidate must be the block author's; the provisioner must ensure that block authors are aware of all available <a href="node/utility/../../types/backing.html#backed-candidate"><code>BackedCandidate</code>s</a>.</p>
<h3 id="signed-bitfields"><a class="header" href="#signed-bitfields">Signed Bitfields</a></h3>
<p><a href="node/utility/../../types/availability.html#signed-availability-bitfield">Signed bitfields</a> are attestations from a particular validator about which candidates it believes are available.</p>
<h3 id="misbehavior-reports"><a class="header" href="#misbehavior-reports">Misbehavior Reports</a></h3>
<p>Misbehavior reports are self-contained proofs of misbehavior by a validator or group of validators. For example, it is very easy to verify a double-voting misbehavior report: the report contains two votes signed by the same key, advocating different outcomes. Concretely, misbehavior reports become inherents which cause dots to be slashed.</p>
<p>Note that there is no mechanism in place which forces a block author to include a misbehavior report which it doesn't like, for example if it would be slashed by such a report. The chain's defense against this is to have a relatively long slash period, such that it's likely to encounter an honest author before the slash period expires.</p>
<h3 id="dispute-inherent"><a class="header" href="#dispute-inherent">Dispute Inherent</a></h3>
<p>The dispute inherent is similar to a misbehavior report in that it is an attestation of misbehavior on the part of a validator or group of validators. Unlike a misbehavior report, it is not self-contained: resolution requires coordinated action by several validators. The canonical example of a dispute inherent involves an approval checker discovering that a set of validators has improperly approved an invalid parachain block: resolving this requires the entire validator set to re-validate the block, so that the minority can be slashed.</p>
<p>Dispute resolution is complex and is explained in substantially more detail <a href="node/utility/../../runtime/disputes.html">here</a>.</p>
<blockquote>
<p>TODO: The provisioner is responsible for selecting remote disputes to replay. Let's figure out the details.</p>
</blockquote>
<h2 id="protocol-15"><a class="header" href="#protocol-15">Protocol</a></h2>
<p>Input: <a href="node/utility/../../types/overseer-protocol.html#provisioner-message"><code>ProvisionerMessage</code></a>. Backed candidates come from the <a href="node/utility/../backing/candidate-backing.html">Candidate Backing subsystem</a>, signed bitfields come from the <a href="node/utility/../availability/bitfield-distribution.html">Bitfield Distribution subsystem</a>, and misbehavior reports and disputes come from the <a href="node/utility/misbehavior-arbitration.html">Misbehavior Arbitration subsystem</a>.</p>
<p>At initialization, this subsystem has no outputs.</p>
<p>Block authors request the inherent data they should use for constructing the inherent in the block which contains parachain execution information.</p>
<h2 id="block-production"><a class="header" href="#block-production">Block Production</a></h2>
<p>When a validator is selected by BABE to author a block, it becomes a block producer. The provisioner is the subsystem best suited to choosing which specific backed candidates and availability bitfields should be assembled into the block. To engage this functionality, a <code>ProvisionerMessage::RequestInherentData</code> is sent; the response is a <a href="node/utility/../../types/runtime.html#parainherentdata"><code>ParaInherentData</code></a>. There are never two distinct parachain candidates included for the same parachain and that new parachain candidates cannot be backed until the previous one either gets declared available or expired. Appropriate bitfields, as outlined in the section on <a href="node/utility/provisioner.html#bitfield-selection">bitfield selection</a>, and any dispute statements should be attached as well.</p>
<h3 id="bitfield-selection"><a class="header" href="#bitfield-selection">Bitfield Selection</a></h3>
<p>Our goal with respect to bitfields is simple: maximize availability. However, it's not quite as simple as always including all bitfields; there are constraints which still need to be met:</p>
<ul>
<li>We cannot choose more than one bitfield per validator.</li>
<li>Each bitfield must correspond to an occupied core.</li>
</ul>
<p>Beyond that, a semi-arbitrary selection policy is fine. In order to meet the goal of maximizing availability, a heuristic of picking the bitfield with the greatest number of 1 bits set in the event of conflict is useful.</p>
<h3 id="candidate-selection"><a class="header" href="#candidate-selection">Candidate Selection</a></h3>
<p>The goal of candidate selection is to determine which cores are free, and then to the degree possible, pick a candidate appropriate to each free core.</p>
<p>To determine availability:</p>
<ul>
<li>Get the list of core states from the runtime API</li>
<li>For each core state:
<ul>
<li>On <code>CoreState::Scheduled</code>, then we can make an <code>OccupiedCoreAssumption::Free</code>.</li>
<li>On <code>CoreState::Occupied</code>, then we may be able to make an assumption:
<ul>
<li>If the bitfields indicate availability and there is a scheduled <code>next_up_on_available</code>, then we can make an <code>OccupiedCoreAssumption::Included</code>.</li>
<li>If the bitfields do not indicate availability, and there is a scheduled <code>next_up_on_time_out</code>, and <code>occupied_core.time_out_at == block_number_under_production</code>, then we can make an <code>OccupiedCoreAssumption::TimedOut</code>.</li>
</ul>
</li>
<li>If we did not make an <code>OccupiedCoreAssumption</code>, then continue on to the next core.</li>
<li>Now compute the core's <code>validation_data_hash</code>: get the <code>PersistedValidationData</code> from the runtime, given the known <code>ParaId</code> and <code>OccupiedCoreAssumption</code>;</li>
<li>Find an appropriate candidate for the core.
<ul>
<li>There are two constraints: <code>backed_candidate.candidate.descriptor.para_id == scheduled_core.para_id &amp;&amp; candidate.candidate.descriptor.validation_data_hash == computed_validation_data_hash</code>.</li>
<li>In the event that more than one candidate meets the constraints, selection between the candidates is arbitrary. However, not more than one candidate can be selected per core.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>The end result of this process is a vector of <code>BackedCandidate</code>s, sorted in order of their core index. Furthermore, this process should select at maximum one candidate which upgrades the runtime validation code.</p>
<h3 id="dispute-statement-selection"><a class="header" href="#dispute-statement-selection">Dispute Statement Selection</a></h3>
<p>This is the point at which the block author provides further votes to active disputes or initiates new disputes in the runtime state.</p>
<p>We must take care not to overwhelm the &quot;spam slots&quot; of the chain. That is, to avoid too many votes from the same validators being placed into the chain, which would trigger the anti-spam protection functionality of the <a href="node/utility/../../runtime/disputes.html">disputes module</a>.</p>
<p>To select disputes:</p>
<ul>
<li>Make a <code>DisputesInfo</code> runtime API call and decompose into <code>{ spam_slots, disputes }</code>. Bind <code>disputes</code> to <code>onchain_disputes</code>.</li>
<li>Issue a <code>DisputeCoordinatorMessage::ActiveDisputes</code> message and wait for the response. Assign the value to <code>offchain_disputes</code>.</li>
<li>Make a <code>CandidatesIncluded</code> runtime API call for each dispute in <code>offchain_disputes</code> and tag each offchain dispute as local if the result for it is <code>true</code>.</li>
<li>Initialize <code>NewSpamSlots: Map&lt;(SessionIndex, ValidatorIndex), u32&gt;</code> as an empty map.</li>
<li>For each dispute in <code>offchain_disputes</code>:
<ol>
<li>Make a <code>RuntimeApiRequest::SessionInfo</code> against the parent hash for the session of the dispute. If <code>None</code>, continue - this chain is in the past relative to the session the dispute belongs to and we can import it when it reaches that session.</li>
<li>Load the spam slots from <code>spam_slots</code> for the given session. If it isn't present, treat as though all zeros.</li>
<li>construct a <code>DisputeStatementSet</code> of all offchain votes we are aware of that the onchain doesn't already have a <code>valid</code> or <code>invalid</code> bit set for, respectively.</li>
<li>If the <code>onchain_disputes</code> contains an entry for the dispute, load that. Otherwise, treat as empty.</li>
<li>If the offchain dispute is local or the <code>DisputeStatementSet</code> and the onchain dispute together have at least <code>byzantine_threshold + 1</code> validators in it, continue to the next offchain dispute.</li>
<li>Otherwise</li>
<li>Filter out all votes from the <code>DisputeStatementSet</code> where the amount of spam slots occupied on-chain by the validator, plus the <code>NewSpamSlots</code> value, plus 1, is greater than <code>spam_slots.max_spam_slots</code>.</li>
<li>After filtering, if either the <code>valid</code> or <code>invalid</code> lists in the combination of the <code>DisputeStatementSet</code> and the onchain dispute is empty, skip this dispute.</li>
<li>Add 1 to the <code>NewSpamSlots</code> value for each validator in the <code>DisputeStatementSet</code>.</li>
</ol>
</li>
<li>Construct a <code>MultiDisputeStatementSet</code> for each <code>DisputeStatement</code> and return that.</li>
</ul>
<h3 id="determining-bitfield-availability"><a class="header" href="#determining-bitfield-availability">Determining Bitfield Availability</a></h3>
<p>An occupied core has a <code>CoreAvailability</code> bitfield. We also have a list of <code>SignedAvailabilityBitfield</code>s. We need to determine from these whether or not a core at a particular index has become available.</p>
<p>The key insight required is that <code>CoreAvailability</code> is transverse to the <code>SignedAvailabilityBitfield</code>s: if we conceptualize the list of bitfields as many rows, each bit of which is its own column, then <code>CoreAvailability</code> for a given core index is the vertical slice of bits in the set at that index.</p>
<p>To compute bitfield availability, then:</p>
<ul>
<li>Start with a copy of <code>OccupiedCore.availability</code></li>
<li>For each bitfield in the list of <code>SignedAvailabilityBitfield</code>s:
<ul>
<li>Get the bitfield's <code>validator_index</code></li>
<li>Update the availability. Conceptually, assuming bit vectors: <code>availability[validator_index] |= bitfield[core_idx]</code></li>
</ul>
</li>
<li>Availability has a 2/3 threshold. Therefore: <code>3 * availability.count_ones() &gt;= 2 * availability.len()</code></li>
</ul>
<h3 id="notes"><a class="header" href="#notes">Notes</a></h3>
<p>See also: <a href="node/utility/../../runtime/scheduler.html#availability-cores">Scheduler Module: Availability Cores</a>.</p>
<h2 id="functionality-15"><a class="header" href="#functionality-15">Functionality</a></h2>
<p>The subsystem should maintain a set of handles to Block Authorship Provisioning Jobs that are currently live.</p>
<h3 id="on-overseer-signal-1"><a class="header" href="#on-overseer-signal-1">On Overseer Signal</a></h3>
<ul>
<li><code>ActiveLeavesUpdate</code>:
<ul>
<li>For each <code>activated</code> head:
<ul>
<li>spawn a Block Authorship Provisioning Job with the given relay parent, storing a bidirectional channel with that job.</li>
</ul>
</li>
<li>For each <code>deactivated</code> head:
<ul>
<li>terminate the Block Authorship Provisioning Job for the given relay parent, if any.</li>
</ul>
</li>
</ul>
</li>
<li><code>Conclude</code>: Forward <code>Conclude</code> to all jobs, waiting a small amount of time for them to join, and then hard-exiting.</li>
</ul>
<h3 id="on-provisionermessage"><a class="header" href="#on-provisionermessage">On <code>ProvisionerMessage</code></a></h3>
<p>Forward the message to the appropriate Block Authorship Provisioning Job, or discard if no appropriate job is currently active.</p>
<h2 id="block-authorship-provisioning-job"><a class="header" href="#block-authorship-provisioning-job">Block Authorship Provisioning Job</a></h2>
<p>Maintain the set of channels to block authors. On receiving provisionable data, send a copy over each channel.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="network-bridge"><a class="header" href="#network-bridge">Network Bridge</a></h1>
<p>One of the main features of the overseer/subsystem duality is to avoid shared ownership of resources and to communicate via message-passing. However, implementing each networking subsystem as its own network protocol brings a fair share of challenges.</p>
<p>The most notable challenge is coordinating and eliminating race conditions of peer connection and disconnection events. If we have many network protocols that peers are supposed to be connected on, it is difficult to enforce that a peer is indeed connected on all of them or the order in which those protocols receive notifications that peers have connected. This becomes especially difficult when attempting to share peer state across protocols. All of the Parachain-Host's gossip protocols eliminate DoS with a data-dependency on current chain heads. However, it is inefficient and confusing to implement the logic for tracking our current chain heads as well as our peers' on each of those subsystems. Having one subsystem for tracking this shared state and distributing it to the others is an improvement in architecture and efficiency.</p>
<p>One other piece of shared state to track is peer reputation. When peers are found to have provided value or cost, we adjust their reputation accordingly.</p>
<p>So in short, this Subsystem acts as a bridge between an actual network component and a subsystem's protocol. The implementation of the underlying network component is beyond the scope of this module. We make certain assumptions about the network component:</p>
<ul>
<li>The network allows registering of protocols and multiple versions of each protocol.</li>
<li>The network handles version negotiation of protocols with peers and only connects the peer on the highest version of the protocol.</li>
<li>Each protocol has its own peer-set, although there may be some overlap.</li>
<li>The network provides peer-set management utilities for discovering the peer-IDs of validators and a means of dialing peers with given IDs.</li>
</ul>
<p>The network bridge makes use of the peer-set feature, but is not generic over peer-set. Instead, it exposes two peer-sets that event producers can attach to: <code>Validation</code> and <code>Collation</code>. More information can be found on the documentation of the <a href="node/utility/../../types/overseer-protocol.html#network-bridge-message"><code>NetworkBridgeMessage</code></a>.</p>
<h2 id="protocol-16"><a class="header" href="#protocol-16">Protocol</a></h2>
<p>Input: <a href="node/utility/../../types/overseer-protocol.html#network-bridge-message"><code>NetworkBridgeMessage</code></a></p>
<p>Output:</p>
<ul>
<li><a href="node/utility/../../types/overseer-protocol.html#approval-distribution-message"><code>ApprovalDistributionMessage</code></a><code>::NetworkBridgeUpdateV1</code></li>
<li><a href="node/utility/../../types/overseer-protocol.html#bitfield-distribution-message"><code>BitfieldDistributionMessage</code></a><code>::NetworkBridgeUpdateV1</code></li>
<li><a href="node/utility/../../types/overseer-protocol.html#collator-protocol-message"><code>CollatorProtocolMessage</code></a><code>::NetworkBridgeUpdateV1</code></li>
<li><a href="node/utility/../../types/overseer-protocol.html#statement-distribution-message"><code>StatementDistributionMessage</code></a><code>::NetworkBridgeUpdateV1</code></li>
</ul>
<h2 id="functionality-16"><a class="header" href="#functionality-16">Functionality</a></h2>
<p>This network bridge sends messages of these types over the network.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum WireMessage&lt;M&gt; {
	ProtocolMessage(M),
	ViewUpdate(View),
}
<span class="boring">}
</span></code></pre></pre>
<p>and instantiates this type twice, once using the <a href="node/utility/../../types/network.html#validation-v1"><code>ValidationProtocolV1</code></a> message type, and once with the <a href="node/utility/../../types/network.html#collation-v1"><code>CollationProtocolV1</code></a> message type.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>type ValidationV1Message = WireMessage&lt;ValidationProtocolV1&gt;;
type CollationV1Message = WireMessage&lt;CollationProtocolV1&gt;;
<span class="boring">}
</span></code></pre></pre>
<h3 id="startup"><a class="header" href="#startup">Startup</a></h3>
<p>On startup, we register two protocols with the underlying network utility. One for validation and one for collation. We register only version 1 of each of these protocols.</p>
<h3 id="main-loop"><a class="header" href="#main-loop">Main Loop</a></h3>
<p>The bulk of the work done by this subsystem is in responding to network events, signals from the overseer, and messages from other subsystems.</p>
<p>Each network event is associated with a particular peer-set.</p>
<h3 id="overseer-signal-activeleavesupdate"><a class="header" href="#overseer-signal-activeleavesupdate">Overseer Signal: ActiveLeavesUpdate</a></h3>
<p>The <code>activated</code> and <code>deactivated</code> lists determine the evolution of our local view over time. A <code>ProtocolMessage::ViewUpdate</code> is issued to each connected peer on each peer-set, and a <code>NetworkBridgeEvent::OurViewChange</code> is issued to each event handler for each protocol.</p>
<p>We only send view updates if the node has indicated that it has finished major blockchain synchronization.</p>
<p>If we are connected to the same peer on both peer-sets, we will send the peer two view updates as a result.</p>
<h3 id="overseer-signal-blockfinalized"><a class="header" href="#overseer-signal-blockfinalized">Overseer Signal: BlockFinalized</a></h3>
<p>We update our view's <code>finalized_number</code> to the provided one and delay <code>ProtocolMessage::ViewUpdate</code> and <code>NetworkBridgeEvent::OurViewChange</code> till the next <code>ActiveLeavesUpdate</code>.</p>
<h3 id="network-event-peer-connected"><a class="header" href="#network-event-peer-connected">Network Event: Peer Connected</a></h3>
<p>Issue a <code>NetworkBridgeEvent::PeerConnected</code> for each <a href="node/utility/network-bridge.html#event-handlers">Event Handler</a> of the peer-set and negotiated protocol version of the peer. Also issue a <code>NetworkBridgeEvent::PeerViewChange</code> and send the peer our current view, but only if the node has indicated that it has finished major blockchain synchronization. Otherwise, we only send the peer an empty view.</p>
<h3 id="network-event-peer-disconnected"><a class="header" href="#network-event-peer-disconnected">Network Event: Peer Disconnected</a></h3>
<p>Issue a <code>NetworkBridgeEvent::PeerDisconnected</code> for each <a href="node/utility/network-bridge.html#event-handlers">Event Handler</a> of the peer-set and negotiated protocol version of the peer.</p>
<h3 id="network-event-protocolmessage"><a class="header" href="#network-event-protocolmessage">Network Event: ProtocolMessage</a></h3>
<p>Map the message onto the corresponding <a href="node/utility/network-bridge.html#event-handlers">Event Handler</a> based on the peer-set this message was received on and dispatch via overseer.</p>
<h3 id="network-event-viewupdate"><a class="header" href="#network-event-viewupdate">Network Event: ViewUpdate</a></h3>
<ul>
<li>Check that the new view is valid and note it as the most recent view update of the peer on this peer-set.</li>
<li>Map a <code>NetworkBridgeEvent::PeerViewChange</code> onto the corresponding <a href="node/utility/network-bridge.html#event-handlers">Event Handler</a> based on the peer-set this message was received on and dispatch  via overseer.</li>
</ul>
<h3 id="reportpeer"><a class="header" href="#reportpeer">ReportPeer</a></h3>
<ul>
<li>Adjust peer reputation according to cost or benefit provided</li>
</ul>
<h3 id="disconnectpeer"><a class="header" href="#disconnectpeer">DisconnectPeer</a></h3>
<ul>
<li>Disconnect the peer from the peer-set requested, if connected.</li>
</ul>
<h3 id="sendvalidationmessage--sendvalidationmessages"><a class="header" href="#sendvalidationmessage--sendvalidationmessages">SendValidationMessage / SendValidationMessages</a></h3>
<ul>
<li>Issue a corresponding <code>ProtocolMessage</code> to each listed peer on the validation peer-set.</li>
</ul>
<h3 id="sendcollationmessage--sendcollationmessages"><a class="header" href="#sendcollationmessage--sendcollationmessages">SendCollationMessage / SendCollationMessages</a></h3>
<ul>
<li>Issue a corresponding <code>ProtocolMessage</code> to each listed peer on the collation peer-set.</li>
</ul>
<h3 id="connecttovalidators"><a class="header" href="#connecttovalidators">ConnectToValidators</a></h3>
<ul>
<li>Determine the DHT keys to use for each validator based on the relay-chain state and Runtime API.</li>
<li>Recover the Peer IDs of the validators from the DHT. There may be more than one peer ID per validator.</li>
<li>Send all <code>(ValidatorId, PeerId)</code> pairs on the response channel.</li>
<li>Feed all Peer IDs to peer set manager the underlying network provides.</li>
</ul>
<h3 id="newgossiptopology"><a class="header" href="#newgossiptopology">NewGossipTopology</a></h3>
<ul>
<li>Map all <code>AuthorityDiscoveryId</code>s to <code>PeerId</code>s and issue a corresponding <code>NetworkBridgeUpdateV1</code>
to all validation subsystems.</li>
</ul>
<h2 id="event-handlers"><a class="header" href="#event-handlers">Event Handlers</a></h2>
<p>Network bridge event handlers are the intended recipients of particular network protocol messages. These are each a variant of a message to be sent via the overseer.</p>
<h3 id="validation-v1"><a class="header" href="#validation-v1">Validation V1</a></h3>
<ul>
<li><code>ApprovalDistributionV1Message -&gt; ApprovalDistributionMessage::NetworkBridgeUpdateV1</code></li>
<li><code>BitfieldDistributionV1Message -&gt; BitfieldDistributionMessage::NetworkBridgeUpdateV1</code></li>
<li><code>StatementDistributionV1Message -&gt; StatementDistributionMessage::NetworkBridgeUpdateV1</code></li>
</ul>
<h3 id="collation-v1"><a class="header" href="#collation-v1">Collation V1</a></h3>
<ul>
<li><code>CollatorProtocolV1Message -&gt; CollatorProtocolMessage::NetworkBridgeUpdateV1</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gossip-support"><a class="header" href="#gossip-support">Gossip Support</a></h1>
<p>The Gossip Support Subsystem is responsible for keeping track of session changes
and issuing a connection request to all validators in the next, current and
a few past sessions if we are a validator in these sessions.
The request will add all validators to a reserved PeerSet, meaning we will not
reject a connection request from any validator in that set.</p>
<p>In addition to that, it creates a gossip overlay topology per session which
limits the amount of messages sent and received to be an order of sqrt of the
validators. Our neighbors in this graph will be forwarded to the network bridge
with the <code>NetworkBridgeMessage::NewGossipTopology</code> message.</p>
<p>See https://github.com/paritytech/polkadot/issues/3239 for more details.</p>
<p>The gossip topology is used by parachain distribution subsystems,
such as Bitfield Distrubution, (small) Statement Distributuion and
Approval Distibution to limit the amount of peers we send messages to
and handle view updates.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="misbehavior-arbitration"><a class="header" href="#misbehavior-arbitration">Misbehavior Arbitration</a></h1>
<p>The Misbehavior Arbitration subsystem collects reports of validator misbehavior, and slashes the stake of both misbehaving validator nodes and false accusers.</p>
<blockquote>
<p>TODO: It is not yet fully specified; that problem is postponed to a future PR.</p>
</blockquote>
<p>One policy question we've decided even so: in the event that MA has to call all validators to check some block about which some validators disagree, the minority voters all get slashed, and the majority voters all get rewarded. Validators which abstain have a minor slash penalty, but probably not in the same order of magnitude as those who vote wrong.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="peer-set-manager"><a class="header" href="#peer-set-manager">Peer Set Manager</a></h1>
<blockquote>
<p>TODO</p>
</blockquote>
<h2 id="protocol-17"><a class="header" href="#protocol-17">Protocol</a></h2>
<h2 id="functionality-17"><a class="header" href="#functionality-17">Functionality</a></h2>
<h2 id="jobs-if-any"><a class="header" href="#jobs-if-any">Jobs, if any</a></h2>
<div style="break-before: page; page-break-before: always;"></div><h1 id="runtime-api"><a class="header" href="#runtime-api">Runtime API</a></h1>
<p>The Runtime API subsystem is responsible for providing a single point of access to runtime state data via a set of pre-determined queries. This prevents shared ownership of a blockchain client resource by providing</p>
<h2 id="protocol-18"><a class="header" href="#protocol-18">Protocol</a></h2>
<p>Input: <a href="node/utility/../../types/overseer-protocol.html#runtime-api-message"><code>RuntimeApiMessage</code></a></p>
<p>Output: None</p>
<h2 id="functionality-18"><a class="header" href="#functionality-18">Functionality</a></h2>
<p>On receipt of <code>RuntimeApiMessage::Request(relay_parent, request)</code>, answer the request using the post-state of the relay_parent provided and provide the response to the side-channel embedded within the request.</p>
<blockquote>
<p>TODO Do some caching. The underlying rocksdb already has a cache of trie nodes so duplicate requests are unlikely to hit disk. Not required for functionality.</p>
</blockquote>
<h2 id="jobs"><a class="header" href="#jobs">Jobs</a></h2>
<blockquote>
<p>TODO Don't limit requests based on parent hash, but limit caching. No caching should be done for any requests on relay_parents that are not active based on <code>ActiveLeavesUpdate</code> messages. Maybe with some leeway for things that have just been stopped.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chain-api"><a class="header" href="#chain-api">Chain API</a></h1>
<p>The Chain API subsystem is responsible for providing a single point of access to chain state data via a set of pre-determined queries.</p>
<h2 id="protocol-19"><a class="header" href="#protocol-19">Protocol</a></h2>
<p>Input: <a href="node/utility/../../types/overseer-protocol.html#chain-api-message"><code>ChainApiMessage</code></a></p>
<p>Output: None</p>
<h2 id="functionality-19"><a class="header" href="#functionality-19">Functionality</a></h2>
<p>On receipt of <code>ChainApiMessage</code>, answer the request and provide the response to the side-channel embedded within the request.</p>
<p>Currently, the following requests are supported:</p>
<ul>
<li>Block hash to number</li>
<li>Block hash to header</li>
<li>Block weight</li>
<li>Finalized block number to hash</li>
<li>Last finalized block number</li>
<li>Ancestors</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chain-selection-subsystem"><a class="header" href="#chain-selection-subsystem">Chain Selection Subsystem</a></h1>
<p>This subsystem implements the necessary metadata for the implementation of the <a href="node/utility/../../protocol-chain-selection.html">chain selection</a> portion of the protocol.</p>
<p>The subsystem wraps a database component which maintains a view of the unfinalized chain and records the properties of each block: whether the block is <strong>viable</strong>, whether it is <strong>stagnant</strong>, and whether it is <strong>reverted</strong>. It should also maintain an updated set of active leaves in accordance with this view, which should be cheap to query. Leaves are ordered descending first by weight and then by block number.</p>
<p>This subsystem needs to update its information on the unfinalized chain:</p>
<ul>
<li>On every leaf-activated signal</li>
<li>On every block-finalized signal</li>
<li>On every <code>ChainSelectionMessage::Approve</code></li>
<li>Periodically, to detect stagnation.</li>
</ul>
<p>Simple implementations of these updates do O(n_unfinalized_blocks) disk operations. If the amount of unfinalized blocks is relatively small, the updates should not take very much time. However, in cases where there are hundreds or thousands of unfinalized blocks the naive implementations of these update algorithms would have to be replaced with more sophisticated versions.</p>
<h3 id="overseersignalactiveleavesupdate-1"><a class="header" href="#overseersignalactiveleavesupdate-1"><code>OverseerSignal::ActiveLeavesUpdate</code></a></h3>
<p>Determine all new blocks implicitly referenced by any new active leaves and add them to the view. Update the set of viable leaves accordingly. The weights of imported blocks can be determined by the <a href="node/utility/../../types/overseer-protocol.html#chain-api-message"><code>ChainApiMessage::BlockWeight</code></a>.</p>
<h3 id="overseersignalblockfinalized-2"><a class="header" href="#overseersignalblockfinalized-2"><code>OverseerSignal::BlockFinalized</code></a></h3>
<p>Delete data for all orphaned chains and update all metadata descending from the new finalized block accordingly, along with the set of viable leaves. Note that finalizing a <strong>reverted</strong> or <strong>stagnant</strong> block means that the descendants of those blocks may lose that status because the definitions of those properties don't include the finalized chain. Update the set of viable leaves accordingly.</p>
<h3 id="chainselectionmessageapproved"><a class="header" href="#chainselectionmessageapproved"><code>ChainSelectionMessage::Approved</code></a></h3>
<p>Update the approval status of the referenced block. If the block was stagnant and thus non-viable and is now viable, then the metadata of all of its descendants needs to be updated as well, as they may no longer be stagnant either. Update the set of viable leaves accordingly.</p>
<h3 id="chainselectionmessagebestleafcontaining"><a class="header" href="#chainselectionmessagebestleafcontaining"><code>ChainSelectionMessage::BestLeafContaining</code></a></h3>
<p>If the required block is unknown or not viable, then return <code>None</code>.
Iterate over all leaves, returning the first leaf containing the required block in its chain, and <code>None</code> otherwise.</p>
<h3 id="periodically"><a class="header" href="#periodically">Periodically</a></h3>
<p>Detect stagnant blocks and apply the stagnant definition to all descendants. Update the set of viable leaves accordingly.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="type-definitions"><a class="header" href="#type-definitions">Type Definitions</a></h1>
<p>This section of the guide provides type definitions of various categories.</p>
<h2 id="v1-overview"><a class="header" href="#v1-overview">V1 Overview</a></h2>
<p>Diagrams are rendered in high resolution; open them in a separate tab to see full scale.</p>
<p>These data types are defined in <code>polkadot/primitives/src/v1.rs</code>:</p>
<p><img src="types/data_structures_and_types_0.generated.svg" alt="" /></p>
<p>These data types are defined in <code>polkadot/parachain/src/primitives.rs</code>:</p>
<p><img src="types/data_structures_and_types_1.generated.svg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="candidate-types"><a class="header" href="#candidate-types">Candidate Types</a></h1>
<p>Para candidates are some of the most common types, both within the runtime and on the Node-side.
Candidates are the fundamental datatype for advancing parachains and parathreads, encapsulating the collator's signature, the context of the parablock, the commitments to the output, and a commitment to the data which proves it valid.</p>
<p>In a way, this entire guide is about these candidates: how they are scheduled, constructed, backed, included, and challenged.</p>
<p>This section will describe the base candidate type, its components, and variants that contain extra data.</p>
<h2 id="para-id"><a class="header" href="#para-id">Para Id</a></h2>
<p>A unique 32-bit identifier referring to a specific para (chain or thread). The relay-chain runtime guarantees that <code>ParaId</code>s are unique for the duration of any session, but recycling and reuse over a longer period of time is permitted.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct ParaId(u32);
<span class="boring">}
</span></code></pre></pre>
<h2 id="candidate-receipt"><a class="header" href="#candidate-receipt">Candidate Receipt</a></h2>
<p>Much info in a <a href="types/candidate.html#full-candidate-receipt"><code>FullCandidateReceipt</code></a> is duplicated from the relay-chain state. When the corresponding relay-chain state is considered widely available, the Candidate Receipt should be favored over the <code>FullCandidateReceipt</code>.</p>
<p>Examples of situations where the state is readily available includes within the scope of work done by subsystems working on a given relay-parent, or within the logic of the runtime importing a backed candidate.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// A candidate-receipt.
struct CandidateReceipt {
	/// The descriptor of the candidate.
	descriptor: CandidateDescriptor,
	/// The hash of the encoded commitments made as a result of candidate execution.
	commitments_hash: Hash,
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="full-candidate-receipt"><a class="header" href="#full-candidate-receipt">Full Candidate Receipt</a></h2>
<p>This is the full receipt type. The <code>PersistedValidationData</code> are technically redundant with the <code>inner.relay_parent</code>, which uniquely describes the block in the blockchain from whose state these values are derived. The <a href="types/candidate.html#candidate-receipt"><code>CandidateReceipt</code></a> variant is often used instead for this reason.</p>
<p>However, the Full Candidate Receipt type is useful as a means of avoiding the implicit dependency on availability of old blockchain state. In situations such as availability and approval, having the full description of the candidate within a self-contained struct is convenient.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// All data pertaining to the execution of a para candidate.
struct FullCandidateReceipt {
	inner: CandidateReceipt,
	validation_data: PeristedValidationData,
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="committed-candidate-receipt"><a class="header" href="#committed-candidate-receipt">Committed Candidate Receipt</a></h2>
<p>This is a variant of the candidate receipt which includes the commitments of the candidate receipt alongside the descriptor. This should be favored over the <a href="types/candidate.html#candidate-receipt"><code>Candidate Receipt</code></a> in situations where the candidate is not going to be executed but the actual data committed to is important. This is often the case in the backing phase.</p>
<p>The hash of the committed candidate receipt will be the same as the corresponding <a href="types/candidate.html#candidate-receipt"><code>Candidate Receipt</code></a>, because it is computed by first hashing the encoding of the commitments to form a plain <a href="types/candidate.html#candidate-receipt"><code>Candidate Receipt</code></a>.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// A candidate-receipt with commitments directly included.
struct CommittedCandidateReceipt {
	/// The descriptor of the candidate.
	descriptor: CandidateDescriptor,
	/// The commitments of the candidate receipt.
	commitments: CandidateCommitments,
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="candidate-descriptor"><a class="header" href="#candidate-descriptor">Candidate Descriptor</a></h2>
<p>This struct is pure description of the candidate, in a lightweight format.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// A unique descriptor of the candidate receipt.
struct CandidateDescriptor {
	/// The ID of the para this is a candidate for.
	para_id: ParaId,
	/// The hash of the relay-chain block this is executed in the context of.
	relay_parent: Hash,
	/// The collator's sr25519 public key.
	collator: CollatorId,
	/// The blake2-256 hash of the persisted validation data. These are extra parameters
	/// derived from relay-chain state that influence the validity of the block which
	/// must also be kept available for secondary checkers.
	persisted_validation_data_hash: Hash,
	/// The blake2-256 hash of the pov-block.
	pov_hash: Hash,
	/// The root of a block's erasure encoding Merkle tree.
	erasure_root: Hash,
	/// Signature on blake2-256 of components of this receipt:
	/// The parachain index, the relay parent, the validation data hash, and the pov_hash.
	signature: CollatorSignature,
	/// Hash of the para header that is being generated by this candidate.
	para_head: Hash,
	/// The blake2-256 hash of the validation code bytes.
	validation_code_hash: ValidationCodeHash,
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="persistedvalidationdata"><a class="header" href="#persistedvalidationdata">PersistedValidationData</a></h2>
<p>The validation data provides information about how to create the inputs for validation of a candidate. This information is derived from the chain state and will vary from para to para, although some of the fields may be the same for every para.</p>
<p>Since this data is used to form inputs to the validation function, it needs to be persisted by the availability system to avoid dependence on availability of the relay-chain state.</p>
<p>Furthermore, the validation data acts as a way to authorize the additional data the collator needs to pass to the validation function. For example, the validation function can check whether the incoming messages (e.g. downward messages) were actually sent by using the data provided in the validation data using so called MQC heads.</p>
<p>Since the commitments of the validation function are checked by the relay-chain, secondary checkers can rely on the invariant that the relay-chain only includes para-blocks for which these checks have already been done. As such, there is no need for the validation data used to inform validators and collators about the checks the relay-chain will perform to be persisted by the availability system.</p>
<p>The <code>PersistedValidationData</code> should be relatively lightweight primarly because it is constructed during inclusion for each candidate and therefore lies on the critical path of inclusion.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct PersistedValidationData {
	/// The parent head-data.
	parent_head: HeadData,
	/// The relay-chain block number this is in the context of. This informs the collator.
	relay_parent_number: BlockNumber,
	/// The relay-chain block storage root this is in the context of.
	relay_parent_storage_root: Hash,
	/// The list of MQC heads for the inbound channels paired with the sender para ids. This
	/// vector is sorted ascending by the para id and doesn't contain multiple entries with the same
	/// sender.
	///
	/// The HRMP MQC heads will be used by the validation function to authorize the input messages passed
	/// by the collator.
	hrmp_mqc_heads: Vec&lt;(ParaId, Hash)&gt;,
	/// The maximum legal size of a POV block, in bytes.
	pub max_pov_size: u32,
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="headdata"><a class="header" href="#headdata">HeadData</a></h2>
<p>Head data is a type-safe abstraction around bytes (<code>Vec&lt;u8&gt;</code>) for the purposes of representing heads of parachains or parathreads.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct HeadData(Vec&lt;u8&gt;);
<span class="boring">}
</span></code></pre></pre>
<h2 id="candidate-commitments"><a class="header" href="#candidate-commitments">Candidate Commitments</a></h2>
<p>The execution and validation of parachain or parathread candidates produces a number of values which either must be committed to on the relay chain or committed to the state of the relay chain.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Commitments made in a `CandidateReceipt`. Many of these are outputs of validation.
#[derive(PartialEq, Eq, Clone, Encode, Decode)]
#[cfg_attr(feature = &quot;std&quot;, derive(Debug, Default))]
struct CandidateCommitments {
	/// Messages directed to other paras routed via the relay chain.
	horizontal_messages: Vec&lt;OutboundHrmpMessage&gt;,
	/// Messages destined to be interpreted by the Relay chain itself.
	upward_messages: Vec&lt;UpwardMessage&gt;,
	/// New validation code.
	new_validation_code: Option&lt;ValidationCode&gt;,
	/// The head-data produced as a result of execution.
	head_data: HeadData,
	/// The number of messages processed from the DMQ.
	processed_downward_messages: u32,
	/// The mark which specifies the block number up to which all inbound HRMP messages are processed.
	hrmp_watermark: BlockNumber,
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="signing-context"><a class="header" href="#signing-context">Signing Context</a></h2>
<p>This struct provides context to signatures by combining with various payloads to localize the signature to a particular session index and relay-chain hash. Having these fields included in the signature makes misbehavior attribution much simpler.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct SigningContext {
	/// The relay-chain block hash this signature is in the context of.
	parent_hash: Hash,
	/// The session index this signature is in the context of.
	session_index: SessionIndex,
}
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="backing-types"><a class="header" href="#backing-types">Backing Types</a></h1>
<p><a href="types/candidate.html">Candidates</a> go through many phases before being considered included in a fork of the relay chain and eventually accepted.</p>
<p>These types describe the data used in the backing phase. Some are sent over the wire within subsystems, and some are simply included in the relay-chain block.</p>
<h2 id="validity-attestation"><a class="header" href="#validity-attestation">Validity Attestation</a></h2>
<p>An attestation of validity for a candidate, used as part of a backing. Both the <code>Seconded</code> and <code>Valid</code> statements are considered attestations of validity. This structure is only useful where the candidate referenced is apparent.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum ValidityAttestation {
  /// Implicit validity attestation by issuing.
  /// This corresponds to issuance of a `Seconded` statement.
  Implicit(ValidatorSignature),
  /// An explicit attestation. This corresponds to issuance of a
  /// `Valid` statement.
  Explicit(ValidatorSignature),
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="signed-wrapper"><a class="header" href="#signed-wrapper">Signed Wrapper</a></h2>
<p>There are a few distinct types which we desire to sign, and validate the signatures of. Instead of duplicating this work, we extract a signed wrapper.</p>
<pre><code class="language-rust ignore">/// A signed type which encapsulates the common desire to sign some data and validate a signature.
///
/// Note that the internal fields are not public; they are all accessable by immutable getters.
/// This reduces the chance that they are accidentally mutated, invalidating the signature.
struct Signed&lt;Payload, RealPayload=Payload&gt; {
    /// The payload is part of the signed data. The rest is the signing context,
    /// which is known both at signing and at validation.
    payload: Payload,
    /// The index of the validator signing this statement.
    validator_index: ValidatorIndex,
    /// The signature by the validator of the signed payload.
    signature: ValidatorSignature,
}

impl&lt;Payload: EncodeAs&lt;RealPayload&gt;, RealPayload: Encode&gt; Signed&lt;Payload, RealPayload&gt; {
    fn sign(payload: Payload, context: SigningContext, index: ValidatorIndex, key: ValidatorPair) -&gt; Signed&lt;Payload, RealPayload&gt; { ... }
    fn validate(&amp;self, context: SigningContext, key: ValidatorId) -&gt; bool { ... }
}
</code></pre>
<p>Note the presence of the <a href="types/../types/candidate.html#signing-context"><code>SigningContext</code></a> in the signatures of the <code>sign</code> and <code>validate</code> methods. To ensure cryptographic security, the actual signed payload is always the SCALE encoding of <code>(payload.into(), signing_context)</code>. Including the signing context prevents replay attacks.</p>
<p><code>EncodeAs</code> is a helper trait with a blanket impl which ensures that any <code>T</code> can <code>EncodeAs&lt;T&gt;</code>. Therefore, for the generic case where <code>RealPayload = Payload</code>, it changes nothing. However, we  <code>impl EncodeAs&lt;CompactStatement&gt; for Statement</code>, which helps efficiency.</p>
<h2 id="statement-type"><a class="header" href="#statement-type">Statement Type</a></h2>
<p>The <a href="types/../node/backing/candidate-backing.html">Candidate Backing subsystem</a> issues and signs these after candidate validation.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// A statement about the validity of a parachain candidate.
enum Statement {
  /// A statement about a new candidate being seconded by a validator. This is an implicit validity vote.
  ///
  /// The main semantic difference between `Seconded` and `Valid` comes from the fact that every validator may
  /// second only 1 candidate; this places an upper bound on the total number of candidates whose validity
  /// needs to be checked. A validator who seconds more than 1 parachain candidate per relay head is subject
  /// to slashing.
  Seconded(CommittedCandidateReceipt),
  /// A statement about the validity of a candidate, based on candidate's hash.
  Valid(Hash),
}

/// A statement about the validity of a parachain candidate.
///
/// This variant should only be used in the production of `SignedStatement`s. The only difference between
/// this enum and `Statement` is that the `Seconded` variant contains a `Hash` instead of a `CandidateReceipt`.
/// The rationale behind the difference is that the signature should always be on the hash instead of the
/// full data, as this lowers the requirement for checking while retaining necessary cryptographic properties
enum CompactStatement {
  /// A statement about a new candidate being seconded by a validator. This is an implicit validity vote.
  Seconded(Hash),
  /// A statement about the validity of a candidate, based on candidate's hash.
  Valid(Hash),
}
<span class="boring">}
</span></code></pre></pre>
<p><code>CompactStatement</code> exists because a <code>CandidateReceipt</code> includes <code>HeadData</code>, which does not have a bounded size.</p>
<h2 id="signed-statement-type"><a class="header" href="#signed-statement-type">Signed Statement Type</a></h2>
<p>A statement which has been <a href="types/backing.html#signed-wrapper">cryptographically signed</a> by a validator.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// A signed statement, containing the committed candidate receipt in the `Seconded` variant.
pub type SignedFullStatement = Signed&lt;Statement, CompactStatement&gt;;

/// A signed statement, containing only the hash.
pub type SignedStatement = Signed&lt;CompactStatement&gt;;
<span class="boring">}
</span></code></pre></pre>
<p>Munging the signed <code>Statement</code> into a <code>CompactStatement</code> before signing allows the candidate receipt itself to be omitted when checking a signature on a <code>Seconded</code> statement.</p>
<h2 id="backed-candidate"><a class="header" href="#backed-candidate">Backed Candidate</a></h2>
<p>An <a href="types/candidate.html#committed-candidate-receipt"><code>CommittedCandidateReceipt</code></a> along with all data necessary to prove its backing. This is submitted to the relay-chain to process and move along the candidate to the pending-availability stage.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct BackedCandidate {
  candidate: CommittedCandidateReceipt,
  validity_votes: Vec&lt;ValidityAttestation&gt;,
  // the indices of validators who signed the candidate within the group. There is no need to include
  // bit for any validators who are not in the group, so this is more compact.
  // The number of bits is the number of validators in the group.
  //
  // the group should be apparent from context.
  validator_indices: BitVec,
}

struct BackedCandidates(Vec&lt;BackedCandidate&gt;); // sorted by para-id.
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="availability"><a class="header" href="#availability">Availability</a></h1>
<p>One of the key roles of validators is to ensure availability of all data necessary to validate
candidates for the duration of a challenge period. This is done via an erasure-coding of the data to keep available.</p>
<h2 id="signed-availability-bitfield"><a class="header" href="#signed-availability-bitfield">Signed Availability Bitfield</a></h2>
<p>A bitfield <a href="types/backing.html#signed-wrapper">signed</a> by a particular validator about the availability of pending candidates.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>type SignedAvailabilityBitfield = Signed&lt;Bitvec&gt;;

struct Bitfields(Vec&lt;(SignedAvailabilityBitfield)&gt;), // bitfields sorted by validator index, ascending
<span class="boring">}
</span></code></pre></pre>
<h3 id="semantics"><a class="header" href="#semantics">Semantics</a></h3>
<p>A <code>SignedAvailabilityBitfield</code> represents the view from a particular validator's perspective. Each bit in the bitfield corresponds to a single <a href="types/../runtime-api/availability-cores.html">availability core</a>. A <code>1</code> bit indicates that the validator believes the following statements to be true for a core:</p>
<ul>
<li>the availability core is occupied</li>
<li>there exists a <a href="types/candidate.html#committed-candidate-receipt"><code>CommittedCandidateReceipt</code></a> corresponding to that core. In other words, that para has a block in progress.</li>
<li>the validator's <a href="types/../node/utility/availability-store.html">Availability Store</a> contains a chunk of that parablock's PoV.</li>
</ul>
<p>In other words, it is the transpose of <a href="types/../runtime-api/availability-cores.html"><code>OccupiedCore::availability</code></a>.</p>
<h2 id="proof-of-validity"><a class="header" href="#proof-of-validity">Proof-of-Validity</a></h2>
<p>Often referred to as PoV, this is a type-safe wrapper around bytes (<code>Vec&lt;u8&gt;</code>) when referring to data that acts as a stateless-client proof of validity of a candidate, when used as input to the validation function of the para.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct PoV(Vec&lt;u8&gt;);
<span class="boring">}
</span></code></pre></pre>
<h2 id="available-data"><a class="header" href="#available-data">Available Data</a></h2>
<p>This is the data we want to keep available for each <a href="types/candidate.html">candidate</a> included in the relay chain. This is the PoV of the block, as well as the <a href="types/candidate.html#persistedvalidationdata"><code>PersistedValidationData</code></a></p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct AvailableData {
    /// The Proof-of-Validation of the candidate.
    pov: Arc&lt;PoV&gt;,
    /// The persisted validation data used to check the candidate.
    validation_data: PersistedValidationData,
}
<span class="boring">}
</span></code></pre></pre>
<blockquote>
<p>TODO: With XCMP, we also need to keep available the outgoing messages as a result of para-validation.</p>
</blockquote>
<h2 id="erasure-chunk"><a class="header" href="#erasure-chunk">Erasure Chunk</a></h2>
<p>The <a href="types/availability.html#availabledata"><code>AvailableData</code></a> is split up into an erasure-coding as part of the availability process. Each validator gets a chunk. This describes one of those chunks, along with its proof against a merkle root hash, which should be apparent from context, and is the <code>erasure_root</code> field of a <a href="types/candidate.html#candidatedescriptor"><code>CandidateDescriptor</code></a>.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct ErasureChunk {
    /// The erasure-encoded chunk of data belonging to the candidate block.
    chunk: Vec&lt;u8&gt;,
    /// The index of this erasure-encoded chunk of data.
    index: u32,
    /// Proof for this chunk's branch in the Merkle tree.
    proof: Vec&lt;Vec&lt;u8&gt;&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="overseer-protocol"><a class="header" href="#overseer-protocol">Overseer Protocol</a></h1>
<p>This chapter contains message types sent to and from the overseer, and the underlying subsystem message types that are transmitted using these.</p>
<h2 id="overseer-signal"><a class="header" href="#overseer-signal">Overseer Signal</a></h2>
<p>Signals from the overseer to a subsystem to request change in execution that has to be obeyed by the subsystem.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum OverseerSignal {
  /// Signal about a change in active leaves.
  ActiveLeavesUpdate(ActiveLeavesUpdate),
  /// Signal about a new best finalized block.
  BlockFinalized(Hash),
  /// Conclude all operation.
  Conclude,
}
<span class="boring">}
</span></code></pre></pre>
<p>All subsystems have their own message types; all of them need to be able to listen for overseer signals as well. There are currently two proposals for how to handle that with unified communication channels:</p>
<ol>
<li>Retaining the <code>OverseerSignal</code> definition above, add <code>enum FromOverseer&lt;T&gt; {Signal(OverseerSignal), Message(T)}</code>.</li>
<li>Add a generic varint to <code>OverseerSignal</code>: <code>Message(T)</code>.</li>
</ol>
<p>Either way, there will be some top-level type encapsulating messages from the overseer to each subsystem.</p>
<h2 id="active-leaves-update"><a class="header" href="#active-leaves-update">Active Leaves Update</a></h2>
<p>Indicates a change in active leaves. Activated leaves should have jobs, whereas deactivated leaves should lead to winding-down of work based on those leaves.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum LeafStatus {
    // A leaf is fresh when it's the first time the leaf has been encountered.
    // Most leaves should be fresh.
    Fresh,
    // A leaf is stale when it's encountered for a subsequent time. This will
    // happen when the chain is reverted or the fork-choice rule abandons some
    // chain.
    Stale,
}

struct ActiveLeavesUpdate {
    activated: [(Hash, Number, LeafStatus)], // in practice, these should probably be a SmallVec
    deactivated: [Hash],
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="all-messages"><a class="header" href="#all-messages">All Messages</a></h2>
<p>A message type tying together all message types that are used across Subsystems.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum AllMessages {
    CandidateValidation(CandidateValidationMessage),
    CandidateBacking(CandidateBackingMessage),
    ChainApi(ChainApiMessage),
    CollatorProtocol(CollatorProtocolMessage),
    StatementDistribution(StatementDistributionMessage),
    AvailabilityDistribution(AvailabilityDistributionMessage),
    AvailabilityRecovery(AvailabilityRecoveryMessage),
    BitfieldDistribution(BitfieldDistributionMessage),
    BitfieldSigning(BitfieldSigningMessage),
    Provisioner(ProvisionerMessage),
    RuntimeApi(RuntimeApiMessage),
    AvailabilityStore(AvailabilityStoreMessage),
    NetworkBridge(NetworkBridgeMessage),
    CollationGeneration(CollationGenerationMessage),
    ApprovalVoting(ApprovalVotingMessage),
    ApprovalDistribution(ApprovalDistributionMessage),
    GossipSupport(GossipSupportMessage),
    DisputeCoordinator(DisputeCoordinatorMessage),
    DisputeParticipation(DisputeParticipationMessage),
    ChainSelection(ChainSelectionMessage),
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="approval-voting-message"><a class="header" href="#approval-voting-message">Approval Voting Message</a></h2>
<p>Messages received by the approval voting subsystem.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum AssignmentCheckResult {
    // The vote was accepted and should be propagated onwards.
    Accepted,
    // The vote was valid but duplicate and should not be propagated onwards.
    AcceptedDuplicate,
    // The vote was valid but too far in the future to accept right now.
    TooFarInFuture,
    // The vote was bad and should be ignored, reporting the peer who propagated it.
    Bad(AssignmentCheckError),
}

pub enum AssignmentCheckError {
    UnknownBlock(Hash),
    UnknownSessionIndex(SessionIndex),
    InvalidCandidateIndex(CandidateIndex),
    InvalidCandidate(CandidateIndex, CandidateHash),
    InvalidCert(ValidatorIndex),
    Internal(Hash, CandidateHash),
}

enum ApprovalCheckResult {
    // The vote was accepted and should be propagated onwards.
    Accepted,
    // The vote was bad and should be ignored, reporting the peer who propagated it.
    Bad(ApprovalCheckError),
}

pub enum ApprovalCheckError {
    UnknownBlock(Hash),
    UnknownSessionIndex(SessionIndex),
    InvalidCandidateIndex(CandidateIndex),
    InvalidValidatorIndex(ValidatorIndex),
    InvalidCandidate(CandidateIndex, CandidateHash),
    InvalidSignature(ValidatorIndex),
    NoAssignment(ValidatorIndex),
    Internal(Hash, CandidateHash),
}

enum ApprovalVotingMessage {
    /// Check if the assignment is valid and can be accepted by our view of the protocol.
    /// Should not be sent unless the block hash is known.
    CheckAndImportAssignment(
        IndirectAssignmentCert,
        CandidateIndex, // The index of the candidate included in the block.
        ResponseChannel&lt;AssignmentCheckResult&gt;,
    ),
    /// Check if the approval vote is valid and can be accepted by our view of the
    /// protocol.
    ///
    /// Should not be sent unless the block hash within the indirect vote is known.
    CheckAndImportApproval(
        IndirectSignedApprovalVote,
        ResponseChannel&lt;ApprovalCheckResult&gt;,
    ),
    /// Returns the highest possible ancestor hash of the provided block hash which is
    /// acceptable to vote on finality for. Along with that, return the lists of candidate hashes
    /// which appear in every block from the (non-inclusive) base number up to (inclusive) the specified
    /// approved ancestor.
    /// This list starts from the highest block (the approved ancestor itself) and moves backwards
    /// towards the base number.
    ///
    /// The base number is typically the number of the last finalized block, but in GRANDPA it is
    /// possible for the base to be slightly higher than the last finalized block.
    ///
    /// The `BlockNumber` provided is the number of the block's ancestor which is the
    /// earliest possible vote.
    ///
    /// It can also return the same block hash, if that is acceptable to vote upon.
    /// Return `None` if the input hash is unrecognized.
    ApprovedAncestor {
        target_hash: Hash,
        base_number: BlockNumber,
        rx: ResponseChannel&lt;Option&lt;(Hash, BlockNumber, Vec&lt;(Hash, Vec&lt;CandidateHash&gt;)&gt;)&gt;&gt;
    },
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="approval-distribution-message"><a class="header" href="#approval-distribution-message">Approval Distribution Message</a></h2>
<p>Messages received by the approval distribution subsystem.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Metadata about a block which is now live in the approval protocol.
struct BlockApprovalMeta {
    /// The hash of the block.
    hash: Hash,
    /// The number of the block.
    number: BlockNumber,
    /// The candidates included by the block. Note that these are not the same as the candidates that appear within the
    /// block body.
    parent_hash: Hash,
    /// The candidates included by the block. Note that these are not the same as the candidates that appear within the
    /// block body.
    candidates: Vec&lt;CandidateHash&gt;,
    /// The consensus slot of the block.
    slot: Slot,
}

enum ApprovalDistributionMessage {
    /// Notify the `ApprovalDistribution` subsystem about new blocks and the candidates contained within
    /// them.
    NewBlocks(Vec&lt;BlockApprovalMeta&gt;),
    /// Distribute an assignment cert from the local validator. The cert is assumed
    /// to be valid, relevant, and for the given relay-parent and validator index.
    ///
    /// The `u32` param is the candidate index in the fully-included list.
    DistributeAssignment(IndirectAssignmentCert, u32),
    /// Distribute an approval vote for the local validator. The approval vote is assumed to be
    /// valid, relevant, and the corresponding approval already issued. If not, the subsystem is free to drop
    /// the message.
    DistributeApproval(IndirectSignedApprovalVote),
    /// An update from the network bridge.
    NetworkBridgeUpdateV1(NetworkBridgeEvent&lt;ApprovalDistributionV1Message&gt;),
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="availability-distribution-message"><a class="header" href="#availability-distribution-message">Availability Distribution Message</a></h2>
<p>Messages received by the availability distribution subsystem.</p>
<p>This is a network protocol that receives messages of type <a href="types/network.html#availability-distribution-v1"><code>AvailabilityDistributionV1Message</code></a>.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum AvailabilityDistributionMessage {
      /// Incoming network request for an availability chunk.
      ChunkFetchingRequest(IncomingRequest&lt;req_res_v1::ChunkFetchingRequest&gt;),
      /// Incoming network request for a seconded PoV.
      PoVFetchingRequest(IncomingRequest&lt;req_res_v1::PoVFetchingRequest&gt;),
      /// Instruct availability distribution to fetch a remote PoV.
      ///
      /// NOTE: The result of this fetch is not yet locally validated and could be bogus.
      FetchPoV {
          /// The relay parent giving the necessary context.
          relay_parent: Hash,
          /// Validator to fetch the PoV from.
          from_validator: ValidatorIndex,
          /// Candidate hash to fetch the PoV for.
          candidate_hash: CandidateHash,
          /// Expected hash of the PoV, a PoV not matching this hash will be rejected.
          pov_hash: Hash,
          /// Sender for getting back the result of this fetch.
          ///
          /// The sender will be canceled if the fetching failed for some reason.
          tx: oneshot::Sender&lt;PoV&gt;,
      },
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="availability-recovery-message"><a class="header" href="#availability-recovery-message">Availability Recovery Message</a></h2>
<p>Messages received by the availability recovery subsystem.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum RecoveryError {
    Invalid,
    Unavailable,
}
enum AvailabilityRecoveryMessage {
    /// Recover available data from validators on the network.
    RecoverAvailableData(
        CandidateReceipt,
        SessionIndex,
        Option&lt;GroupIndex&gt;, // Backing validator group to request the data directly from.
        ResponseChannel&lt;Result&lt;AvailableData, RecoveryError&gt;&gt;,
    ),
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="availability-store-message"><a class="header" href="#availability-store-message">Availability Store Message</a></h2>
<p>Messages to and from the availability store.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum AvailabilityStoreMessage {
    /// Query the `AvailableData` of a candidate by hash.
    QueryAvailableData(CandidateHash, ResponseChannel&lt;Option&lt;AvailableData&gt;&gt;),
    /// Query whether an `AvailableData` exists within the AV Store.
    QueryDataAvailability(CandidateHash, ResponseChannel&lt;bool&gt;),
    /// Query a specific availability chunk of the candidate's erasure-coding by validator index.
    /// Returns the chunk and its inclusion proof against the candidate's erasure-root.
    QueryChunk(CandidateHash, ValidatorIndex, ResponseChannel&lt;Option&lt;ErasureChunk&gt;&gt;),
    /// Query all chunks that we have locally for the given candidate hash.
    QueryAllChunks(CandidateHash, ResponseChannel&lt;Vec&lt;ErasureChunk&gt;&gt;),
    /// Store a specific chunk of the candidate's erasure-coding by validator index, with an
    /// accompanying proof.
    StoreChunk(CandidateHash, ErasureChunk, ResponseChannel&lt;Result&lt;()&gt;&gt;),
    /// Store `AvailableData`. If `ValidatorIndex` is provided, also store this validator's
    /// `ErasureChunk`.
    StoreAvailableData(CandidateHash, Option&lt;ValidatorIndex&gt;, u32, AvailableData, ResponseChannel&lt;Result&lt;()&gt;&gt;),
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="bitfield-distribution-message"><a class="header" href="#bitfield-distribution-message">Bitfield Distribution Message</a></h2>
<p>Messages received by the bitfield distribution subsystem.
This is a network protocol that receives messages of type <a href="types/network.html#bitfield-distribution-v1"><code>BitfieldDistributionV1Message</code></a>.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum BitfieldDistributionMessage {
    /// Distribute a bitfield signed by a validator to other validators.
    /// The bitfield distribution subsystem will assume this is indeed correctly signed.
    DistributeBitfield(relay_parent, SignedAvailabilityBitfield),
    /// Receive a network bridge update.
    NetworkBridgeUpdateV1(NetworkBridgeEvent&lt;BitfieldDistributionV1Message&gt;),
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="bitfield-signing-message"><a class="header" href="#bitfield-signing-message">Bitfield Signing Message</a></h2>
<p>Currently, the bitfield signing subsystem receives no specific messages.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Non-instantiable message type
enum BitfieldSigningMessage { }
<span class="boring">}
</span></code></pre></pre>
<h2 id="candidate-backing-message"><a class="header" href="#candidate-backing-message">Candidate Backing Message</a></h2>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum CandidateBackingMessage {
  /// Requests a set of backable candidates that could be backed in a child of the given
  /// relay-parent, referenced by its hash.
  GetBackedCandidates(Hash, Vec&lt;CandidateHash&gt;, ResponseChannel&lt;Vec&lt;BackedCandidate&gt;&gt;),
  /// Note that the Candidate Backing subsystem should second the given candidate in the context of the
  /// given relay-parent (ref. by hash). This candidate must be validated using the provided PoV.
  /// The PoV is expected to match the `pov_hash` in the descriptor.
  Second(Hash, CandidateReceipt, PoV),
  /// Note a peer validator's statement about a particular candidate. Disagreements about validity must be escalated
  /// to a broader check by Misbehavior Arbitration. Agreements are simply tallied until a quorum is reached.
  Statement(Statement),
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="chain-api-message"><a class="header" href="#chain-api-message">Chain API Message</a></h2>
<p>The Chain API subsystem is responsible for providing an interface to chain data.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum ChainApiMessage {
    /// Get the block number by hash.
    /// Returns `None` if a block with the given hash is not present in the db.
    BlockNumber(Hash, ResponseChannel&lt;Result&lt;Option&lt;BlockNumber&gt;, Error&gt;&gt;),
    /// Request the block header by hash.
    /// Returns `None` if a block with the given hash is not present in the db.
    BlockHeader(Hash, ResponseChannel&lt;Result&lt;Option&lt;BlockHeader&gt;, Error&gt;&gt;),
    /// Get the cumulative weight of the given block, by hash.
    /// If the block or weight is unknown, this returns `None`.
    /// 
    /// Weight is used for comparing blocks in a fork-choice rule.
    BlockWeight(Hash, ResponseChannel&lt;Result&lt;Option&lt;Weight&gt;, Error&gt;&gt;),
    /// Get the finalized block hash by number.
    /// Returns `None` if a block with the given number is not present in the db.
    /// Note: the caller must ensure the block is finalized.
    FinalizedBlockHash(BlockNumber, ResponseChannel&lt;Result&lt;Option&lt;Hash&gt;, Error&gt;&gt;),
    /// Get the last finalized block number.
    /// This request always succeeds.
    FinalizedBlockNumber(ResponseChannel&lt;Result&lt;BlockNumber, Error&gt;&gt;),
    /// Request the `k` ancestors block hashes of a block with the given hash.
    /// The response channel may return a `Vec` of size up to `k`
    /// filled with ancestors hashes with the following order:
    /// `parent`, `grandparent`, ...
    Ancestors {
        /// The hash of the block in question.
        hash: Hash,
        /// The number of ancestors to request.
        k: usize,
        /// The response channel.
        response_channel: ResponseChannel&lt;Result&lt;Vec&lt;Hash&gt;, Error&gt;&gt;,
    }
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="chain-selection-message"><a class="header" href="#chain-selection-message">Chain Selection Message</a></h2>
<p>Messages received by the <a href="types/../node/utility/chain-selection.html">Chain Selection subsystem</a></p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum ChainSelectionMessage {
    /// Signal to the chain selection subsystem that a specific block has been approved.
    Approved(Hash),
    /// Request the leaves in descending order by score.
    Leaves(ResponseChannel&lt;Vec&lt;Hash&gt;&gt;),
    /// Request the best leaf containing the given block in its ancestry. Return `None` if
    /// there is no such leaf.
    BestLeafContaining(Hash, ResponseChannel&lt;Option&lt;Hash&gt;&gt;),

}
<span class="boring">}
</span></code></pre></pre>
<h2 id="collator-protocol-message"><a class="header" href="#collator-protocol-message">Collator Protocol Message</a></h2>
<p>Messages received by the <a href="types/../node/collators/collator-protocol.html">Collator Protocol subsystem</a></p>
<p>This is a network protocol that receives messages of type <a href="types/network.html#collator-protocol-v1"><code>CollatorProtocolV1Message</code></a>.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum CollatorProtocolMessage {
    /// Signal to the collator protocol that it should connect to validators with the expectation
    /// of collating on the given para. This is only expected to be called once, early on, if at all,
    /// and only by the Collation Generation subsystem. As such, it will overwrite the value of
    /// the previous signal.
    ///
    /// This should be sent before any `DistributeCollation` message.
    CollateOn(ParaId),
    /// Provide a collation to distribute to validators with an optional result sender.
    ///
    /// The result sender should be informed when at least one parachain validator seconded the collation. It is also
    /// completely okay to just drop the sender.
    DistributeCollation(CandidateReceipt, PoV, Option&lt;oneshot::Sender&lt;SignedFullStatement&gt;&gt;),
    /// Fetch a collation under the given relay-parent for the given ParaId.
    FetchCollation(Hash, ParaId, ResponseChannel&lt;(CandidateReceipt, PoV)&gt;),
    /// Report a collator as having provided an invalid collation. This should lead to disconnect
    /// and blacklist of the collator.
    ReportCollator(CollatorId),
    /// Note a collator as having provided a good collation.
    NoteGoodCollation(CollatorId, SignedFullStatement),
    /// Notify a collator that its collation was seconded.
    NotifyCollationSeconded(CollatorId, Hash, SignedFullStatement),
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="dispute-coordinator-message"><a class="header" href="#dispute-coordinator-message">Dispute Coordinator Message</a></h2>
<p>Messages received by the <a href="types/../node/disputes/dispute-coordinator.html">Dispute Coordinator subsystem</a></p>
<p>This subsystem coordinates participation in disputes, tracks live disputes, and observed statements of validators from subsystems.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum DisputeCoordinatorMessage {
    /// Import a statement by a validator about a candidate.
    ///
    /// The subsystem will silently discard ancient statements or sets of only dispute-specific statements for
    /// candidates that are previously unknown to the subsystem. The former is simply because ancient
    /// data is not relevant and the latter is as a DoS prevention mechanism. Both backing and approval
    /// statements already undergo anti-DoS procedures in their respective subsystems, but statements
    /// cast specifically for disputes are not necessarily relevant to any candidate the system is
    /// already aware of and thus present a DoS vector. Our expectation is that nodes will notify each
    /// other of disputes over the network by providing (at least) 2 conflicting statements, of which one is either
    /// a backing or validation statement.
    ///
    /// This does not do any checking of the message signature.
    ImportStatements {
        /// The hash of the candidate.
        candidate_hash: CandidateHash,
        /// The candidate receipt itself.
        candidate_receipt: CandidateReceipt,
        /// The session the candidate appears in.
        session: SessionIndex,
        /// Triples containing the following:
        /// - A statement, either indicating validity or invalidity of the candidate.
        /// - The validator index (within the session of the candidate) of the validator casting the vote.
        /// - The signature of the validator casting the vote.
        statements: Vec&lt;(DisputeStatement, ValidatorIndex, ValidatorSignature)&gt;,

        /// Inform the requester once we finished importing.
        ///
        /// This is, we either discarded the votes, just record them because we
        /// casted our vote already or recovered availability for the candidate
        /// successfully.
        pending_confirmation: oneshot::Sender&lt;()&gt;,
    },
    /// Fetch a list of all active disputes that the co-ordinator is aware of.
    ActiveDisputes(ResponseChannel&lt;Vec&lt;(SessionIndex, CandidateHash)&gt;&gt;),
    /// Get candidate votes for a candidate.
    QueryCandidateVotes(SessionIndex, CandidateHash, ResponseChannel&lt;Option&lt;CandidateVotes&gt;&gt;),
    /// Sign and issue local dispute votes. A value of `true` indicates validity, and `false` invalidity.
    IssueLocalStatement(SessionIndex, CandidateHash, CandidateReceipt, bool),
    /// Determine the highest undisputed block within the given chain, based on where candidates
    /// were included. If even the base block should not be finalized due to a dispute,
    /// then `None` should be returned on the channel.
    ///
    /// The block descriptions begin counting upwards from the block after the given `base_number`. The `base_number`
    /// is typically the number of the last finalized block but may be slightly higher. This block
    /// is inevitably going to be finalized so it is not accounted for by this function.
    DetermineUndisputedChain {
        base_number: BlockNumber,
        block_descriptions: Vec&lt;(BlockHash, SessionIndex, Vec&lt;CandidateHash&gt;)&gt;,
        rx: ResponseSender&lt;Option&lt;(BlockNumber, BlockHash)&gt;&gt;,
    }
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="dispute-participation-message"><a class="header" href="#dispute-participation-message">Dispute Participation Message</a></h2>
<p>Messages received by the <a href="types/../node/disputes/dispute-participation.html">Dispute Participation subsystem</a></p>
<p>This subsystem simply executes requests to evaluate a candidate.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum DisputeParticipationMessage {
    /// Validate a candidate for the purposes of participating in a dispute.
    Participate {
        /// The hash of the candidate
        candidate_hash: CandidateHash,
        /// The candidate receipt itself.
        candidate_receipt: CandidateReceipt,
        /// The session the candidate appears in.
        session: SessionIndex,
        /// The number of validators in the session.
        n_validators: u32,
    }
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="dispute-distribution-message"><a class="header" href="#dispute-distribution-message">Dispute Distribution Message</a></h2>
<p>Messages received by the <a href="types/../node/disputes/dispute-distribution.html">Dispute Distribution
subsystem</a>. This subsystem is
responsible of distributing explicit dispute statements.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum DisputeDistributionMessage {

  /// Tell dispute distribution to distribute an explicit dispute statement to
  /// validators.
  SendDispute((ValidVote, InvalidVote)),

  /// Ask DisputeDistribution to get votes we don't know about.
  /// Fetched votes will be reported via `DisputeCoordinatorMessage::ImportStatements`
  FetchMissingVotes {
    candidate_hash: CandidateHash,
    session: SessionIndex,
    known_valid_votes: Bitfield,
    known_invalid_votes: Bitfield,
    /// Optional validator to query from. `ValidatorIndex` as in the above
    /// referenced session.
    from_validator: Option&lt;ValidatorIndex&gt;,
  }
  /// Tell the subsystem that a candidate is not available. Dispute distribution
  /// can punish peers distributing votes on unavailable hashes.
  ReportCandidateUnavailable(CandidateHash),
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="network-bridge-message"><a class="header" href="#network-bridge-message">Network Bridge Message</a></h2>
<p>Messages received by the network bridge. This subsystem is invoked by others to manipulate access
to the low-level networking code.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Peer-sets handled by the network bridge.
enum PeerSet {
    /// The collation peer-set is used to distribute collations from collators to validators.
    Collation,
    /// The validation peer-set is used to distribute information relevant to parachain
    /// validation among validators. This may include nodes which are not validators,
    /// as some protocols on this peer-set are expected to be gossip.
    Validation,
}

enum NetworkBridgeMessage {
    /// Report a cost or benefit of a peer. Negative values are costs, positive are benefits.
    ReportPeer(PeerId, cost_benefit: i32),
    /// Disconnect a peer from the given peer-set without affecting their reputation.
    DisconnectPeer(PeerId, PeerSet),
    /// Send a message to one or more peers on the validation peerset.
    SendValidationMessage([PeerId], ValidationProtocolV1),
    /// Send a message to one or more peers on the collation peerset.
    SendCollationMessage([PeerId], ValidationProtocolV1),
    /// Send multiple validation messages.
    SendValidationMessages([([PeerId, ValidationProtocolV1])]),
    /// Send multiple collation messages.
    SendCollationMessages([([PeerId, ValidationProtocolV1])]),
    /// Connect to peers who represent the given `validator_ids`.
    ///
    /// Also ask the network to stay connected to these peers at least
    /// until a new request is issued.
    ///
    /// Because it overrides the previous request, it must be ensured
    /// that `validator_ids` include all peers the subsystems
    /// are interested in (per `PeerSet`).
    ///
    /// A caller can learn about validator connections by listening to the
    /// `PeerConnected` events from the network bridge.
    ConnectToValidators {
        /// Ids of the validators to connect to.
        validator_ids: Vec&lt;AuthorityDiscoveryId&gt;,
        /// The underlying protocol to use for this request.
        peer_set: PeerSet,
        /// Sends back the number of `AuthorityDiscoveryId`s which
        /// authority discovery has failed to resolve.
        failed: oneshot::Sender&lt;usize&gt;,
    },
    /// Inform the distribution subsystems about the new
    /// gossip network topology formed.
    NewGossipTopology {
        /// Ids of our neighbors in the new gossip topology.
        /// We're not necessarily connected to all of them, but we should.
        our_neighbors: HashSet&lt;AuthorityDiscoveryId&gt;,
    }
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="misbehavior-report"><a class="header" href="#misbehavior-report">Misbehavior Report</a></h2>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub type Misbehavior = generic::Misbehavior&lt;
    CommittedCandidateReceipt,
    CandidateHash,
    ValidatorIndex,
    ValidatorSignature,
&gt;;

mod generic {
    /// Misbehavior: voting more than one way on candidate validity.
    ///
    /// Since there are three possible ways to vote, a double vote is possible in
    /// three possible combinations (unordered)
    pub enum ValidityDoubleVote&lt;Candidate, Digest, Signature&gt; {
        /// Implicit vote by issuing and explicitly voting validity.
        IssuedAndValidity((Candidate, Signature), (Digest, Signature)),
        /// Implicit vote by issuing and explicitly voting invalidity
        IssuedAndInvalidity((Candidate, Signature), (Digest, Signature)),
        /// Direct votes for validity and invalidity
        ValidityAndInvalidity(Candidate, Signature, Signature),
    }

    /// Misbehavior: multiple signatures on same statement.
    pub enum DoubleSign&lt;Candidate, Digest, Signature&gt; {
        /// On candidate.
        Candidate(Candidate, Signature, Signature),
        /// On validity.
        Validity(Digest, Signature, Signature),
        /// On invalidity.
        Invalidity(Digest, Signature, Signature),
    }

    /// Misbehavior: declaring multiple candidates.
    pub struct MultipleCandidates&lt;Candidate, Signature&gt; {
        /// The first candidate seen.
        pub first: (Candidate, Signature),
        /// The second candidate seen.
        pub second: (Candidate, Signature),
    }

    /// Misbehavior: submitted statement for wrong group.
    pub struct UnauthorizedStatement&lt;Candidate, Digest, AuthorityId, Signature&gt; {
        /// A signed statement which was submitted without proper authority.
        pub statement: SignedStatement&lt;Candidate, Digest, AuthorityId, Signature&gt;,
    }

    pub enum Misbehavior&lt;Candidate, Digest, AuthorityId, Signature&gt; {
        /// Voted invalid and valid on validity.
        ValidityDoubleVote(ValidityDoubleVote&lt;Candidate, Digest, Signature&gt;),
        /// Submitted multiple candidates.
        MultipleCandidates(MultipleCandidates&lt;Candidate, Signature&gt;),
        /// Submitted a message that was unauthorized.
        UnauthorizedStatement(UnauthorizedStatement&lt;Candidate, Digest, AuthorityId, Signature&gt;),
        /// Submitted two valid signatures for the same message.
        DoubleSign(DoubleSign&lt;Candidate, Digest, Signature&gt;),
    }
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="pov-distribution-message"><a class="header" href="#pov-distribution-message">PoV Distribution Message</a></h2>
<p>This is a network protocol that receives messages of type <a href="types/network.html#pov-distribution-v1"><code>PoVDistributionV1Message</code></a>.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum PoVDistributionMessage {
    /// Fetch a PoV from the network.
    ///
    /// This `CandidateDescriptor` should correspond to a candidate seconded under the provided
    /// relay-parent hash.
    FetchPoV(Hash, CandidateDescriptor, ResponseChannel&lt;PoV&gt;),
    /// Distribute a PoV for the given relay-parent and CandidateDescriptor.
    /// The PoV should correctly hash to the PoV hash mentioned in the CandidateDescriptor
    DistributePoV(Hash, CandidateDescriptor, PoV),
    /// An update from the network bridge.
    NetworkBridgeUpdateV1(NetworkBridgeEvent&lt;PoVDistributionV1Message&gt;),
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="provisioner-message"><a class="header" href="#provisioner-message">Provisioner Message</a></h2>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// This data becomes intrinsics or extrinsics which should be included in a future relay chain block.
enum ProvisionableData {
  /// This bitfield indicates the availability of various candidate blocks.
  Bitfield(Hash, SignedAvailabilityBitfield),
  /// The Candidate Backing subsystem believes that this candidate is valid, pending availability.
  BackedCandidate(CandidateReceipt),
  /// Misbehavior reports are self-contained proofs of validator misbehavior.
  MisbehaviorReport(Hash, MisbehaviorReport),
  /// Disputes trigger a broad dispute resolution process.
  Dispute(Hash, Signature),
}

/// Message to the Provisioner.
///
/// In all cases, the Hash is that of the relay parent.
enum ProvisionerMessage {
  /// This message allows external subsystems to request current inherent data that could be used for
  /// advancing the state of parachain consensus in a block building upon the given hash.
  ///
  /// If called at different points in time, this may give different results.
  RequestInherentData(Hash, oneshot::Sender&lt;ParaInherentData&gt;),
  /// This data should become part of a relay chain block
  ProvisionableData(ProvisionableData),
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="runtime-api-message"><a class="header" href="#runtime-api-message">Runtime API Message</a></h2>
<p>The Runtime API subsystem is responsible for providing an interface to the state of the chain's runtime.</p>
<p>This is fueled by an auxiliary type encapsulating all request types defined in the Runtime API section of the guide.</p>
<blockquote>
<p>TODO: link to the Runtime API section. Not possible currently because of https://github.com/Michael-F-Bryan/mdbook-linkcheck/issues/25. Once v0.7.1 is released it will work.</p>
</blockquote>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum RuntimeApiRequest {
    /// Get the current validator set.
    Validators(ResponseChannel&lt;Vec&lt;ValidatorId&gt;&gt;),
    /// Get the validator groups and rotation info.
    ValidatorGroups(ResponseChannel&lt;(Vec&lt;Vec&lt;ValidatorIndex&gt;&gt;, GroupRotationInfo)&gt;),
    /// Get information about all availability cores.
    AvailabilityCores(ResponseChannel&lt;Vec&lt;CoreState&gt;&gt;),
    /// with the given occupied core assumption.
    PersistedValidationData(
        ParaId,
        OccupiedCoreAssumption,
        ResponseChannel&lt;Option&lt;PersistedValidationData&gt;&gt;,
    ),
    /// Sends back `true` if the commitments pass all acceptance criteria checks.
    CheckValidationOutputs(
        ParaId,
        CandidateCommitments,
        RuntimeApiSender&lt;bool&gt;,
    ),
    /// Get the session index for children of the block. This can be used to construct a signing
    /// context.
    SessionIndexForChild(ResponseChannel&lt;SessionIndex&gt;),
    /// Get the validation code for a specific para, using the given occupied core assumption.
    ValidationCode(ParaId, OccupiedCoreAssumption, ResponseChannel&lt;Option&lt;ValidationCode&gt;&gt;),
    /// Get validation code by its hash, either past, current or future code can be returned,
    /// as long as state is still available.
    ValidationCodeByHash(ValidationCodeHash, RuntimeApiSender&lt;Option&lt;ValidationCode&gt;&gt;),
    /// Get a committed candidate receipt for all candidates pending availability.
    CandidatePendingAvailability(ParaId, ResponseChannel&lt;Option&lt;CommittedCandidateReceipt&gt;&gt;),
    /// Get all events concerning candidates in the last block.
    CandidateEvents(ResponseChannel&lt;Vec&lt;CandidateEvent&gt;&gt;),
    /// Get the session info for the given session, if stored.
    SessionInfo(SessionIndex, ResponseChannel&lt;Option&lt;SessionInfo&gt;&gt;),
    /// Get all the pending inbound messages in the downward message queue for a para.
    DmqContents(ParaId, ResponseChannel&lt;Vec&lt;InboundDownwardMessage&lt;BlockNumber&gt;&gt;&gt;),
    /// Get the contents of all channels addressed to the given recipient. Channels that have no
    /// messages in them are also included.
    InboundHrmpChannelsContents(ParaId, ResponseChannel&lt;BTreeMap&lt;ParaId, Vec&lt;InboundHrmpMessage&lt;BlockNumber&gt;&gt;&gt;&gt;),
    /// Get information about the BABE epoch this block was produced in.
    BabeEpoch(ResponseChannel&lt;BabeEpoch&gt;),
}

enum RuntimeApiMessage {
    /// Make a request of the runtime API against the post-state of the given relay-parent.
    Request(Hash, RuntimeApiRequest),
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="statement-distribution-message"><a class="header" href="#statement-distribution-message">Statement Distribution Message</a></h2>
<p>The Statement Distribution subsystem distributes signed statements and candidates from validators to other validators. It does this by distributing full statements, which embed the candidate receipt, as opposed to compact statements which don't.
It receives updates from the network bridge and signed statements to share with other validators.</p>
<p>This is a network protocol that receives messages of type <a href="types/network.html#statement-distribution-v1"><code>StatementDistributionV1Message</code></a>.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum StatementDistributionMessage {
    /// An update from the network bridge.
    NetworkBridgeUpdateV1(NetworkBridgeEvent&lt;StatementDistributionV1Message&gt;),
    /// We have validated a candidate and want to share our judgment with our peers.
    /// The hash is the relay parent.
    ///
    /// The statement distribution subsystem assumes that the statement should be correctly
    /// signed.
    Share(Hash, SignedFullStatement),
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="validation-request-type"><a class="header" href="#validation-request-type">Validation Request Type</a></h2>
<p>Various modules request that the <a href="types/../node/utility/candidate-validation.html">Candidate Validation subsystem</a> validate a block with this message. It returns <a href="types/candidate.html#validationoutputs"><code>ValidationOutputs</code></a> for successful validation.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span>
<span class="boring">fn main() {
</span>/// Result of the validation of the candidate.
enum ValidationResult {
    /// Candidate is valid, and here are the outputs and the validation data used to form inputs.
    /// In practice, this should be a shared type so that validation caching can be done.
    Valid(CandidateCommitments, PersistedValidationData),
    /// Candidate is invalid.
    Invalid,
}

/// Messages received by the Validation subsystem.
///
/// ## Validation Requests
///
/// Validation requests made to the subsystem should return an error only on internal error.
/// Otherwise, they should return either `Ok(ValidationResult::Valid(_))`
/// or `Ok(ValidationResult::Invalid)`.
#[derive(Debug)]
pub enum CandidateValidationMessage {
    /// Validate a candidate with provided parameters using relay-chain state.
    ///
    /// This will implicitly attempt to gather the `PersistedValidationData` and `ValidationCode`
    /// from the runtime API of the chain, based on the `relay_parent`
    /// of the `CandidateDescriptor`.
    ///
    /// This will also perform checking of validation outputs against the acceptance criteria.
    ///
    /// If there is no state available which can provide this data or the core for
    /// the para is not free at the relay-parent, an error is returned.
    ValidateFromChainState(
        CandidateDescriptor,
        Arc&lt;PoV&gt;,
        oneshot::Sender&lt;Result&lt;ValidationResult, ValidationFailed&gt;&gt;,
    ),
    /// Validate a candidate with provided, exhaustive parameters for validation.
    ///
    /// Explicitly provide the `PersistedValidationData` and `ValidationCode` so this can do full
    /// validation without needing to access the state of the relay-chain.
    ///
    /// This request doesn't involve acceptance criteria checking, therefore only useful for the
    /// cases where the validity of the candidate is established. This is the case for the typical
    /// use-case: secondary checkers would use this request relying on the full prior checks
    /// performed by the relay-chain.
    ValidateFromExhaustive(
        PersistedValidationData,
        ValidationCode,
        CandidateDescriptor,
        Arc&lt;PoV&gt;,
        oneshot::Sender&lt;Result&lt;ValidationResult, ValidationFailed&gt;&gt;,
    ),
}
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="runtime"><a class="header" href="#runtime">Runtime</a></h1>
<p>Types used within the runtime exclusively and pervasively.</p>
<h2 id="host-configuration"><a class="header" href="#host-configuration">Host Configuration</a></h2>
<p>The internal-to-runtime configuration of the parachain host. This is expected to be altered only by governance procedures.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct HostConfiguration {
	/// The minimum frequency at which parachains can update their validation code.
	pub validation_upgrade_frequency: BlockNumber,
	/// The delay, in blocks, before a validation upgrade is applied.
	pub validation_upgrade_delay: BlockNumber,
	/// How long to keep code on-chain, in blocks. This should be sufficiently long that disputes
	/// have concluded.
	pub code_retention_period: BlockNumber,
	/// The maximum validation code size, in bytes.
	pub max_code_size: u32,
	/// The maximum head-data size, in bytes.
	pub max_head_data_size: u32,
	/// The amount of availability cores to dedicate to parathreads.
	pub parathread_cores: u32,
	/// The number of retries that a parathread author has to submit their block.
	pub parathread_retries: u32,
	/// How often parachain groups should be rotated across parachains.
	pub group_rotation_frequency: BlockNumber,
	/// The availability period, in blocks, for parachains. This is the amount of blocks
	/// after inclusion that validators have to make the block available and signal its availability to
	/// the chain. Must be at least 1.
	pub chain_availability_period: BlockNumber,
	/// The availability period, in blocks, for parathreads. Same as the `chain_availability_period`,
	/// but a differing timeout due to differing requirements. Must be at least 1.
	pub thread_availability_period: BlockNumber,
	/// The amount of blocks ahead to schedule parathreads.
	pub scheduling_lookahead: u32,
	/// The maximum number of validators to have per core. `None` means no maximum.
	pub max_validators_per_core: Option&lt;u32&gt;,
	/// The maximum number of validators to use for parachains, in total. `None` means no maximum.
	pub max_validators: Option&lt;u32&gt;,
	/// The amount of sessions to keep for disputes.
	pub dispute_period: SessionIndex,
	/// How long after dispute conclusion to accept statements.
	pub dispute_post_conclusion_acceptance_period: BlockNumber,
	/// The maximum number of dispute spam slots 
	pub dispute_max_spam_slots: u32,
	/// How long it takes for a dispute to conclude by time-out, if no supermajority is reached.
	pub dispute_conclusion_by_time_out_period: BlockNumber,
	/// The amount of consensus slots that must pass between submitting an assignment and
	/// submitting an approval vote before a validator is considered a no-show.
	/// Must be at least 1.
	pub no_show_slots: u32,
	/// The number of delay tranches in total.
	pub n_delay_tranches: u32,
	/// The width of the zeroth delay tranche for approval assignments. This many delay tranches
	/// beyond 0 are all consolidated to form a wide 0 tranche.
	pub zeroth_delay_tranche_width: u32,
	/// The number of validators needed to approve a block.
	pub needed_approvals: u32,
	/// The number of samples to do of the RelayVRFModulo approval assignment criterion.
	pub relay_vrf_modulo_samples: u32,
	/// Total number of individual messages allowed in the parachain -&gt; relay-chain message queue.
	pub max_upward_queue_count: u32,
	/// Total size of messages allowed in the parachain -&gt; relay-chain message queue before which
	/// no further messages may be added to it. If it exceeds this then the queue may contain only
	/// a single message.
	pub max_upward_queue_size: u32,
	/// The amount of weight we wish to devote to the processing the dispatchable upward messages
	/// stage.
	///
	/// NOTE that this is a soft limit and could be exceeded.
	pub ump_service_total_weight: Weight,
	/// The maximum size of an upward message that can be sent by a candidate.
	///
	/// This parameter affects the upper bound of size of `CandidateCommitments`.
	pub max_upward_message_size: u32,
	/// The maximum number of messages that a candidate can contain.
	///
	/// This parameter affects the upper bound of size of `CandidateCommitments`.
	pub max_upward_message_num_per_candidate: u32,
	/// The maximum size of a message that can be put in a downward message queue.
	///
	/// Since we require receiving at least one DMP message the obvious upper bound of the size is
	/// the PoV size. Of course, there is a lot of other different things that a parachain may
	/// decide to do with its PoV so this value in practice will be picked as a fraction of the PoV
	/// size.
	pub max_downward_message_size: u32,
	/// Number of sessions after which an HRMP open channel request expires.
	pub hrmp_open_request_ttl: u32,
	/// The deposit that the sender should provide for opening an HRMP channel.
	pub hrmp_sender_deposit: u32,
	/// The deposit that the recipient should provide for accepting opening an HRMP channel.
	pub hrmp_recipient_deposit: u32,
	/// The maximum number of messages allowed in an HRMP channel at once.
	pub hrmp_channel_max_capacity: u32,
	/// The maximum total size of messages in bytes allowed in an HRMP channel at once.
	pub hrmp_channel_max_total_size: u32,
	/// The maximum number of inbound HRMP channels a parachain is allowed to accept.
	pub hrmp_max_parachain_inbound_channels: u32,
	/// The maximum number of inbound HRMP channels a parathread is allowed to accept.
	pub hrmp_max_parathread_inbound_channels: u32,
	/// The maximum size of a message that could ever be put into an HRMP channel.
	///
	/// This parameter affects the upper bound of size of `CandidateCommitments`.
	pub hrmp_channel_max_message_size: u32,
	/// The maximum number of outbound HRMP channels a parachain is allowed to open.
	pub hrmp_max_parachain_outbound_channels: u32,
	/// The maximum number of outbound HRMP channels a parathread is allowed to open.
	pub hrmp_max_parathread_outbound_channels: u32,
	/// The maximum number of outbound HRMP messages can be sent by a candidate.
	///
	/// This parameter affects the upper bound of size of `CandidateCommitments`.
	pub hrmp_max_message_num_per_candidate: u32,
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="parainherentdata"><a class="header" href="#parainherentdata">ParaInherentData</a></h2>
<p>Inherent data passed to a runtime entry-point for the advancement of parachain consensus.</p>
<p>This contains 3 pieces of data:</p>
<ol>
<li><a href="types/availability.html#signed-availability-bitfield"><code>Bitfields</code></a> </li>
<li><a href="types/backing.html#backed-candidate"><code>BackedCandidates</code></a></li>
<li><a href="types/disputes.html#multidisputestatementset"><code>MultiDisputeStatementSet</code></a></li>
</ol>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct ParaInherentData {
	bitfields: Bitfields,
	backed_candidates: BackedCandidates,
	dispute_statements: MultiDisputeStatementSet,
}
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chain"><a class="header" href="#chain">Chain</a></h1>
<p>Types pertaining to the relay-chain - events, structures, etc.</p>
<h2 id="block-import-event"><a class="header" href="#block-import-event">Block Import Event</a></h2>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Indicates that a new block has been added to the blockchain.
struct BlockImportEvent {
  /// The block header-hash.
  hash: Hash,
  /// The header itself.
  header: Header,
  /// Whether this block is considered the head of the best chain according to the
  /// event emitter's fork-choice rule.
  new_best: bool,
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="block-finalization-event"><a class="header" href="#block-finalization-event">Block Finalization Event</a></h2>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Indicates that a new block has been finalized.
struct BlockFinalizationEvent {
  /// The block header-hash.
  hash: Hash,
  /// The header of the finalized block.
  header: Header,
}
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="message-types"><a class="header" href="#message-types">Message types</a></h1>
<p>Types of messages that are passed between parachains and the relay chain: UMP, DMP, XCMP.</p>
<p>There is also HRMP (Horizontally Relay-routed Message Passing) which provides the same functionality
although with smaller scalability potential.</p>
<h2 id="vertical-message-passing-1"><a class="header" href="#vertical-message-passing-1">Vertical Message Passing</a></h2>
<p>Types required for message passing between the relay-chain and a parachain.</p>
<p>Actual contents of the messages is specified by the XCM standard.</p>
<pre><code class="language-rust ignore">/// A message sent from a parachain to the relay-chain.
type UpwardMessage = Vec&lt;u8&gt;;

/// A message sent from the relay-chain down to a parachain.
///
/// The size of the message is limited by the `config.max_downward_message_size`
/// parameter.
type DownwardMessage = Vec&lt;u8&gt;;

/// This struct extends `DownwardMessage` by adding the relay-chain block number when the message was
/// enqueued in the downward message queue.
struct InboundDownwardMessage {
	/// The block number at which this messages was put into the downward message queue.
	pub sent_at: BlockNumber,
	/// The actual downward message to processes.
	pub msg: DownwardMessage,
}
</code></pre>
<h2 id="horizontal-message-passing-1"><a class="header" href="#horizontal-message-passing-1">Horizontal Message Passing</a></h2>
<h2 id="hrmpchannelid"><a class="header" href="#hrmpchannelid">HrmpChannelId</a></h2>
<p>A type that uniquely identifies an HRMP channel. An HRMP channel is established between two paras.
In text, we use the notation <code>(A, B)</code> to specify a channel between A and B. The channels are
unidirectional, meaning that <code>(A, B)</code> and <code>(B, A)</code> refer to different channels. The convention is
that we use the first item tuple for the sender and the second for the recipient. Only one channel
is allowed between two participants in one direction, i.e. there cannot be 2 different channels
identified by <code>(A, B)</code>.</p>
<pre><code class="language-rust ignore">struct HrmpChannelId {
    sender: ParaId,
    recipient: ParaId,
}
</code></pre>
<h2 id="horizontal-message"><a class="header" href="#horizontal-message">Horizontal Message</a></h2>
<p>This is a message sent from a parachain to another parachain that travels through the relay chain.
This message ends up in the recipient's mailbox. A size of a horizontal message is defined by its
<code>data</code> payload.</p>
<pre><code class="language-rust ignore">struct OutboundHrmpMessage {
	/// The para that will get this message in its downward message queue.
	pub recipient: ParaId,
	/// The message payload.
	pub data: Vec&lt;u8&gt;,
}

struct InboundHrmpMessage {
	/// The block number at which this message was sent.
	/// Specifically, it is the block number at which the candidate that sends this message was
	/// enacted.
	pub sent_at: BlockNumber,
	/// The message payload.
	pub data: Vec&lt;u8&gt;,
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="network-types"><a class="header" href="#network-types">Network Types</a></h1>
<p>These types are those that are actually sent over the network to subsystems.</p>
<h2 id="universal-types"><a class="header" href="#universal-types">Universal Types</a></h2>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>type RequestId = u64;
type ProtocolVersion = u32;
struct PeerId(...); // opaque, unique identifier of a peer.
struct View {
	// Up to `N` (5?) chain heads.
	heads: Vec&lt;Hash&gt;,
	// The number of the finalized block.
	finalized_number: BlockNumber,
}

enum ObservedRole {
	Full,
	Light,
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="v1-network-subsystem-message-types"><a class="header" href="#v1-network-subsystem-message-types">V1 Network Subsystem Message Types</a></h2>
<h3 id="approval-distribution-v1"><a class="header" href="#approval-distribution-v1">Approval Distribution V1</a></h3>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum ApprovalDistributionV1Message {
	/// Assignments for candidates in recent, unfinalized blocks.
	///
	/// The u32 is the claimed index of the candidate this assignment corresponds to. Actually checking the assignment
	/// may yield a different result.
	Assignments(Vec&lt;(IndirectAssignmentCert, u32)&gt;),
	/// Approvals for candidates in some recent, unfinalized block.
	Approvals(Vec&lt;IndirectSignedApprovalVote&gt;),
}
<span class="boring">}
</span></code></pre></pre>
<h3 id="availability-distribution-v1"><a class="header" href="#availability-distribution-v1">Availability Distribution V1</a></h3>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum AvailabilityDistributionV1Message {
	/// An erasure chunk for a given candidate hash.
	Chunk(CandidateHash, ErasureChunk),
}
<span class="boring">}
</span></code></pre></pre>
<h3 id="availability-recovery-v1"><a class="header" href="#availability-recovery-v1">Availability Recovery V1</a></h3>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum AvailabilityRecoveryV1Message {
	/// Request a chunk for a given candidate hash and validator index.
	RequestChunk(RequestId, CandidateHash, ValidatorIndex),
	/// Respond with chunk for a given candidate hash and validator index.
	/// The response may be `None` if the requestee does not have the chunk.
	Chunk(RequestId, Option&lt;ErasureChunk&gt;),
	/// Request the full data for a given candidate hash.
	RequestFullData(RequestId, CandidateHash),
	/// Respond with data for a given candidate hash and validator index.
	/// The response may be `None` if the requestee does not have the data.
	FullData(RequestId, Option&lt;AvailableData&gt;),

}
<span class="boring">}
</span></code></pre></pre>
<h3 id="bitfield-distribution-v1"><a class="header" href="#bitfield-distribution-v1">Bitfield Distribution V1</a></h3>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum BitfieldDistributionV1Message {
	/// A signed availability bitfield for a given relay-parent hash.
	Bitfield(Hash, SignedAvailabilityBitfield),
}
<span class="boring">}
</span></code></pre></pre>
<h3 id="pov-distribution-v1"><a class="header" href="#pov-distribution-v1">PoV Distribution V1</a></h3>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum PoVDistributionV1Message {
	/// Notification that we are awaiting the given PoVs (by hash) against a
	/// specific relay-parent hash.
	Awaiting(Hash, Vec&lt;Hash&gt;),
	/// Notification of an awaited PoV, in a given relay-parent context.
	/// (relay_parent, pov_hash, pov)
	SendPoV(Hash, Hash, PoV),
}
<span class="boring">}
</span></code></pre></pre>
<h3 id="statement-distribution-v1"><a class="header" href="#statement-distribution-v1">Statement Distribution V1</a></h3>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum StatementDistributionV1Message {
	/// A signed full statement under a given relay-parent.
	Statement(Hash, SignedFullStatement)
}
<span class="boring">}
</span></code></pre></pre>
<h3 id="collator-protocol-v1"><a class="header" href="#collator-protocol-v1">Collator Protocol V1</a></h3>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum CollatorProtocolV1Message {
	/// Declare the intent to advertise collations under a collator ID and `Para`, attaching a
	/// signature of the `PeerId` of the node using the given collator ID key.
	Declare(CollatorId, ParaId, CollatorSignature),
	/// Advertise a collation to a validator. Can only be sent once the peer has
	/// declared that they are a collator with given ID.
	AdvertiseCollation(Hash),
	/// A collation sent to a validator was seconded.
	CollationSeconded(SignedFullStatement),
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="v1-wire-protocols"><a class="header" href="#v1-wire-protocols">V1 Wire Protocols</a></h2>
<h3 id="validation-v1-1"><a class="header" href="#validation-v1-1">Validation V1</a></h3>
<p>These are the messages for the protocol on the validation peer-set.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum ValidationProtocolV1 {
	ApprovalDistribution(ApprovalDistributionV1Message),
	AvailabilityDistribution(AvailabilityDistributionV1Message),
	AvailabilityRecovery(AvailabilityRecoveryV1Message),
	BitfieldDistribution(BitfieldDistributionV1Message),
	PoVDistribution(PoVDistributionV1Message),
	StatementDistribution(StatementDistributionV1Message),
}
<span class="boring">}
</span></code></pre></pre>
<h3 id="collation-v1-1"><a class="header" href="#collation-v1-1">Collation V1</a></h3>
<p>These are the messages for the protocol on the collation peer-set</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum CollationProtocolV1 {
	CollatorProtocol(CollatorProtocolV1Message),
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="network-bridge-event"><a class="header" href="#network-bridge-event">Network Bridge Event</a></h2>
<p>These updates are posted from the <a href="types/../node/utility/network-bridge.html">Network Bridge Subsystem</a> to other subsystems based on registered listeners.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum NetworkBridgeEvent&lt;M&gt; {
	/// A peer with given ID is now connected.
	PeerConnected(PeerId, ObservedRole),
	/// A peer with given ID is now disconnected.
	PeerDisconnected(PeerId),
	/// Our neighbors in the new gossip topology.
	/// We're not necessarily connected to all of them.
	///
	/// This message is issued only on the validation peer set.
	///
	/// Note, that the distribution subsystems need to handle the last
	/// view update of the newly added gossip peers manually.
	NewGossipTopology(HashSet&lt;PeerId&gt;),
	/// We received a message from the given peer.
	PeerMessage(PeerId, M),
	/// The given peer has updated its description of its view.
	PeerViewChange(PeerId, View), // guaranteed to come after peer connected event.
	/// We have posted the given view update to all connected peers.
	OurViewChange(View),
}
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="approval-types"><a class="header" href="#approval-types">Approval Types</a></h1>
<h2 id="assignmentid"><a class="header" href="#assignmentid">AssignmentId</a></h2>
<p>The public key of a keypair used by a validator for determining assignments to approve included parachain candidates.</p>
<h2 id="assignmentcert"><a class="header" href="#assignmentcert">AssignmentCert</a></h2>
<p>An <code>AssignmentCert</code>, short for Assignment Certificate, is a piece of data provided by a validator to prove that they have been selected to perform secondary approval checks on an included candidate.</p>
<p>These certificates can be checked in the context of a specific block, candidate, and validator assignment VRF key. The block state will also provide further context about the availability core states at that block.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum AssignmentCertKind {
    RelayVRFModulo {
        sample: u32,
    },
    RelayVRFDelay {
        core_index: CoreIndex,
    }
}

struct AssignmentCert {
    // The criterion which is claimed to be met by this cert.
    kind: AssignmentCertKind,
    // The VRF showing the criterion is met.
    vrf: (VRFPreOut, VRFProof),
}
<span class="boring">}
</span></code></pre></pre>
<blockquote>
<p>TODO: RelayEquivocation cert. Probably can only be broadcast to chains that have handled an equivocation report.</p>
</blockquote>
<h2 id="indirectassignmentcert"><a class="header" href="#indirectassignmentcert">IndirectAssignmentCert</a></h2>
<p>An assignment cert which refers to the candidate under which the assignment is relevant by block hash.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct IndirectAssignmentCert {
    // A block hash where the candidate appears.
    block_hash: Hash,
    validator: ValidatorIndex,
    cert: AssignmentCert,
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="approvalvote"><a class="header" href="#approvalvote">ApprovalVote</a></h2>
<p>A vote of approval on a candidate.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct ApprovalVote(Hash);
<span class="boring">}
</span></code></pre></pre>
<h2 id="signedapprovalvote"><a class="header" href="#signedapprovalvote">SignedApprovalVote</a></h2>
<p>An approval vote signed with a validator's key. This should be verifiable under the <code>ValidatorId</code> corresponding to the <code>ValidatorIndex</code> of the session, which should be implicit from context.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct SignedApprovalVote {
    vote: ApprovalVote,
    validator: ValidatorIndex,
    signature: ValidatorSignature,
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="indirectsignedapprovalvote"><a class="header" href="#indirectsignedapprovalvote">IndirectSignedApprovalVote</a></h2>
<p>A signed approval vote which references the candidate indirectly via the block. If there exists a look-up to the candidate hash from the block hash and candidate index, then this can be transformed into a <code>SignedApprovalVote</code>.</p>
<p>Although this vote references the candidate by a specific block hash and candidate index, the signature is computed on the actual <code>SignedApprovalVote</code> payload.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct IndirectSignedApprovalVote {
    // A block hash where the candidate appears.
    block_hash: Hash,
    // The index of the candidate in the list of candidates fully included as-of the block.
    candidate_index: CandidateIndex,
    validator: ValidatorIndex,
    signature: ValidatorSignature,
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="checkedassignmentcert"><a class="header" href="#checkedassignmentcert">CheckedAssignmentCert</a></h2>
<p>An assignment cert which has checked both the VRF and the validity of the implied assignment according to the selection criteria rules of the protocol. This type should be declared in such a way as to be instantiable only when the checks have actually been done. Fields should be accessible via getters, not direct struct access.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct CheckedAssignmentCert {
    cert: AssignmentCert,
    validator: ValidatorIndex,
    relay_block: Hash,
    candidate_hash: Hash,
    delay_tranche: DelayTranche,
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="delaytranche"><a class="header" href="#delaytranche">DelayTranche</a></h2>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>type DelayTranche = u32;
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="disputes-2"><a class="header" href="#disputes-2">Disputes</a></h1>
<h2 id="disputestatementset"><a class="header" href="#disputestatementset">DisputeStatementSet</a></h2>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// A set of statements about a specific candidate.
struct DisputeStatementSet {
    candidate_hash: CandidateHash,
    session: SessionIndex,
    statements: Vec&lt;(DisputeStatement, ValidatorIndex, ValidatorSignature)&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="disputestatement"><a class="header" href="#disputestatement">DisputeStatement</a></h2>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// A statement about a candidate, to be used within some dispute resolution process.
///
/// Statements are either in favor of the candidate's validity or against it.
enum DisputeStatement {
    /// A valid statement, of the given kind
    Valid(ValidDisputeStatementKind),
    /// An invalid statement, of the given kind.
    Invalid(InvalidDisputeStatementKind),
}

<span class="boring">}
</span></code></pre></pre>
<h2 id="dispute-statement-kinds"><a class="header" href="#dispute-statement-kinds">Dispute Statement Kinds</a></h2>
<p>Kinds of dispute statements. Each of these can be combined with a candidate hash, session index, validator public key, and validator signature to reproduce and check the original statement.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum ValidDisputeStatementKind {
    Explicit,
    BackingSeconded,
    BackingValid,
    ApprovalChecking,
}

enum InvalidDisputeStatementKind {
    Explicit,
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="explicitdisputestatement"><a class="header" href="#explicitdisputestatement">ExplicitDisputeStatement</a></h2>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct ExplicitDisputeStatement {
    valid: bool,
    candidate_hash: CandidateHash,
    session: SessionIndex,
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="multidisputestatementset"><a class="header" href="#multidisputestatementset">MultiDisputeStatementSet</a></h2>
<p>Sets of statements for many (zero or more) disputes.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>type MultiDisputeStatementSet = Vec&lt;DisputeStatementSet&gt;;
<span class="boring">}
</span></code></pre></pre>
<h2 id="disputestate"><a class="header" href="#disputestate">DisputeState</a></h2>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct DisputeState {
    validators_for: Bitfield, // one bit per validator.
    validators_against: Bitfield, // one bit per validator.
    start: BlockNumber,
    concluded_at: Option&lt;BlockNumber&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="glossary"><a class="header" href="#glossary">Glossary</a></h1>
<p>Here you can find definitions of a bunch of jargon, usually specific to the Polkadot project.</p>
<ul>
<li>BABE: (Blind Assignment for Blockchain Extension). The algorithm validators use to safely extend the Relay Chain. See <a href="https://wiki.polkadot.network/docs/learn-consensus">the Polkadot wiki</a> for more information.</li>
<li>Backable Candidate: A Parachain Candidate which is backed by a majority of validators assigned to a given parachain.</li>
<li>Backed Candidate: A Backable Candidate noted in a relay-chain block</li>
<li>Backing: A set of statements proving that a Parachain Candidate is backable.</li>
<li>Collator: A node who generates Proofs-of-Validity (PoV) for blocks of a specific parachain.</li>
<li>DMP: (Downward Message Passing). Message passing from the relay-chain to a parachain. Also there is a runtime parachains module with the same name.</li>
<li>DMQ: (Downward Message Queue). A message queue for messages from the relay-chain down to a parachain. A parachain has
exactly one downward message queue.</li>
<li>Extrinsic: An element of a relay-chain block which triggers a specific entry-point of a runtime module with given arguments.</li>
<li>GRANDPA: (Ghost-based Recursive ANcestor Deriving Prefix Agreement). The algorithm validators use to guarantee finality of the Relay Chain.</li>
<li>HRMP: (Horizontally Relay-routed Message Passing). A mechanism for message passing between parachains (hence horizontal) that leverages the relay-chain storage. Predates XCMP. Also there is a runtime parachains module with the same name.</li>
<li>Inclusion Pipeline: The set of steps taken to carry a Parachain Candidate from authoring, to backing, to availability and full inclusion in an active fork of its parachain.</li>
<li>Module: A component of the Runtime logic, encapsulating storage, routines, and entry-points.</li>
<li>Module Entry Point: A recipient of new information presented to the Runtime. This may trigger routines.</li>
<li>Module Routine: A piece of code executed within a module by block initialization, closing, or upon an entry point being triggered. This may execute computation, and read or write storage.</li>
<li>MQC: (Message Queue Chain). A cryptographic data structure that resembles an append-only linked list which doesn't store original values but only their hashes. The whole structure is described by a single hash, referred as a &quot;head&quot;. When a value is appended, it's contents hashed with the previous head creating a hash that becomes a new head.</li>
<li>Node: A participant in the Polkadot network, who follows the protocols of communication and connection to other nodes. Nodes form a peer-to-peer network topology without a central authority.</li>
<li>Parachain Candidate, or Candidate: A proposed block for inclusion into a parachain.</li>
<li>Parablock: A block in a parachain.</li>
<li>Parachain: A constituent chain secured by the Relay Chain's validators.</li>
<li>Parachain Validators: A subset of validators assigned during a period of time to back candidates for a specific parachain</li>
<li>Parathread: A parachain which is scheduled on a pay-as-you-go basis.</li>
<li>PDK (Parachain Development Kit): A toolset that allows one to develop a parachain. Cumulus is a PDK.</li>
<li>Preimage: In our context, if <code>H(X) = Y</code> where <code>H</code> is a hash function and <code>Y</code> is the hash, then <code>X</code> is the hash preimage.</li>
<li>Proof-of-Validity (PoV): A stateless-client proof that a parachain candidate is valid, with respect to some validation function.</li>
<li>Relay Parent: A block in the relay chain, referred to in a context where work is being done in the context of the state at this block.</li>
<li>Router: The router module is a meta module that consists of three runtime modules responsible for routing messages between paras and the relay chain. The three separate runtime modules are: Dmp, Ump, Hrmp, each responsible for the respective part of message routing.</li>
<li>Runtime: The relay-chain state machine.</li>
<li>Runtime Module: See Module.</li>
<li>Runtime API: A means for the node-side behavior to access structured information based on the state of a fork of the blockchain.</li>
<li>Secondary Checker: A validator who has been randomly selected to perform secondary approval checks on a parablock which is pending approval.</li>
<li>Subsystem: A long-running task which is responsible for carrying out a particular category of work.</li>
<li>UMP: (Upward Message Passing) A vertical message passing mechanism from a parachain to the relay chain.</li>
<li>Validator: Specially-selected node in the network who is responsible for validating parachain blocks and issuing attestations about their validity.</li>
<li>Validation Function: A piece of Wasm code that describes the state-transition function of a parachain.</li>
<li>VMP: (Vertical Message Passing) A family of mechanisms that are responsible for message exchange between the relay chain and parachains.</li>
<li>XCMP (Cross-Chain Message Passing) A type of horizontal message passing (i.e. between parachains) that allows secure message passing directly between parachains and has minimal resource requirements from the relay chain, thus highly scalable.</li>
</ul>
<p>Also of use is the <a href="https://substrate.dev/docs/en/knowledgebase/getting-started/glossary">Substrate Glossary</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="further-reading"><a class="header" href="#further-reading">Further Reading</a></h1>
<ul>
<li>Polkadot Wiki on Consensus: <a href="https://wiki.polkadot.network/docs/learn-consensus">https://wiki.polkadot.network/docs/learn-consensus</a></li>
<li>Polkadot Spec: <a href="https://github.com/w3f/polkadot-spec">https://github.com/w3f/polkadot-spec</a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                        
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                            </nav>

        </div>

        
        
        
                <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        
        
                <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
                <script type="text/javascript" src="mermaid.min.js"></script>
                <script type="text/javascript" src="mermaid-init.js"></script>
        
                        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
                
    </body>
</html>
